WARNING: Logging before InitGoogleLogging() is written to STDERR
I0422 07:02:41.681248  2460 solver.cpp:48] Initializing solver from parameters: 
train_net: "/home/ubuntu/pynb/caffe-future/train/config/train.prototxt"
test_net: "/home/ubuntu/pynb/caffe-future/train/config/train.prototxt"
test_iter: 736
test_interval: 999999999
base_lr: 1e-10
display: 20
max_iter: 100000
lr_policy: "fixed"
momentum: 0.99
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "/home/ubuntu/pynb/caffe-future/ilsvrc-nets/"
test_initialization: false
average_loss: 20
iter_size: 1
I0422 07:02:41.681406  2460 solver.cpp:81] Creating training net from train_net file: /home/ubuntu/pynb/caffe-future/train/config/train.prototxt
I0422 07:02:41.682379  2460 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  python_param {
    module: "voc_layers"
    layer: "SBDDSegDataLayer"
    param_str: "{\'sbdd_dir\': \'../caffe-future/train/dataset\', \'seed\': 1337, \'split\': \'train\', \'mean\': (104.00699, 116.66877, 122.67892)}"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 100
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_3"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_3"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    pad: 0
    kernel_size: 7
    stride: 1
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "Convolution"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score_fr"
  type: "Convolution"
  bottom: "fc7"
  top: "score_fr"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 21
    pad: 0
    kernel_size: 1
  }
}
layer {
  name: "upscore"
  type: "Deconvolution"
  bottom: "score_fr"
  top: "upscore"
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 21
    bias_term: false
    kernel_size: 64
    stride: 32
  }
}
layer {
  name: "score"
  type: "Crop"
  bottom: "upscore"
  bottom: "data"
  top: "score"
  crop_param {
    axis: 2
    offset: 19
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
  loss_param {
    ignore_label: 255
    normalize: false
  }
}
I0422 07:02:41.682554  2460 layer_factory.hpp:77] Creating layer data
I0422 07:02:41.682915  2460 net.cpp:91] Creating Layer data
I0422 07:02:41.682938  2460 net.cpp:399] data -> data
I0422 07:02:41.682963  2460 net.cpp:399] data -> label
I0422 07:02:41.720293  2460 net.cpp:141] Setting up data
I0422 07:02:41.720348  2460 net.cpp:148] Top shape: 1 3 375 500 (562500)
I0422 07:02:41.720357  2460 net.cpp:148] Top shape: 1 1 375 500 (187500)
I0422 07:02:41.720363  2460 net.cpp:156] Memory required for data: 3000000
I0422 07:02:41.720374  2460 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 07:02:41.720401  2460 net.cpp:91] Creating Layer data_data_0_split
I0422 07:02:41.720408  2460 net.cpp:425] data_data_0_split <- data
I0422 07:02:41.720423  2460 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 07:02:41.720441  2460 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 07:02:41.720486  2460 net.cpp:141] Setting up data_data_0_split
I0422 07:02:41.720497  2460 net.cpp:148] Top shape: 1 3 375 500 (562500)
I0422 07:02:41.720504  2460 net.cpp:148] Top shape: 1 3 375 500 (562500)
I0422 07:02:41.720510  2460 net.cpp:156] Memory required for data: 7500000
I0422 07:02:41.720515  2460 layer_factory.hpp:77] Creating layer conv1_1
I0422 07:02:41.720537  2460 net.cpp:91] Creating Layer conv1_1
I0422 07:02:41.720547  2460 net.cpp:425] conv1_1 <- data_data_0_split_0
I0422 07:02:41.720558  2460 net.cpp:399] conv1_1 -> conv1_1
I0422 07:02:42.193756  2460 net.cpp:141] Setting up conv1_1
I0422 07:02:42.193814  2460 net.cpp:148] Top shape: 1 64 573 698 (25597056)
I0422 07:02:42.193820  2460 net.cpp:156] Memory required for data: 109888224
I0422 07:02:42.193850  2460 layer_factory.hpp:77] Creating layer relu1_1
I0422 07:02:42.193871  2460 net.cpp:91] Creating Layer relu1_1
I0422 07:02:42.193879  2460 net.cpp:425] relu1_1 <- conv1_1
I0422 07:02:42.193889  2460 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0422 07:02:42.194043  2460 net.cpp:141] Setting up relu1_1
I0422 07:02:42.194057  2460 net.cpp:148] Top shape: 1 64 573 698 (25597056)
I0422 07:02:42.194063  2460 net.cpp:156] Memory required for data: 212276448
I0422 07:02:42.194069  2460 layer_factory.hpp:77] Creating layer conv1_2
I0422 07:02:42.194089  2460 net.cpp:91] Creating Layer conv1_2
I0422 07:02:42.194095  2460 net.cpp:425] conv1_2 <- conv1_1
I0422 07:02:42.194103  2460 net.cpp:399] conv1_2 -> conv1_2
I0422 07:02:42.196744  2460 net.cpp:141] Setting up conv1_2
I0422 07:02:42.196766  2460 net.cpp:148] Top shape: 1 64 573 698 (25597056)
I0422 07:02:42.196772  2460 net.cpp:156] Memory required for data: 314664672
I0422 07:02:42.196785  2460 layer_factory.hpp:77] Creating layer relu1_2
I0422 07:02:42.196804  2460 net.cpp:91] Creating Layer relu1_2
I0422 07:02:42.196810  2460 net.cpp:425] relu1_2 <- conv1_2
I0422 07:02:42.196820  2460 net.cpp:386] relu1_2 -> conv1_2 (in-place)
I0422 07:02:42.197067  2460 net.cpp:141] Setting up relu1_2
I0422 07:02:42.197083  2460 net.cpp:148] Top shape: 1 64 573 698 (25597056)
I0422 07:02:42.197088  2460 net.cpp:156] Memory required for data: 417052896
I0422 07:02:42.197095  2460 layer_factory.hpp:77] Creating layer pool1
I0422 07:02:42.197111  2460 net.cpp:91] Creating Layer pool1
I0422 07:02:42.197124  2460 net.cpp:425] pool1 <- conv1_2
I0422 07:02:42.197134  2460 net.cpp:399] pool1 -> pool1
I0422 07:02:42.197188  2460 net.cpp:141] Setting up pool1
I0422 07:02:42.197199  2460 net.cpp:148] Top shape: 1 64 287 349 (6410432)
I0422 07:02:42.197204  2460 net.cpp:156] Memory required for data: 442694624
I0422 07:02:42.197211  2460 layer_factory.hpp:77] Creating layer conv2_1
I0422 07:02:42.197223  2460 net.cpp:91] Creating Layer conv2_1
I0422 07:02:42.197229  2460 net.cpp:425] conv2_1 <- pool1
I0422 07:02:42.197239  2460 net.cpp:399] conv2_1 -> conv2_1
I0422 07:02:42.198721  2460 net.cpp:141] Setting up conv2_1
I0422 07:02:42.198738  2460 net.cpp:148] Top shape: 1 128 287 349 (12820864)
I0422 07:02:42.198745  2460 net.cpp:156] Memory required for data: 493978080
I0422 07:02:42.198757  2460 layer_factory.hpp:77] Creating layer relu2_1
I0422 07:02:42.198770  2460 net.cpp:91] Creating Layer relu2_1
I0422 07:02:42.198777  2460 net.cpp:425] relu2_1 <- conv2_1
I0422 07:02:42.198796  2460 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0422 07:02:42.199043  2460 net.cpp:141] Setting up relu2_1
I0422 07:02:42.199057  2460 net.cpp:148] Top shape: 1 128 287 349 (12820864)
I0422 07:02:42.199064  2460 net.cpp:156] Memory required for data: 545261536
I0422 07:02:42.199070  2460 layer_factory.hpp:77] Creating layer conv2_2
I0422 07:02:42.199090  2460 net.cpp:91] Creating Layer conv2_2
I0422 07:02:42.199097  2460 net.cpp:425] conv2_2 <- conv2_1
I0422 07:02:42.199107  2460 net.cpp:399] conv2_2 -> conv2_2
I0422 07:02:42.200743  2460 net.cpp:141] Setting up conv2_2
I0422 07:02:42.200764  2460 net.cpp:148] Top shape: 1 128 287 349 (12820864)
I0422 07:02:42.200770  2460 net.cpp:156] Memory required for data: 596544992
I0422 07:02:42.200780  2460 layer_factory.hpp:77] Creating layer relu2_2
I0422 07:02:42.200798  2460 net.cpp:91] Creating Layer relu2_2
I0422 07:02:42.200804  2460 net.cpp:425] relu2_2 <- conv2_2
I0422 07:02:42.200812  2460 net.cpp:386] relu2_2 -> conv2_2 (in-place)
I0422 07:02:42.200968  2460 net.cpp:141] Setting up relu2_2
I0422 07:02:42.200981  2460 net.cpp:148] Top shape: 1 128 287 349 (12820864)
I0422 07:02:42.200989  2460 net.cpp:156] Memory required for data: 647828448
I0422 07:02:42.200995  2460 layer_factory.hpp:77] Creating layer pool2
I0422 07:02:42.201004  2460 net.cpp:91] Creating Layer pool2
I0422 07:02:42.201010  2460 net.cpp:425] pool2 <- conv2_2
I0422 07:02:42.201019  2460 net.cpp:399] pool2 -> pool2
I0422 07:02:42.201067  2460 net.cpp:141] Setting up pool2
I0422 07:02:42.201078  2460 net.cpp:148] Top shape: 1 128 144 175 (3225600)
I0422 07:02:42.201084  2460 net.cpp:156] Memory required for data: 660730848
I0422 07:02:42.201093  2460 layer_factory.hpp:77] Creating layer conv3_1
I0422 07:02:42.201107  2460 net.cpp:91] Creating Layer conv3_1
I0422 07:02:42.201112  2460 net.cpp:425] conv3_1 <- pool2
I0422 07:02:42.201122  2460 net.cpp:399] conv3_1 -> conv3_1
I0422 07:02:42.202839  2460 net.cpp:141] Setting up conv3_1
I0422 07:02:42.202857  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.202863  2460 net.cpp:156] Memory required for data: 686535648
I0422 07:02:42.202877  2460 layer_factory.hpp:77] Creating layer relu3_1
I0422 07:02:42.202885  2460 net.cpp:91] Creating Layer relu3_1
I0422 07:02:42.202893  2460 net.cpp:425] relu3_1 <- conv3_1
I0422 07:02:42.202901  2460 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0422 07:02:42.203161  2460 net.cpp:141] Setting up relu3_1
I0422 07:02:42.203176  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.203181  2460 net.cpp:156] Memory required for data: 712340448
I0422 07:02:42.203187  2460 layer_factory.hpp:77] Creating layer conv3_2
I0422 07:02:42.203207  2460 net.cpp:91] Creating Layer conv3_2
I0422 07:02:42.203212  2460 net.cpp:425] conv3_2 <- conv3_1
I0422 07:02:42.203222  2460 net.cpp:399] conv3_2 -> conv3_2
I0422 07:02:42.205696  2460 net.cpp:141] Setting up conv3_2
I0422 07:02:42.205729  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.205734  2460 net.cpp:156] Memory required for data: 738145248
I0422 07:02:42.205745  2460 layer_factory.hpp:77] Creating layer relu3_2
I0422 07:02:42.205760  2460 net.cpp:91] Creating Layer relu3_2
I0422 07:02:42.205766  2460 net.cpp:425] relu3_2 <- conv3_2
I0422 07:02:42.205775  2460 net.cpp:386] relu3_2 -> conv3_2 (in-place)
I0422 07:02:42.206049  2460 net.cpp:141] Setting up relu3_2
I0422 07:02:42.206064  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.206070  2460 net.cpp:156] Memory required for data: 763950048
I0422 07:02:42.206076  2460 layer_factory.hpp:77] Creating layer conv3_3
I0422 07:02:42.206095  2460 net.cpp:91] Creating Layer conv3_3
I0422 07:02:42.206102  2460 net.cpp:425] conv3_3 <- conv3_2
I0422 07:02:42.206113  2460 net.cpp:399] conv3_3 -> conv3_3
I0422 07:02:42.208647  2460 net.cpp:141] Setting up conv3_3
I0422 07:02:42.208673  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.208679  2460 net.cpp:156] Memory required for data: 789754848
I0422 07:02:42.208690  2460 layer_factory.hpp:77] Creating layer relu3_3
I0422 07:02:42.208711  2460 net.cpp:91] Creating Layer relu3_3
I0422 07:02:42.208719  2460 net.cpp:425] relu3_3 <- conv3_3
I0422 07:02:42.208727  2460 net.cpp:386] relu3_3 -> conv3_3 (in-place)
I0422 07:02:42.208884  2460 net.cpp:141] Setting up relu3_3
I0422 07:02:42.208897  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.208902  2460 net.cpp:156] Memory required for data: 815559648
I0422 07:02:42.208917  2460 layer_factory.hpp:77] Creating layer pool3
I0422 07:02:42.208930  2460 net.cpp:91] Creating Layer pool3
I0422 07:02:42.208937  2460 net.cpp:425] pool3 <- conv3_3
I0422 07:02:42.208945  2460 net.cpp:399] pool3 -> pool3
I0422 07:02:42.209005  2460 net.cpp:141] Setting up pool3
I0422 07:02:42.209017  2460 net.cpp:148] Top shape: 1 256 72 88 (1622016)
I0422 07:02:42.209022  2460 net.cpp:156] Memory required for data: 822047712
I0422 07:02:42.209028  2460 layer_factory.hpp:77] Creating layer conv4_1
I0422 07:02:42.209043  2460 net.cpp:91] Creating Layer conv4_1
I0422 07:02:42.209048  2460 net.cpp:425] conv4_1 <- pool3
I0422 07:02:42.209059  2460 net.cpp:399] conv4_1 -> conv4_1
I0422 07:02:42.214362  2460 net.cpp:141] Setting up conv4_1
I0422 07:02:42.214411  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.214416  2460 net.cpp:156] Memory required for data: 835023840
I0422 07:02:42.214429  2460 layer_factory.hpp:77] Creating layer relu4_1
I0422 07:02:42.214443  2460 net.cpp:91] Creating Layer relu4_1
I0422 07:02:42.214450  2460 net.cpp:425] relu4_1 <- conv4_1
I0422 07:02:42.214469  2460 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0422 07:02:42.214725  2460 net.cpp:141] Setting up relu4_1
I0422 07:02:42.214741  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.214747  2460 net.cpp:156] Memory required for data: 847999968
I0422 07:02:42.214753  2460 layer_factory.hpp:77] Creating layer conv4_2
I0422 07:02:42.214774  2460 net.cpp:91] Creating Layer conv4_2
I0422 07:02:42.214783  2460 net.cpp:425] conv4_2 <- conv4_1
I0422 07:02:42.214792  2460 net.cpp:399] conv4_2 -> conv4_2
I0422 07:02:42.222080  2460 net.cpp:141] Setting up conv4_2
I0422 07:02:42.222142  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.222149  2460 net.cpp:156] Memory required for data: 860976096
I0422 07:02:42.222182  2460 layer_factory.hpp:77] Creating layer relu4_2
I0422 07:02:42.222199  2460 net.cpp:91] Creating Layer relu4_2
I0422 07:02:42.222208  2460 net.cpp:425] relu4_2 <- conv4_2
I0422 07:02:42.222218  2460 net.cpp:386] relu4_2 -> conv4_2 (in-place)
I0422 07:02:42.222494  2460 net.cpp:141] Setting up relu4_2
I0422 07:02:42.222515  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.222522  2460 net.cpp:156] Memory required for data: 873952224
I0422 07:02:42.222528  2460 layer_factory.hpp:77] Creating layer conv4_3
I0422 07:02:42.222546  2460 net.cpp:91] Creating Layer conv4_3
I0422 07:02:42.222553  2460 net.cpp:425] conv4_3 <- conv4_2
I0422 07:02:42.222564  2460 net.cpp:399] conv4_3 -> conv4_3
I0422 07:02:42.229352  2460 net.cpp:141] Setting up conv4_3
I0422 07:02:42.229382  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.229387  2460 net.cpp:156] Memory required for data: 886928352
I0422 07:02:42.229398  2460 layer_factory.hpp:77] Creating layer relu4_3
I0422 07:02:42.229408  2460 net.cpp:91] Creating Layer relu4_3
I0422 07:02:42.229414  2460 net.cpp:425] relu4_3 <- conv4_3
I0422 07:02:42.229429  2460 net.cpp:386] relu4_3 -> conv4_3 (in-place)
I0422 07:02:42.229589  2460 net.cpp:141] Setting up relu4_3
I0422 07:02:42.229602  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.229609  2460 net.cpp:156] Memory required for data: 899904480
I0422 07:02:42.229614  2460 layer_factory.hpp:77] Creating layer pool4
I0422 07:02:42.229626  2460 net.cpp:91] Creating Layer pool4
I0422 07:02:42.229632  2460 net.cpp:425] pool4 <- conv4_3
I0422 07:02:42.229642  2460 net.cpp:399] pool4 -> pool4
I0422 07:02:42.229698  2460 net.cpp:141] Setting up pool4
I0422 07:02:42.229709  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.229725  2460 net.cpp:156] Memory required for data: 903148512
I0422 07:02:42.229732  2460 layer_factory.hpp:77] Creating layer conv5_1
I0422 07:02:42.229748  2460 net.cpp:91] Creating Layer conv5_1
I0422 07:02:42.229753  2460 net.cpp:425] conv5_1 <- pool4
I0422 07:02:42.229761  2460 net.cpp:399] conv5_1 -> conv5_1
I0422 07:02:42.236232  2460 net.cpp:141] Setting up conv5_1
I0422 07:02:42.236263  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.236269  2460 net.cpp:156] Memory required for data: 906392544
I0422 07:02:42.236280  2460 layer_factory.hpp:77] Creating layer relu5_1
I0422 07:02:42.236291  2460 net.cpp:91] Creating Layer relu5_1
I0422 07:02:42.236299  2460 net.cpp:425] relu5_1 <- conv5_1
I0422 07:02:42.236310  2460 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0422 07:02:42.236562  2460 net.cpp:141] Setting up relu5_1
I0422 07:02:42.236577  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.236583  2460 net.cpp:156] Memory required for data: 909636576
I0422 07:02:42.236595  2460 layer_factory.hpp:77] Creating layer conv5_2
I0422 07:02:42.236611  2460 net.cpp:91] Creating Layer conv5_2
I0422 07:02:42.236618  2460 net.cpp:425] conv5_2 <- conv5_1
I0422 07:02:42.236627  2460 net.cpp:399] conv5_2 -> conv5_2
I0422 07:02:42.243769  2460 net.cpp:141] Setting up conv5_2
I0422 07:02:42.243831  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.243846  2460 net.cpp:156] Memory required for data: 912880608
I0422 07:02:42.243862  2460 layer_factory.hpp:77] Creating layer relu5_2
I0422 07:02:42.243880  2460 net.cpp:91] Creating Layer relu5_2
I0422 07:02:42.243890  2460 net.cpp:425] relu5_2 <- conv5_2
I0422 07:02:42.243911  2460 net.cpp:386] relu5_2 -> conv5_2 (in-place)
I0422 07:02:42.244180  2460 net.cpp:141] Setting up relu5_2
I0422 07:02:42.244196  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.244205  2460 net.cpp:156] Memory required for data: 916124640
I0422 07:02:42.244211  2460 layer_factory.hpp:77] Creating layer conv5_3
I0422 07:02:42.244230  2460 net.cpp:91] Creating Layer conv5_3
I0422 07:02:42.244236  2460 net.cpp:425] conv5_3 <- conv5_2
I0422 07:02:42.244247  2460 net.cpp:399] conv5_3 -> conv5_3
I0422 07:02:42.251323  2460 net.cpp:141] Setting up conv5_3
I0422 07:02:42.251376  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.251384  2460 net.cpp:156] Memory required for data: 919368672
I0422 07:02:42.251401  2460 layer_factory.hpp:77] Creating layer relu5_3
I0422 07:02:42.251418  2460 net.cpp:91] Creating Layer relu5_3
I0422 07:02:42.251426  2460 net.cpp:425] relu5_3 <- conv5_3
I0422 07:02:42.251437  2460 net.cpp:386] relu5_3 -> conv5_3 (in-place)
I0422 07:02:42.251710  2460 net.cpp:141] Setting up relu5_3
I0422 07:02:42.251727  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.251734  2460 net.cpp:156] Memory required for data: 922612704
I0422 07:02:42.251739  2460 layer_factory.hpp:77] Creating layer pool5
I0422 07:02:42.251760  2460 net.cpp:91] Creating Layer pool5
I0422 07:02:42.251766  2460 net.cpp:425] pool5 <- conv5_3
I0422 07:02:42.251777  2460 net.cpp:399] pool5 -> pool5
I0422 07:02:42.251837  2460 net.cpp:141] Setting up pool5
I0422 07:02:42.251849  2460 net.cpp:148] Top shape: 1 512 18 22 (202752)
I0422 07:02:42.251854  2460 net.cpp:156] Memory required for data: 923423712
I0422 07:02:42.251860  2460 layer_factory.hpp:77] Creating layer fc6
I0422 07:02:42.251878  2460 net.cpp:91] Creating Layer fc6
I0422 07:02:42.251883  2460 net.cpp:425] fc6 <- pool5
I0422 07:02:42.251894  2460 net.cpp:399] fc6 -> fc6
I0422 07:02:42.529201  2460 net.cpp:141] Setting up fc6
I0422 07:02:42.529269  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.529276  2460 net.cpp:156] Memory required for data: 926569440
I0422 07:02:42.529294  2460 layer_factory.hpp:77] Creating layer relu6
I0422 07:02:42.529310  2460 net.cpp:91] Creating Layer relu6
I0422 07:02:42.529320  2460 net.cpp:425] relu6 <- fc6
I0422 07:02:42.529332  2460 net.cpp:386] relu6 -> fc6 (in-place)
I0422 07:02:42.529602  2460 net.cpp:141] Setting up relu6
I0422 07:02:42.529629  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.529635  2460 net.cpp:156] Memory required for data: 929715168
I0422 07:02:42.529642  2460 layer_factory.hpp:77] Creating layer drop6
I0422 07:02:42.529662  2460 net.cpp:91] Creating Layer drop6
I0422 07:02:42.529670  2460 net.cpp:425] drop6 <- fc6
I0422 07:02:42.529680  2460 net.cpp:386] drop6 -> fc6 (in-place)
I0422 07:02:42.529718  2460 net.cpp:141] Setting up drop6
I0422 07:02:42.529729  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.529736  2460 net.cpp:156] Memory required for data: 932860896
I0422 07:02:42.529741  2460 layer_factory.hpp:77] Creating layer fc7
I0422 07:02:42.529758  2460 net.cpp:91] Creating Layer fc7
I0422 07:02:42.529764  2460 net.cpp:425] fc7 <- fc6
I0422 07:02:42.529774  2460 net.cpp:399] fc7 -> fc7
I0422 07:02:42.577535  2460 net.cpp:141] Setting up fc7
I0422 07:02:42.577608  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.577616  2460 net.cpp:156] Memory required for data: 936006624
I0422 07:02:42.577644  2460 layer_factory.hpp:77] Creating layer relu7
I0422 07:02:42.577661  2460 net.cpp:91] Creating Layer relu7
I0422 07:02:42.577669  2460 net.cpp:425] relu7 <- fc7
I0422 07:02:42.577680  2460 net.cpp:386] relu7 -> fc7 (in-place)
I0422 07:02:42.577950  2460 net.cpp:141] Setting up relu7
I0422 07:02:42.577965  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.577972  2460 net.cpp:156] Memory required for data: 939152352
I0422 07:02:42.577978  2460 layer_factory.hpp:77] Creating layer drop7
I0422 07:02:42.577991  2460 net.cpp:91] Creating Layer drop7
I0422 07:02:42.577996  2460 net.cpp:425] drop7 <- fc7
I0422 07:02:42.578006  2460 net.cpp:386] drop7 -> fc7 (in-place)
I0422 07:02:42.578049  2460 net.cpp:141] Setting up drop7
I0422 07:02:42.578060  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.578065  2460 net.cpp:156] Memory required for data: 942298080
I0422 07:02:42.578071  2460 layer_factory.hpp:77] Creating layer score_fr
I0422 07:02:42.578088  2460 net.cpp:91] Creating Layer score_fr
I0422 07:02:42.578094  2460 net.cpp:425] score_fr <- fc7
I0422 07:02:42.578102  2460 net.cpp:399] score_fr -> score_fr
I0422 07:02:42.579186  2460 net.cpp:141] Setting up score_fr
I0422 07:02:42.579205  2460 net.cpp:148] Top shape: 1 21 12 16 (4032)
I0422 07:02:42.579210  2460 net.cpp:156] Memory required for data: 942314208
I0422 07:02:42.579221  2460 layer_factory.hpp:77] Creating layer upscore
I0422 07:02:42.579241  2460 net.cpp:91] Creating Layer upscore
I0422 07:02:42.579246  2460 net.cpp:425] upscore <- score_fr
I0422 07:02:42.579257  2460 net.cpp:399] upscore -> upscore
I0422 07:02:42.584273  2460 net.cpp:141] Setting up upscore
I0422 07:02:42.584331  2460 net.cpp:148] Top shape: 1 21 416 544 (4752384)
I0422 07:02:42.584337  2460 net.cpp:156] Memory required for data: 961323744
I0422 07:02:42.584360  2460 layer_factory.hpp:77] Creating layer score
I0422 07:02:42.584381  2460 net.cpp:91] Creating Layer score
I0422 07:02:42.584389  2460 net.cpp:425] score <- upscore
I0422 07:02:42.584398  2460 net.cpp:425] score <- data_data_0_split_1
I0422 07:02:42.584408  2460 net.cpp:399] score -> score
I0422 07:02:42.584471  2460 net.cpp:141] Setting up score
I0422 07:02:42.584487  2460 net.cpp:148] Top shape: 1 21 375 500 (3937500)
I0422 07:02:42.584493  2460 net.cpp:156] Memory required for data: 977073744
I0422 07:02:42.584511  2460 layer_factory.hpp:77] Creating layer loss
I0422 07:02:42.584525  2460 net.cpp:91] Creating Layer loss
I0422 07:02:42.584532  2460 net.cpp:425] loss <- score
I0422 07:02:42.584539  2460 net.cpp:425] loss <- label
I0422 07:02:42.584547  2460 net.cpp:399] loss -> loss
I0422 07:02:42.584566  2460 layer_factory.hpp:77] Creating layer loss
I0422 07:02:42.594856  2460 net.cpp:141] Setting up loss
I0422 07:02:42.594911  2460 net.cpp:148] Top shape: (1)
I0422 07:02:42.594918  2460 net.cpp:151]     with loss weight 1
I0422 07:02:42.594944  2460 net.cpp:156] Memory required for data: 977073748
I0422 07:02:42.594962  2460 net.cpp:217] loss needs backward computation.
I0422 07:02:42.594982  2460 net.cpp:217] score needs backward computation.
I0422 07:02:42.594988  2460 net.cpp:217] upscore needs backward computation.
I0422 07:02:42.594995  2460 net.cpp:217] score_fr needs backward computation.
I0422 07:02:42.595001  2460 net.cpp:217] drop7 needs backward computation.
I0422 07:02:42.595006  2460 net.cpp:217] relu7 needs backward computation.
I0422 07:02:42.595011  2460 net.cpp:217] fc7 needs backward computation.
I0422 07:02:42.595017  2460 net.cpp:217] drop6 needs backward computation.
I0422 07:02:42.595023  2460 net.cpp:217] relu6 needs backward computation.
I0422 07:02:42.595036  2460 net.cpp:217] fc6 needs backward computation.
I0422 07:02:42.595041  2460 net.cpp:217] pool5 needs backward computation.
I0422 07:02:42.595047  2460 net.cpp:217] relu5_3 needs backward computation.
I0422 07:02:42.595052  2460 net.cpp:217] conv5_3 needs backward computation.
I0422 07:02:42.595058  2460 net.cpp:217] relu5_2 needs backward computation.
I0422 07:02:42.595064  2460 net.cpp:217] conv5_2 needs backward computation.
I0422 07:02:42.595069  2460 net.cpp:217] relu5_1 needs backward computation.
I0422 07:02:42.595074  2460 net.cpp:217] conv5_1 needs backward computation.
I0422 07:02:42.595080  2460 net.cpp:217] pool4 needs backward computation.
I0422 07:02:42.595087  2460 net.cpp:217] relu4_3 needs backward computation.
I0422 07:02:42.595091  2460 net.cpp:217] conv4_3 needs backward computation.
I0422 07:02:42.595098  2460 net.cpp:217] relu4_2 needs backward computation.
I0422 07:02:42.595103  2460 net.cpp:217] conv4_2 needs backward computation.
I0422 07:02:42.595108  2460 net.cpp:217] relu4_1 needs backward computation.
I0422 07:02:42.595114  2460 net.cpp:217] conv4_1 needs backward computation.
I0422 07:02:42.595120  2460 net.cpp:217] pool3 needs backward computation.
I0422 07:02:42.595125  2460 net.cpp:217] relu3_3 needs backward computation.
I0422 07:02:42.595131  2460 net.cpp:217] conv3_3 needs backward computation.
I0422 07:02:42.595136  2460 net.cpp:217] relu3_2 needs backward computation.
I0422 07:02:42.595142  2460 net.cpp:217] conv3_2 needs backward computation.
I0422 07:02:42.595147  2460 net.cpp:217] relu3_1 needs backward computation.
I0422 07:02:42.595152  2460 net.cpp:217] conv3_1 needs backward computation.
I0422 07:02:42.595158  2460 net.cpp:217] pool2 needs backward computation.
I0422 07:02:42.595165  2460 net.cpp:217] relu2_2 needs backward computation.
I0422 07:02:42.595170  2460 net.cpp:217] conv2_2 needs backward computation.
I0422 07:02:42.595175  2460 net.cpp:217] relu2_1 needs backward computation.
I0422 07:02:42.595180  2460 net.cpp:217] conv2_1 needs backward computation.
I0422 07:02:42.595185  2460 net.cpp:217] pool1 needs backward computation.
I0422 07:02:42.595191  2460 net.cpp:217] relu1_2 needs backward computation.
I0422 07:02:42.595196  2460 net.cpp:217] conv1_2 needs backward computation.
I0422 07:02:42.595201  2460 net.cpp:217] relu1_1 needs backward computation.
I0422 07:02:42.595207  2460 net.cpp:217] conv1_1 needs backward computation.
I0422 07:02:42.595213  2460 net.cpp:219] data_data_0_split does not need backward computation.
I0422 07:02:42.595221  2460 net.cpp:219] data does not need backward computation.
I0422 07:02:42.595226  2460 net.cpp:261] This network produces output loss
I0422 07:02:42.595259  2460 net.cpp:274] Network initialization done.
I0422 07:02:42.596153  2460 solver.cpp:181] Creating test net (#0) specified by test_net file: /home/ubuntu/pynb/caffe-future/train/config/train.prototxt
I0422 07:02:42.596460  2460 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  python_param {
    module: "voc_layers"
    layer: "SBDDSegDataLayer"
    param_str: "{\'sbdd_dir\': \'../caffe-future/train/dataset\', \'seed\': 1337, \'split\': \'train\', \'mean\': (104.00699, 116.66877, 122.67892)}"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 100
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_3"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_3"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    pad: 0
    kernel_size: 7
    stride: 1
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "Convolution"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score_fr"
  type: "Convolution"
  bottom: "fc7"
  top: "score_fr"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 21
    pad: 0
    kernel_size: 1
  }
}
layer {
  name: "upscore"
  type: "Deconvolution"
  bottom: "score_fr"
  top: "upscore"
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 21
    bias_term: false
    kernel_size: 64
    stride: 32
  }
}
layer {
  name: "score"
  type: "Crop"
  bottom: "upscore"
  bottom: "data"
  top: "score"
  crop_param {
    axis: 2
    offset: 19
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
  loss_param {
    ignore_label: 255
    normalize: false
  }
}
I0422 07:02:42.596639  2460 layer_factory.hpp:77] Creating layer data
I0422 07:02:42.596730  2460 net.cpp:91] Creating Layer data
I0422 07:02:42.596741  2460 net.cpp:399] data -> data
I0422 07:02:42.596753  2460 net.cpp:399] data -> label
I0422 07:02:42.607182  2460 net.cpp:141] Setting up data
I0422 07:02:42.607221  2460 net.cpp:148] Top shape: 1 3 375 500 (562500)
I0422 07:02:42.607230  2460 net.cpp:148] Top shape: 1 1 375 500 (187500)
I0422 07:02:42.607236  2460 net.cpp:156] Memory required for data: 3000000
I0422 07:02:42.607245  2460 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 07:02:42.607262  2460 net.cpp:91] Creating Layer data_data_0_split
I0422 07:02:42.607270  2460 net.cpp:425] data_data_0_split <- data
I0422 07:02:42.607280  2460 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 07:02:42.607295  2460 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 07:02:42.607344  2460 net.cpp:141] Setting up data_data_0_split
I0422 07:02:42.607357  2460 net.cpp:148] Top shape: 1 3 375 500 (562500)
I0422 07:02:42.607363  2460 net.cpp:148] Top shape: 1 3 375 500 (562500)
I0422 07:02:42.607368  2460 net.cpp:156] Memory required for data: 7500000
I0422 07:02:42.607374  2460 layer_factory.hpp:77] Creating layer conv1_1
I0422 07:02:42.607398  2460 net.cpp:91] Creating Layer conv1_1
I0422 07:02:42.607405  2460 net.cpp:425] conv1_1 <- data_data_0_split_0
I0422 07:02:42.607424  2460 net.cpp:399] conv1_1 -> conv1_1
I0422 07:02:42.609688  2460 net.cpp:141] Setting up conv1_1
I0422 07:02:42.609724  2460 net.cpp:148] Top shape: 1 64 573 698 (25597056)
I0422 07:02:42.609730  2460 net.cpp:156] Memory required for data: 109888224
I0422 07:02:42.609746  2460 layer_factory.hpp:77] Creating layer relu1_1
I0422 07:02:42.609758  2460 net.cpp:91] Creating Layer relu1_1
I0422 07:02:42.609766  2460 net.cpp:425] relu1_1 <- conv1_1
I0422 07:02:42.609773  2460 net.cpp:386] relu1_1 -> conv1_1 (in-place)
I0422 07:02:42.610049  2460 net.cpp:141] Setting up relu1_1
I0422 07:02:42.610065  2460 net.cpp:148] Top shape: 1 64 573 698 (25597056)
I0422 07:02:42.610071  2460 net.cpp:156] Memory required for data: 212276448
I0422 07:02:42.610077  2460 layer_factory.hpp:77] Creating layer conv1_2
I0422 07:02:42.610092  2460 net.cpp:91] Creating Layer conv1_2
I0422 07:02:42.610098  2460 net.cpp:425] conv1_2 <- conv1_1
I0422 07:02:42.610107  2460 net.cpp:399] conv1_2 -> conv1_2
I0422 07:02:42.612041  2460 net.cpp:141] Setting up conv1_2
I0422 07:02:42.612062  2460 net.cpp:148] Top shape: 1 64 573 698 (25597056)
I0422 07:02:42.612069  2460 net.cpp:156] Memory required for data: 314664672
I0422 07:02:42.612082  2460 layer_factory.hpp:77] Creating layer relu1_2
I0422 07:02:42.612093  2460 net.cpp:91] Creating Layer relu1_2
I0422 07:02:42.612102  2460 net.cpp:425] relu1_2 <- conv1_2
I0422 07:02:42.612115  2460 net.cpp:386] relu1_2 -> conv1_2 (in-place)
I0422 07:02:42.612414  2460 net.cpp:141] Setting up relu1_2
I0422 07:02:42.612432  2460 net.cpp:148] Top shape: 1 64 573 698 (25597056)
I0422 07:02:42.612437  2460 net.cpp:156] Memory required for data: 417052896
I0422 07:02:42.612443  2460 layer_factory.hpp:77] Creating layer pool1
I0422 07:02:42.612455  2460 net.cpp:91] Creating Layer pool1
I0422 07:02:42.612462  2460 net.cpp:425] pool1 <- conv1_2
I0422 07:02:42.612470  2460 net.cpp:399] pool1 -> pool1
I0422 07:02:42.612529  2460 net.cpp:141] Setting up pool1
I0422 07:02:42.612541  2460 net.cpp:148] Top shape: 1 64 287 349 (6410432)
I0422 07:02:42.612547  2460 net.cpp:156] Memory required for data: 442694624
I0422 07:02:42.612553  2460 layer_factory.hpp:77] Creating layer conv2_1
I0422 07:02:42.612565  2460 net.cpp:91] Creating Layer conv2_1
I0422 07:02:42.612572  2460 net.cpp:425] conv2_1 <- pool1
I0422 07:02:42.612581  2460 net.cpp:399] conv2_1 -> conv2_1
I0422 07:02:42.614166  2460 net.cpp:141] Setting up conv2_1
I0422 07:02:42.614193  2460 net.cpp:148] Top shape: 1 128 287 349 (12820864)
I0422 07:02:42.614200  2460 net.cpp:156] Memory required for data: 493978080
I0422 07:02:42.614213  2460 layer_factory.hpp:77] Creating layer relu2_1
I0422 07:02:42.614225  2460 net.cpp:91] Creating Layer relu2_1
I0422 07:02:42.614239  2460 net.cpp:425] relu2_1 <- conv2_1
I0422 07:02:42.614248  2460 net.cpp:386] relu2_1 -> conv2_1 (in-place)
I0422 07:02:42.614403  2460 net.cpp:141] Setting up relu2_1
I0422 07:02:42.614418  2460 net.cpp:148] Top shape: 1 128 287 349 (12820864)
I0422 07:02:42.614424  2460 net.cpp:156] Memory required for data: 545261536
I0422 07:02:42.614435  2460 layer_factory.hpp:77] Creating layer conv2_2
I0422 07:02:42.614451  2460 net.cpp:91] Creating Layer conv2_2
I0422 07:02:42.614464  2460 net.cpp:425] conv2_2 <- conv2_1
I0422 07:02:42.614476  2460 net.cpp:399] conv2_2 -> conv2_2
I0422 07:02:42.616305  2460 net.cpp:141] Setting up conv2_2
I0422 07:02:42.616328  2460 net.cpp:148] Top shape: 1 128 287 349 (12820864)
I0422 07:02:42.616334  2460 net.cpp:156] Memory required for data: 596544992
I0422 07:02:42.616344  2460 layer_factory.hpp:77] Creating layer relu2_2
I0422 07:02:42.616354  2460 net.cpp:91] Creating Layer relu2_2
I0422 07:02:42.616361  2460 net.cpp:425] relu2_2 <- conv2_2
I0422 07:02:42.616369  2460 net.cpp:386] relu2_2 -> conv2_2 (in-place)
I0422 07:02:42.616639  2460 net.cpp:141] Setting up relu2_2
I0422 07:02:42.616655  2460 net.cpp:148] Top shape: 1 128 287 349 (12820864)
I0422 07:02:42.616662  2460 net.cpp:156] Memory required for data: 647828448
I0422 07:02:42.616679  2460 layer_factory.hpp:77] Creating layer pool2
I0422 07:02:42.616691  2460 net.cpp:91] Creating Layer pool2
I0422 07:02:42.616698  2460 net.cpp:425] pool2 <- conv2_2
I0422 07:02:42.616706  2460 net.cpp:399] pool2 -> pool2
I0422 07:02:42.616771  2460 net.cpp:141] Setting up pool2
I0422 07:02:42.616786  2460 net.cpp:148] Top shape: 1 128 144 175 (3225600)
I0422 07:02:42.616797  2460 net.cpp:156] Memory required for data: 660730848
I0422 07:02:42.616806  2460 layer_factory.hpp:77] Creating layer conv3_1
I0422 07:02:42.616829  2460 net.cpp:91] Creating Layer conv3_1
I0422 07:02:42.616835  2460 net.cpp:425] conv3_1 <- pool2
I0422 07:02:42.616844  2460 net.cpp:399] conv3_1 -> conv3_1
I0422 07:02:42.618619  2460 net.cpp:141] Setting up conv3_1
I0422 07:02:42.618638  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.618650  2460 net.cpp:156] Memory required for data: 686535648
I0422 07:02:42.618664  2460 layer_factory.hpp:77] Creating layer relu3_1
I0422 07:02:42.618675  2460 net.cpp:91] Creating Layer relu3_1
I0422 07:02:42.618681  2460 net.cpp:425] relu3_1 <- conv3_1
I0422 07:02:42.618690  2460 net.cpp:386] relu3_1 -> conv3_1 (in-place)
I0422 07:02:42.618966  2460 net.cpp:141] Setting up relu3_1
I0422 07:02:42.618983  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.618988  2460 net.cpp:156] Memory required for data: 712340448
I0422 07:02:42.618994  2460 layer_factory.hpp:77] Creating layer conv3_2
I0422 07:02:42.619007  2460 net.cpp:91] Creating Layer conv3_2
I0422 07:02:42.619014  2460 net.cpp:425] conv3_2 <- conv3_1
I0422 07:02:42.619032  2460 net.cpp:399] conv3_2 -> conv3_2
I0422 07:02:42.621529  2460 net.cpp:141] Setting up conv3_2
I0422 07:02:42.621548  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.621556  2460 net.cpp:156] Memory required for data: 738145248
I0422 07:02:42.621565  2460 layer_factory.hpp:77] Creating layer relu3_2
I0422 07:02:42.621575  2460 net.cpp:91] Creating Layer relu3_2
I0422 07:02:42.621582  2460 net.cpp:425] relu3_2 <- conv3_2
I0422 07:02:42.621590  2460 net.cpp:386] relu3_2 -> conv3_2 (in-place)
I0422 07:02:42.621750  2460 net.cpp:141] Setting up relu3_2
I0422 07:02:42.621770  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.621775  2460 net.cpp:156] Memory required for data: 763950048
I0422 07:02:42.621783  2460 layer_factory.hpp:77] Creating layer conv3_3
I0422 07:02:42.621798  2460 net.cpp:91] Creating Layer conv3_3
I0422 07:02:42.621803  2460 net.cpp:425] conv3_3 <- conv3_2
I0422 07:02:42.621814  2460 net.cpp:399] conv3_3 -> conv3_3
I0422 07:02:42.624296  2460 net.cpp:141] Setting up conv3_3
I0422 07:02:42.624315  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.624320  2460 net.cpp:156] Memory required for data: 789754848
I0422 07:02:42.624330  2460 layer_factory.hpp:77] Creating layer relu3_3
I0422 07:02:42.624341  2460 net.cpp:91] Creating Layer relu3_3
I0422 07:02:42.624346  2460 net.cpp:425] relu3_3 <- conv3_3
I0422 07:02:42.624354  2460 net.cpp:386] relu3_3 -> conv3_3 (in-place)
I0422 07:02:42.624614  2460 net.cpp:141] Setting up relu3_3
I0422 07:02:42.624630  2460 net.cpp:148] Top shape: 1 256 144 175 (6451200)
I0422 07:02:42.624636  2460 net.cpp:156] Memory required for data: 815559648
I0422 07:02:42.624644  2460 layer_factory.hpp:77] Creating layer pool3
I0422 07:02:42.624655  2460 net.cpp:91] Creating Layer pool3
I0422 07:02:42.624660  2460 net.cpp:425] pool3 <- conv3_3
I0422 07:02:42.624670  2460 net.cpp:399] pool3 -> pool3
I0422 07:02:42.624729  2460 net.cpp:141] Setting up pool3
I0422 07:02:42.624740  2460 net.cpp:148] Top shape: 1 256 72 88 (1622016)
I0422 07:02:42.624747  2460 net.cpp:156] Memory required for data: 822047712
I0422 07:02:42.624752  2460 layer_factory.hpp:77] Creating layer conv4_1
I0422 07:02:42.624764  2460 net.cpp:91] Creating Layer conv4_1
I0422 07:02:42.624770  2460 net.cpp:425] conv4_1 <- pool3
I0422 07:02:42.624779  2460 net.cpp:399] conv4_1 -> conv4_1
I0422 07:02:42.628486  2460 net.cpp:141] Setting up conv4_1
I0422 07:02:42.628505  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.628521  2460 net.cpp:156] Memory required for data: 835023840
I0422 07:02:42.628532  2460 layer_factory.hpp:77] Creating layer relu4_1
I0422 07:02:42.628541  2460 net.cpp:91] Creating Layer relu4_1
I0422 07:02:42.628548  2460 net.cpp:425] relu4_1 <- conv4_1
I0422 07:02:42.628556  2460 net.cpp:386] relu4_1 -> conv4_1 (in-place)
I0422 07:02:42.628808  2460 net.cpp:141] Setting up relu4_1
I0422 07:02:42.628824  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.628830  2460 net.cpp:156] Memory required for data: 847999968
I0422 07:02:42.628837  2460 layer_factory.hpp:77] Creating layer conv4_2
I0422 07:02:42.628850  2460 net.cpp:91] Creating Layer conv4_2
I0422 07:02:42.628856  2460 net.cpp:425] conv4_2 <- conv4_1
I0422 07:02:42.628866  2460 net.cpp:399] conv4_2 -> conv4_2
I0422 07:02:42.635573  2460 net.cpp:141] Setting up conv4_2
I0422 07:02:42.635597  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.635604  2460 net.cpp:156] Memory required for data: 860976096
I0422 07:02:42.635637  2460 layer_factory.hpp:77] Creating layer relu4_2
I0422 07:02:42.635651  2460 net.cpp:91] Creating Layer relu4_2
I0422 07:02:42.635658  2460 net.cpp:425] relu4_2 <- conv4_2
I0422 07:02:42.635666  2460 net.cpp:386] relu4_2 -> conv4_2 (in-place)
I0422 07:02:42.635824  2460 net.cpp:141] Setting up relu4_2
I0422 07:02:42.635839  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.635844  2460 net.cpp:156] Memory required for data: 873952224
I0422 07:02:42.635851  2460 layer_factory.hpp:77] Creating layer conv4_3
I0422 07:02:42.635864  2460 net.cpp:91] Creating Layer conv4_3
I0422 07:02:42.635870  2460 net.cpp:425] conv4_3 <- conv4_2
I0422 07:02:42.635880  2460 net.cpp:399] conv4_3 -> conv4_3
I0422 07:02:42.643321  2460 net.cpp:141] Setting up conv4_3
I0422 07:02:42.643388  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.643394  2460 net.cpp:156] Memory required for data: 886928352
I0422 07:02:42.643409  2460 layer_factory.hpp:77] Creating layer relu4_3
I0422 07:02:42.643429  2460 net.cpp:91] Creating Layer relu4_3
I0422 07:02:42.643437  2460 net.cpp:425] relu4_3 <- conv4_3
I0422 07:02:42.643450  2460 net.cpp:386] relu4_3 -> conv4_3 (in-place)
I0422 07:02:42.643760  2460 net.cpp:141] Setting up relu4_3
I0422 07:02:42.643777  2460 net.cpp:148] Top shape: 1 512 72 88 (3244032)
I0422 07:02:42.643784  2460 net.cpp:156] Memory required for data: 899904480
I0422 07:02:42.643790  2460 layer_factory.hpp:77] Creating layer pool4
I0422 07:02:42.643808  2460 net.cpp:91] Creating Layer pool4
I0422 07:02:42.643816  2460 net.cpp:425] pool4 <- conv4_3
I0422 07:02:42.643824  2460 net.cpp:399] pool4 -> pool4
I0422 07:02:42.643894  2460 net.cpp:141] Setting up pool4
I0422 07:02:42.643906  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.643918  2460 net.cpp:156] Memory required for data: 903148512
I0422 07:02:42.643924  2460 layer_factory.hpp:77] Creating layer conv5_1
I0422 07:02:42.643940  2460 net.cpp:91] Creating Layer conv5_1
I0422 07:02:42.643946  2460 net.cpp:425] conv5_1 <- pool4
I0422 07:02:42.643956  2460 net.cpp:399] conv5_1 -> conv5_1
I0422 07:02:42.651273  2460 net.cpp:141] Setting up conv5_1
I0422 07:02:42.651335  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.651345  2460 net.cpp:156] Memory required for data: 906392544
I0422 07:02:42.651366  2460 layer_factory.hpp:77] Creating layer relu5_1
I0422 07:02:42.651392  2460 net.cpp:91] Creating Layer relu5_1
I0422 07:02:42.651401  2460 net.cpp:425] relu5_1 <- conv5_1
I0422 07:02:42.651414  2460 net.cpp:386] relu5_1 -> conv5_1 (in-place)
I0422 07:02:42.651729  2460 net.cpp:141] Setting up relu5_1
I0422 07:02:42.651746  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.651752  2460 net.cpp:156] Memory required for data: 909636576
I0422 07:02:42.651758  2460 layer_factory.hpp:77] Creating layer conv5_2
I0422 07:02:42.651778  2460 net.cpp:91] Creating Layer conv5_2
I0422 07:02:42.651792  2460 net.cpp:425] conv5_2 <- conv5_1
I0422 07:02:42.651814  2460 net.cpp:399] conv5_2 -> conv5_2
I0422 07:02:42.658524  2460 net.cpp:141] Setting up conv5_2
I0422 07:02:42.658560  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.658567  2460 net.cpp:156] Memory required for data: 912880608
I0422 07:02:42.658579  2460 layer_factory.hpp:77] Creating layer relu5_2
I0422 07:02:42.658591  2460 net.cpp:91] Creating Layer relu5_2
I0422 07:02:42.658597  2460 net.cpp:425] relu5_2 <- conv5_2
I0422 07:02:42.658605  2460 net.cpp:386] relu5_2 -> conv5_2 (in-place)
I0422 07:02:42.658777  2460 net.cpp:141] Setting up relu5_2
I0422 07:02:42.658792  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.658797  2460 net.cpp:156] Memory required for data: 916124640
I0422 07:02:42.658803  2460 layer_factory.hpp:77] Creating layer conv5_3
I0422 07:02:42.658818  2460 net.cpp:91] Creating Layer conv5_3
I0422 07:02:42.658823  2460 net.cpp:425] conv5_3 <- conv5_2
I0422 07:02:42.658834  2460 net.cpp:399] conv5_3 -> conv5_3
I0422 07:02:42.665722  2460 net.cpp:141] Setting up conv5_3
I0422 07:02:42.665760  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.665767  2460 net.cpp:156] Memory required for data: 919368672
I0422 07:02:42.665779  2460 layer_factory.hpp:77] Creating layer relu5_3
I0422 07:02:42.665791  2460 net.cpp:91] Creating Layer relu5_3
I0422 07:02:42.665798  2460 net.cpp:425] relu5_3 <- conv5_3
I0422 07:02:42.665819  2460 net.cpp:386] relu5_3 -> conv5_3 (in-place)
I0422 07:02:42.666100  2460 net.cpp:141] Setting up relu5_3
I0422 07:02:42.666115  2460 net.cpp:148] Top shape: 1 512 36 44 (811008)
I0422 07:02:42.666121  2460 net.cpp:156] Memory required for data: 922612704
I0422 07:02:42.666127  2460 layer_factory.hpp:77] Creating layer pool5
I0422 07:02:42.666146  2460 net.cpp:91] Creating Layer pool5
I0422 07:02:42.666152  2460 net.cpp:425] pool5 <- conv5_3
I0422 07:02:42.666162  2460 net.cpp:399] pool5 -> pool5
I0422 07:02:42.666229  2460 net.cpp:141] Setting up pool5
I0422 07:02:42.666241  2460 net.cpp:148] Top shape: 1 512 18 22 (202752)
I0422 07:02:42.666247  2460 net.cpp:156] Memory required for data: 923423712
I0422 07:02:42.666254  2460 layer_factory.hpp:77] Creating layer fc6
I0422 07:02:42.666270  2460 net.cpp:91] Creating Layer fc6
I0422 07:02:42.666277  2460 net.cpp:425] fc6 <- pool5
I0422 07:02:42.666285  2460 net.cpp:399] fc6 -> fc6
I0422 07:02:42.946178  2460 net.cpp:141] Setting up fc6
I0422 07:02:42.946238  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.946244  2460 net.cpp:156] Memory required for data: 926569440
I0422 07:02:42.946260  2460 layer_factory.hpp:77] Creating layer relu6
I0422 07:02:42.946280  2460 net.cpp:91] Creating Layer relu6
I0422 07:02:42.946290  2460 net.cpp:425] relu6 <- fc6
I0422 07:02:42.946300  2460 net.cpp:386] relu6 -> fc6 (in-place)
I0422 07:02:42.946581  2460 net.cpp:141] Setting up relu6
I0422 07:02:42.946599  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.946604  2460 net.cpp:156] Memory required for data: 929715168
I0422 07:02:42.946610  2460 layer_factory.hpp:77] Creating layer drop6
I0422 07:02:42.946624  2460 net.cpp:91] Creating Layer drop6
I0422 07:02:42.946631  2460 net.cpp:425] drop6 <- fc6
I0422 07:02:42.946638  2460 net.cpp:386] drop6 -> fc6 (in-place)
I0422 07:02:42.946686  2460 net.cpp:141] Setting up drop6
I0422 07:02:42.946707  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.946727  2460 net.cpp:156] Memory required for data: 932860896
I0422 07:02:42.946732  2460 layer_factory.hpp:77] Creating layer fc7
I0422 07:02:42.946750  2460 net.cpp:91] Creating Layer fc7
I0422 07:02:42.946756  2460 net.cpp:425] fc7 <- fc6
I0422 07:02:42.946765  2460 net.cpp:399] fc7 -> fc7
I0422 07:02:42.995257  2460 net.cpp:141] Setting up fc7
I0422 07:02:42.995326  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.995332  2460 net.cpp:156] Memory required for data: 936006624
I0422 07:02:42.995347  2460 layer_factory.hpp:77] Creating layer relu7
I0422 07:02:42.995365  2460 net.cpp:91] Creating Layer relu7
I0422 07:02:42.995373  2460 net.cpp:425] relu7 <- fc7
I0422 07:02:42.995399  2460 net.cpp:386] relu7 -> fc7 (in-place)
I0422 07:02:42.995589  2460 net.cpp:141] Setting up relu7
I0422 07:02:42.995610  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.995616  2460 net.cpp:156] Memory required for data: 939152352
I0422 07:02:42.995646  2460 layer_factory.hpp:77] Creating layer drop7
I0422 07:02:42.995661  2460 net.cpp:91] Creating Layer drop7
I0422 07:02:42.995666  2460 net.cpp:425] drop7 <- fc7
I0422 07:02:42.995678  2460 net.cpp:386] drop7 -> fc7 (in-place)
I0422 07:02:42.995725  2460 net.cpp:141] Setting up drop7
I0422 07:02:42.995736  2460 net.cpp:148] Top shape: 1 4096 12 16 (786432)
I0422 07:02:42.995741  2460 net.cpp:156] Memory required for data: 942298080
I0422 07:02:42.995748  2460 layer_factory.hpp:77] Creating layer score_fr
I0422 07:02:42.995764  2460 net.cpp:91] Creating Layer score_fr
I0422 07:02:42.995770  2460 net.cpp:425] score_fr <- fc7
I0422 07:02:42.995780  2460 net.cpp:399] score_fr -> score_fr
I0422 07:02:42.996953  2460 net.cpp:141] Setting up score_fr
I0422 07:02:42.996970  2460 net.cpp:148] Top shape: 1 21 12 16 (4032)
I0422 07:02:42.996976  2460 net.cpp:156] Memory required for data: 942314208
I0422 07:02:42.996986  2460 layer_factory.hpp:77] Creating layer upscore
I0422 07:02:42.997000  2460 net.cpp:91] Creating Layer upscore
I0422 07:02:42.997007  2460 net.cpp:425] upscore <- score_fr
I0422 07:02:42.997023  2460 net.cpp:399] upscore -> upscore
I0422 07:02:43.001526  2460 net.cpp:141] Setting up upscore
I0422 07:02:43.001544  2460 net.cpp:148] Top shape: 1 21 416 544 (4752384)
I0422 07:02:43.001550  2460 net.cpp:156] Memory required for data: 961323744
I0422 07:02:43.001569  2460 layer_factory.hpp:77] Creating layer score
I0422 07:02:43.001580  2460 net.cpp:91] Creating Layer score
I0422 07:02:43.001586  2460 net.cpp:425] score <- upscore
I0422 07:02:43.001593  2460 net.cpp:425] score <- data_data_0_split_1
I0422 07:02:43.001601  2460 net.cpp:399] score -> score
I0422 07:02:43.001646  2460 net.cpp:141] Setting up score
I0422 07:02:43.001657  2460 net.cpp:148] Top shape: 1 21 375 500 (3937500)
I0422 07:02:43.001663  2460 net.cpp:156] Memory required for data: 977073744
I0422 07:02:43.001669  2460 layer_factory.hpp:77] Creating layer loss
I0422 07:02:43.001682  2460 net.cpp:91] Creating Layer loss
I0422 07:02:43.001688  2460 net.cpp:425] loss <- score
I0422 07:02:43.001695  2460 net.cpp:425] loss <- label
I0422 07:02:43.001703  2460 net.cpp:399] loss -> loss
I0422 07:02:43.001716  2460 layer_factory.hpp:77] Creating layer loss
I0422 07:02:43.012861  2460 net.cpp:141] Setting up loss
I0422 07:02:43.012923  2460 net.cpp:148] Top shape: (1)
I0422 07:02:43.012929  2460 net.cpp:151]     with loss weight 1
I0422 07:02:43.012950  2460 net.cpp:156] Memory required for data: 977073748
I0422 07:02:43.012959  2460 net.cpp:217] loss needs backward computation.
I0422 07:02:43.012969  2460 net.cpp:217] score needs backward computation.
I0422 07:02:43.012974  2460 net.cpp:217] upscore needs backward computation.
I0422 07:02:43.012981  2460 net.cpp:217] score_fr needs backward computation.
I0422 07:02:43.012987  2460 net.cpp:217] drop7 needs backward computation.
I0422 07:02:43.012992  2460 net.cpp:217] relu7 needs backward computation.
I0422 07:02:43.012997  2460 net.cpp:217] fc7 needs backward computation.
I0422 07:02:43.013003  2460 net.cpp:217] drop6 needs backward computation.
I0422 07:02:43.013010  2460 net.cpp:217] relu6 needs backward computation.
I0422 07:02:43.013015  2460 net.cpp:217] fc6 needs backward computation.
I0422 07:02:43.013020  2460 net.cpp:217] pool5 needs backward computation.
I0422 07:02:43.013026  2460 net.cpp:217] relu5_3 needs backward computation.
I0422 07:02:43.013031  2460 net.cpp:217] conv5_3 needs backward computation.
I0422 07:02:43.013037  2460 net.cpp:217] relu5_2 needs backward computation.
I0422 07:02:43.013043  2460 net.cpp:217] conv5_2 needs backward computation.
I0422 07:02:43.013051  2460 net.cpp:217] relu5_1 needs backward computation.
I0422 07:02:43.013057  2460 net.cpp:217] conv5_1 needs backward computation.
I0422 07:02:43.013080  2460 net.cpp:217] pool4 needs backward computation.
I0422 07:02:43.013087  2460 net.cpp:217] relu4_3 needs backward computation.
I0422 07:02:43.013092  2460 net.cpp:217] conv4_3 needs backward computation.
I0422 07:02:43.013098  2460 net.cpp:217] relu4_2 needs backward computation.
I0422 07:02:43.013103  2460 net.cpp:217] conv4_2 needs backward computation.
I0422 07:02:43.013109  2460 net.cpp:217] relu4_1 needs backward computation.
I0422 07:02:43.013114  2460 net.cpp:217] conv4_1 needs backward computation.
I0422 07:02:43.013120  2460 net.cpp:217] pool3 needs backward computation.
I0422 07:02:43.013125  2460 net.cpp:217] relu3_3 needs backward computation.
I0422 07:02:43.013131  2460 net.cpp:217] conv3_3 needs backward computation.
I0422 07:02:43.013136  2460 net.cpp:217] relu3_2 needs backward computation.
I0422 07:02:43.013142  2460 net.cpp:217] conv3_2 needs backward computation.
I0422 07:02:43.013147  2460 net.cpp:217] relu3_1 needs backward computation.
I0422 07:02:43.013152  2460 net.cpp:217] conv3_1 needs backward computation.
I0422 07:02:43.013159  2460 net.cpp:217] pool2 needs backward computation.
I0422 07:02:43.013164  2460 net.cpp:217] relu2_2 needs backward computation.
I0422 07:02:43.013170  2460 net.cpp:217] conv2_2 needs backward computation.
I0422 07:02:43.013175  2460 net.cpp:217] relu2_1 needs backward computation.
I0422 07:02:43.013180  2460 net.cpp:217] conv2_1 needs backward computation.
I0422 07:02:43.013185  2460 net.cpp:217] pool1 needs backward computation.
I0422 07:02:43.013191  2460 net.cpp:217] relu1_2 needs backward computation.
I0422 07:02:43.013196  2460 net.cpp:217] conv1_2 needs backward computation.
I0422 07:02:43.013211  2460 net.cpp:217] relu1_1 needs backward computation.
I0422 07:02:43.013216  2460 net.cpp:217] conv1_1 needs backward computation.
I0422 07:02:43.013221  2460 net.cpp:219] data_data_0_split does not need backward computation.
I0422 07:02:43.013228  2460 net.cpp:219] data does not need backward computation.
I0422 07:02:43.013233  2460 net.cpp:261] This network produces output loss
I0422 07:02:43.013272  2460 net.cpp:274] Network initialization done.
I0422 07:02:43.013413  2460 solver.cpp:60] Solver scaffolding done.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:505] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 544614812
I0422 07:02:44.719774  2460 solver.cpp:228] Iteration 0, loss = 200032
I0422 07:02:44.719866  2460 solver.cpp:244]     Train net output #0: loss = 200032 (* 1 = 200032 loss)
I0422 07:02:44.719883  2460 sgd_solver.cpp:106] Iteration 0, lr = 1e-10
I0422 07:03:14.704555  2460 solver.cpp:228] Iteration 20, loss = 268755
I0422 07:03:14.704641  2460 solver.cpp:244]     Train net output #0: loss = 189011 (* 1 = 189011 loss)
I0422 07:03:14.704654  2460 sgd_solver.cpp:106] Iteration 20, lr = 1e-10
I0422 07:03:44.681581  2460 solver.cpp:228] Iteration 40, loss = 120160
I0422 07:03:44.681668  2460 solver.cpp:244]     Train net output #0: loss = 54619.1 (* 1 = 54619.1 loss)
I0422 07:03:44.681681  2460 sgd_solver.cpp:106] Iteration 40, lr = 1e-10
I0422 07:04:14.656935  2460 solver.cpp:228] Iteration 60, loss = 108550
I0422 07:04:14.657058  2460 solver.cpp:244]     Train net output #0: loss = 40860.1 (* 1 = 40860.1 loss)
I0422 07:04:14.657081  2460 sgd_solver.cpp:106] Iteration 60, lr = 1e-10
I0422 07:04:44.638046  2460 solver.cpp:228] Iteration 80, loss = 113714
I0422 07:04:44.638157  2460 solver.cpp:244]     Train net output #0: loss = 41804.6 (* 1 = 41804.6 loss)
I0422 07:04:44.638182  2460 sgd_solver.cpp:106] Iteration 80, lr = 1e-10
/home/ubuntu/pynb/caffe-future/score.py:51: RuntimeWarning: invalid value encountered in true_divide
  acc = np.diag(hist) / hist.sum(1)
/home/ubuntu/pynb/caffe-future/score.py:54: RuntimeWarning: invalid value encountered in true_divide
  iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))
I0422 07:05:25.796680  2460 solver.cpp:228] Iteration 100, loss = 58285.8
I0422 07:05:25.796787  2460 solver.cpp:244]     Train net output #0: loss = 58285.8 (* 1 = 58285.8 loss)
I0422 07:05:25.796804  2460 sgd_solver.cpp:106] Iteration 100, lr = 1e-10
I0422 07:05:55.759150  2460 solver.cpp:228] Iteration 120, loss = 98150.4
I0422 07:05:55.759261  2460 solver.cpp:244]     Train net output #0: loss = 98736.4 (* 1 = 98736.4 loss)
I0422 07:05:55.759287  2460 sgd_solver.cpp:106] Iteration 120, lr = 1e-10
I0422 07:06:25.736323  2460 solver.cpp:228] Iteration 140, loss = 89548.6
I0422 07:06:25.736441  2460 solver.cpp:244]     Train net output #0: loss = 137968 (* 1 = 137968 loss)
I0422 07:06:25.736461  2460 sgd_solver.cpp:106] Iteration 140, lr = 1e-10
I0422 07:06:55.716243  2460 solver.cpp:228] Iteration 160, loss = 84824.4
I0422 07:06:55.716363  2460 solver.cpp:244]     Train net output #0: loss = 83882 (* 1 = 83882 loss)
I0422 07:06:55.716379  2460 sgd_solver.cpp:106] Iteration 160, lr = 1e-10
I0422 07:07:25.693702  2460 solver.cpp:228] Iteration 180, loss = 88084.2
I0422 07:07:25.693811  2460 solver.cpp:244]     Train net output #0: loss = 45374.8 (* 1 = 45374.8 loss)
I0422 07:07:25.693827  2460 sgd_solver.cpp:106] Iteration 180, lr = 1e-10
I0422 07:08:06.759234  2460 solver.cpp:228] Iteration 200, loss = 35024.5
I0422 07:08:06.759346  2460 solver.cpp:244]     Train net output #0: loss = 35024.5 (* 1 = 35024.5 loss)
I0422 07:08:06.759368  2460 sgd_solver.cpp:106] Iteration 200, lr = 1e-10
I0422 07:08:36.733362  2460 solver.cpp:228] Iteration 220, loss = 61525.7
I0422 07:08:36.733465  2460 solver.cpp:244]     Train net output #0: loss = 69291.8 (* 1 = 69291.8 loss)
I0422 07:08:36.733481  2460 sgd_solver.cpp:106] Iteration 220, lr = 1e-10
I0422 07:09:06.712962  2460 solver.cpp:228] Iteration 240, loss = 89121.2
I0422 07:09:06.713070  2460 solver.cpp:244]     Train net output #0: loss = 96266.1 (* 1 = 96266.1 loss)
I0422 07:09:06.713085  2460 sgd_solver.cpp:106] Iteration 240, lr = 1e-10
I0422 07:09:36.679508  2460 solver.cpp:228] Iteration 260, loss = 76106.2
I0422 07:09:36.679648  2460 solver.cpp:244]     Train net output #0: loss = 141022 (* 1 = 141022 loss)
I0422 07:09:36.679666  2460 sgd_solver.cpp:106] Iteration 260, lr = 1e-10
I0422 07:10:06.668582  2460 solver.cpp:228] Iteration 280, loss = 61379.3
I0422 07:10:06.668682  2460 solver.cpp:244]     Train net output #0: loss = 48024.2 (* 1 = 48024.2 loss)
I0422 07:10:06.668697  2460 sgd_solver.cpp:106] Iteration 280, lr = 1e-10
I0422 07:10:47.732121  2460 solver.cpp:228] Iteration 300, loss = 114443
I0422 07:10:47.732228  2460 solver.cpp:244]     Train net output #0: loss = 114443 (* 1 = 114443 loss)
I0422 07:10:47.732244  2460 sgd_solver.cpp:106] Iteration 300, lr = 1e-10
I0422 07:11:17.704511  2460 solver.cpp:228] Iteration 320, loss = 58975.8
I0422 07:11:17.704617  2460 solver.cpp:244]     Train net output #0: loss = 110013 (* 1 = 110013 loss)
I0422 07:11:17.704632  2460 sgd_solver.cpp:106] Iteration 320, lr = 1e-10
I0422 07:11:47.698595  2460 solver.cpp:228] Iteration 340, loss = 58408.6
I0422 07:11:47.698703  2460 solver.cpp:244]     Train net output #0: loss = 47391.1 (* 1 = 47391.1 loss)
I0422 07:11:47.698717  2460 sgd_solver.cpp:106] Iteration 340, lr = 1e-10
I0422 07:12:17.669970  2460 solver.cpp:228] Iteration 360, loss = 58872.4
I0422 07:12:17.670092  2460 solver.cpp:244]     Train net output #0: loss = 21897.1 (* 1 = 21897.1 loss)
I0422 07:12:17.670109  2460 sgd_solver.cpp:106] Iteration 360, lr = 1e-10
I0422 07:12:47.645087  2460 solver.cpp:228] Iteration 380, loss = 51605.9
I0422 07:12:47.645208  2460 solver.cpp:244]     Train net output #0: loss = 60520.9 (* 1 = 60520.9 loss)
I0422 07:12:47.645226  2460 sgd_solver.cpp:106] Iteration 380, lr = 1e-10
I0422 07:13:28.721715  2460 solver.cpp:228] Iteration 400, loss = 123716
I0422 07:13:28.721824  2460 solver.cpp:244]     Train net output #0: loss = 123716 (* 1 = 123716 loss)
I0422 07:13:28.721855  2460 sgd_solver.cpp:106] Iteration 400, lr = 1e-10
I0422 07:13:58.704830  2460 solver.cpp:228] Iteration 420, loss = 48626.7
I0422 07:13:58.704936  2460 solver.cpp:244]     Train net output #0: loss = 106257 (* 1 = 106257 loss)
I0422 07:13:58.704952  2460 sgd_solver.cpp:106] Iteration 420, lr = 1e-10
I0422 07:14:28.675711  2460 solver.cpp:228] Iteration 440, loss = 58095.3
I0422 07:14:28.675830  2460 solver.cpp:244]     Train net output #0: loss = 77589.7 (* 1 = 77589.7 loss)
I0422 07:14:28.675845  2460 sgd_solver.cpp:106] Iteration 440, lr = 1e-10
I0422 07:14:58.656236  2460 solver.cpp:228] Iteration 460, loss = 50633.3
I0422 07:14:58.656342  2460 solver.cpp:244]     Train net output #0: loss = 64813.2 (* 1 = 64813.2 loss)
I0422 07:14:58.656358  2460 sgd_solver.cpp:106] Iteration 460, lr = 1e-10
I0422 07:15:28.638651  2460 solver.cpp:228] Iteration 480, loss = 59102.2
I0422 07:15:28.638756  2460 solver.cpp:244]     Train net output #0: loss = 19048.7 (* 1 = 19048.7 loss)
I0422 07:15:28.638772  2460 sgd_solver.cpp:106] Iteration 480, lr = 1e-10
I0422 07:16:09.704679  2460 solver.cpp:228] Iteration 500, loss = 21319.8
I0422 07:16:09.704785  2460 solver.cpp:244]     Train net output #0: loss = 21319.8 (* 1 = 21319.8 loss)
I0422 07:16:09.704802  2460 sgd_solver.cpp:106] Iteration 500, lr = 1e-10
I0422 07:16:39.683940  2460 solver.cpp:228] Iteration 520, loss = 47756.1
I0422 07:16:39.684073  2460 solver.cpp:244]     Train net output #0: loss = 34379.3 (* 1 = 34379.3 loss)
I0422 07:16:39.684095  2460 sgd_solver.cpp:106] Iteration 520, lr = 1e-10
I0422 07:17:09.653304  2460 solver.cpp:228] Iteration 540, loss = 48330.3
I0422 07:17:09.653406  2460 solver.cpp:244]     Train net output #0: loss = 213907 (* 1 = 213907 loss)
I0422 07:17:09.653422  2460 sgd_solver.cpp:106] Iteration 540, lr = 1e-10
I0422 07:17:39.636452  2460 solver.cpp:228] Iteration 560, loss = 33991.3
I0422 07:17:39.636554  2460 solver.cpp:244]     Train net output #0: loss = 194981 (* 1 = 194981 loss)
I0422 07:17:39.636569  2460 sgd_solver.cpp:106] Iteration 560, lr = 1e-10
I0422 07:18:09.629127  2460 solver.cpp:228] Iteration 580, loss = 38505.2
I0422 07:18:09.629223  2460 solver.cpp:244]     Train net output #0: loss = 21040.2 (* 1 = 21040.2 loss)
I0422 07:18:09.629237  2460 sgd_solver.cpp:106] Iteration 580, lr = 1e-10
I0422 07:18:50.965762  2460 solver.cpp:228] Iteration 600, loss = 19453.5
I0422 07:18:50.965868  2460 solver.cpp:244]     Train net output #0: loss = 19453.5 (* 1 = 19453.5 loss)
I0422 07:18:50.965884  2460 sgd_solver.cpp:106] Iteration 600, lr = 1e-10
I0422 07:19:20.947162  2460 solver.cpp:228] Iteration 620, loss = 43535.1
I0422 07:19:20.947283  2460 solver.cpp:244]     Train net output #0: loss = 132259 (* 1 = 132259 loss)
I0422 07:19:20.947299  2460 sgd_solver.cpp:106] Iteration 620, lr = 1e-10
I0422 07:19:50.921527  2460 solver.cpp:228] Iteration 640, loss = 45645.4
I0422 07:19:50.921653  2460 solver.cpp:244]     Train net output #0: loss = 17120.7 (* 1 = 17120.7 loss)
I0422 07:19:50.921669  2460 sgd_solver.cpp:106] Iteration 640, lr = 1e-10
I0422 07:20:20.893195  2460 solver.cpp:228] Iteration 660, loss = 38165.2
I0422 07:20:20.893296  2460 solver.cpp:244]     Train net output #0: loss = 14763.3 (* 1 = 14763.3 loss)
I0422 07:20:20.893311  2460 sgd_solver.cpp:106] Iteration 660, lr = 1e-10
I0422 07:20:50.867605  2460 solver.cpp:228] Iteration 680, loss = 41192.8
I0422 07:20:50.867724  2460 solver.cpp:244]     Train net output #0: loss = 14719.7 (* 1 = 14719.7 loss)
I0422 07:20:50.867740  2460 sgd_solver.cpp:106] Iteration 680, lr = 1e-10
I0422 07:21:31.933347  2460 solver.cpp:228] Iteration 700, loss = 41389.4
I0422 07:21:31.933449  2460 solver.cpp:244]     Train net output #0: loss = 41389.4 (* 1 = 41389.4 loss)
I0422 07:21:31.933465  2460 sgd_solver.cpp:106] Iteration 700, lr = 1e-10
I0422 07:22:01.906522  2460 solver.cpp:228] Iteration 720, loss = 26550.5
I0422 07:22:01.906633  2460 solver.cpp:244]     Train net output #0: loss = 22737.7 (* 1 = 22737.7 loss)
I0422 07:22:01.906656  2460 sgd_solver.cpp:106] Iteration 720, lr = 1e-10
I0422 07:22:31.881537  2460 solver.cpp:228] Iteration 740, loss = 51620.4
I0422 07:22:31.881649  2460 solver.cpp:244]     Train net output #0: loss = 16858.8 (* 1 = 16858.8 loss)
I0422 07:22:31.881664  2460 sgd_solver.cpp:106] Iteration 740, lr = 1e-10
I0422 07:23:01.860333  2460 solver.cpp:228] Iteration 760, loss = 48901.5
I0422 07:23:01.860436  2460 solver.cpp:244]     Train net output #0: loss = 70474.7 (* 1 = 70474.7 loss)
I0422 07:23:01.860451  2460 sgd_solver.cpp:106] Iteration 760, lr = 1e-10
I0422 07:23:31.839654  2460 solver.cpp:228] Iteration 780, loss = 36481.8
I0422 07:23:31.839756  2460 solver.cpp:244]     Train net output #0: loss = 38106.1 (* 1 = 38106.1 loss)
I0422 07:23:31.839771  2460 sgd_solver.cpp:106] Iteration 780, lr = 1e-10
I0422 07:24:12.875432  2460 solver.cpp:228] Iteration 800, loss = 41177.9
I0422 07:24:12.875526  2460 solver.cpp:244]     Train net output #0: loss = 41177.9 (* 1 = 41177.9 loss)
I0422 07:24:12.875542  2460 sgd_solver.cpp:106] Iteration 800, lr = 1e-10
I0422 07:24:42.854094  2460 solver.cpp:228] Iteration 820, loss = 37582.8
I0422 07:24:42.854212  2460 solver.cpp:244]     Train net output #0: loss = 34215.6 (* 1 = 34215.6 loss)
I0422 07:24:42.854228  2460 sgd_solver.cpp:106] Iteration 820, lr = 1e-10
I0422 07:25:12.825022  2460 solver.cpp:228] Iteration 840, loss = 23239.5
I0422 07:25:12.825126  2460 solver.cpp:244]     Train net output #0: loss = 31818.9 (* 1 = 31818.9 loss)
I0422 07:25:12.825142  2460 sgd_solver.cpp:106] Iteration 840, lr = 1e-10
I0422 07:25:42.801904  2460 solver.cpp:228] Iteration 860, loss = 47725.9
I0422 07:25:42.802026  2460 solver.cpp:244]     Train net output #0: loss = 20157.3 (* 1 = 20157.3 loss)
I0422 07:25:42.802042  2460 sgd_solver.cpp:106] Iteration 860, lr = 1e-10
I0422 07:26:12.773907  2460 solver.cpp:228] Iteration 880, loss = 30553.6
I0422 07:26:12.774008  2460 solver.cpp:244]     Train net output #0: loss = 18958.4 (* 1 = 18958.4 loss)
I0422 07:26:12.774025  2460 sgd_solver.cpp:106] Iteration 880, lr = 1e-10
I0422 07:26:53.840749  2460 solver.cpp:228] Iteration 900, loss = 18517.4
I0422 07:26:53.840854  2460 solver.cpp:244]     Train net output #0: loss = 18517.4 (* 1 = 18517.4 loss)
I0422 07:26:53.840870  2460 sgd_solver.cpp:106] Iteration 900, lr = 1e-10
I0422 07:27:23.814424  2460 solver.cpp:228] Iteration 920, loss = 36856.5
I0422 07:27:23.814544  2460 solver.cpp:244]     Train net output #0: loss = 19617.4 (* 1 = 19617.4 loss)
I0422 07:27:23.814560  2460 sgd_solver.cpp:106] Iteration 920, lr = 1e-10
I0422 07:27:53.791447  2460 solver.cpp:228] Iteration 940, loss = 57531.8
I0422 07:27:53.791553  2460 solver.cpp:244]     Train net output #0: loss = 70479.4 (* 1 = 70479.4 loss)
I0422 07:27:53.791590  2460 sgd_solver.cpp:106] Iteration 940, lr = 1e-10
I0422 07:28:23.758777  2460 solver.cpp:228] Iteration 960, loss = 34777.1
I0422 07:28:23.758880  2460 solver.cpp:244]     Train net output #0: loss = 43857.8 (* 1 = 43857.8 loss)
I0422 07:28:23.758905  2460 sgd_solver.cpp:106] Iteration 960, lr = 1e-10
I0422 07:28:53.747598  2460 solver.cpp:228] Iteration 980, loss = 30784.6
I0422 07:28:53.747732  2460 solver.cpp:244]     Train net output #0: loss = 15323.7 (* 1 = 15323.7 loss)
I0422 07:28:53.747750  2460 sgd_solver.cpp:106] Iteration 980, lr = 1e-10
I0422 07:29:22.228368  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_1000.caffemodel
I0422 07:29:25.732672  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_1000.solverstate
I0422 07:29:38.236449  2460 solver.cpp:228] Iteration 1000, loss = 15475.1
I0422 07:29:38.236557  2460 solver.cpp:244]     Train net output #0: loss = 15475.1 (* 1 = 15475.1 loss)
I0422 07:29:38.236582  2460 sgd_solver.cpp:106] Iteration 1000, lr = 1e-10
I0422 07:30:08.214063  2460 solver.cpp:228] Iteration 1020, loss = 31777.4
I0422 07:30:08.214172  2460 solver.cpp:244]     Train net output #0: loss = 19884.6 (* 1 = 19884.6 loss)
I0422 07:30:08.214188  2460 sgd_solver.cpp:106] Iteration 1020, lr = 1e-10
I0422 07:30:38.203749  2460 solver.cpp:228] Iteration 1040, loss = 34141
I0422 07:30:38.203861  2460 solver.cpp:244]     Train net output #0: loss = 51638.7 (* 1 = 51638.7 loss)
I0422 07:30:38.203876  2460 sgd_solver.cpp:106] Iteration 1040, lr = 1e-10
I0422 07:31:08.179322  2460 solver.cpp:228] Iteration 1060, loss = 32547.7
I0422 07:31:08.179430  2460 solver.cpp:244]     Train net output #0: loss = 32783.8 (* 1 = 32783.8 loss)
I0422 07:31:08.179448  2460 sgd_solver.cpp:106] Iteration 1060, lr = 1e-10
I0422 07:31:38.152468  2460 solver.cpp:228] Iteration 1080, loss = 43251.2
I0422 07:31:38.152572  2460 solver.cpp:244]     Train net output #0: loss = 148559 (* 1 = 148559 loss)
I0422 07:31:38.152590  2460 sgd_solver.cpp:106] Iteration 1080, lr = 1e-10
>>> 2017-04-22 07:05:13.111393 Begin seg tests
>>> 2017-04-22 07:05:20.310765 Iteration 100 loss 87407.5249023
>>> 2017-04-22 07:05:20.310923 Iteration 100 overall accuracy 0.807168
>>> 2017-04-22 07:05:20.311226 Iteration 100 mean accuracy 0.798850407658
>>> 2017-04-22 07:05:20.311542 Iteration 100 mean IU 0.670413684305
>>> 2017-04-22 07:05:20.311729 Iteration 100 fwavacc 0.67571878594
>>> 2017-04-22 07:07:54.166753 Begin seg tests
>>> 2017-04-22 07:08:01.249941 Iteration 200 loss 81518.9138184
>>> 2017-04-22 07:08:01.250031 Iteration 200 overall accuracy 0.831728444444
>>> 2017-04-22 07:08:01.250089 Iteration 200 mean accuracy 0.814784341997
>>> 2017-04-22 07:08:01.250290 Iteration 200 mean IU 0.698117419061
>>> 2017-04-22 07:08:01.250396 Iteration 200 fwavacc 0.70465120368
>>> 2017-04-22 07:10:35.139872 Begin seg tests
>>> 2017-04-22 07:10:42.227137 Iteration 300 loss 47416.5035807
>>> 2017-04-22 07:10:42.227249 Iteration 300 overall accuracy 0.892224
>>> 2017-04-22 07:10:42.227320 Iteration 300 mean accuracy 0.891774463398
>>> 2017-04-22 07:10:42.227549 Iteration 300 mean IU 0.804714309032
>>> 2017-04-22 07:10:42.227712 Iteration 300 fwavacc 0.805501753664
>>> 2017-04-22 07:13:16.125873 Begin seg tests
>>> 2017-04-22 07:13:23.208777 Iteration 400 loss 30531.4070638
>>> 2017-04-22 07:13:23.208885 Iteration 400 overall accuracy 0.934653777778
>>> 2017-04-22 07:13:23.208944 Iteration 400 mean accuracy 0.934855902354
>>> 2017-04-22 07:13:23.209164 Iteration 400 mean IU 0.877323959533
>>> 2017-04-22 07:13:23.209271 Iteration 400 fwavacc 0.877324508468
>>> 2017-04-22 07:15:57.117655 Begin seg tests
>>> 2017-04-22 07:16:04.197748 Iteration 500 loss 54810.2874349
>>> 2017-04-22 07:16:04.197858 Iteration 500 overall accuracy 0.872995555556
>>> 2017-04-22 07:16:04.197928 Iteration 500 mean accuracy 0.876643653439
>>> 2017-04-22 07:16:04.198134 Iteration 500 mean IU 0.774615889134
>>> 2017-04-22 07:16:04.198238 Iteration 500 fwavacc 0.774613702136
>>> 2017-04-22 07:18:38.114673 Begin seg tests
>>> 2017-04-22 07:18:45.205665 Iteration 600 loss 60678.686849
>>> 2017-04-22 07:18:45.205764 Iteration 600 overall accuracy 0.865532444444
>>> 2017-04-22 07:18:45.205829 Iteration 600 mean accuracy 0.864286447999
>>> 2017-04-22 07:18:45.206048 Iteration 600 mean IU 0.762137818754
>>> 2017-04-22 07:18:45.206171 Iteration 600 fwavacc 0.762613938503
>>> 2017-04-22 07:21:19.347522 Begin seg tests
>>> 2017-04-22 07:21:26.429274 Iteration 700 loss 50954.25
>>> 2017-04-22 07:21:26.429378 Iteration 700 overall accuracy 0.894208
>>> 2017-04-22 07:21:26.429444 Iteration 700 mean accuracy 0.896773871155
>>> 2017-04-22 07:21:26.429652 Iteration 700 mean IU 0.807386768155
>>> 2017-04-22 07:21:26.429774 Iteration 700 fwavacc 0.809317735124
>>> 2017-04-22 07:24:00.326105 Begin seg tests
>>> 2017-04-22 07:24:07.410719 Iteration 800 loss 27808.3943685
>>> 2017-04-22 07:24:07.410817 Iteration 800 overall accuracy 0.941908888889
>>> 2017-04-22 07:24:07.410886 Iteration 800 mean accuracy 0.942573316497
>>> 2017-04-22 07:24:07.411135 Iteration 800 mean IU 0.889538599368
>>> 2017-04-22 07:24:07.411250 Iteration 800 fwavacc 0.890340851701
>>> 2017-04-22 07:26:41.253506 Begin seg tests
>>> 2017-04-22 07:26:48.335909 Iteration 900 loss 46764.3968913
>>> 2017-04-22 07:26:48.336006 Iteration 900 overall accuracy 0.923012444444
>>> 2017-04-22 07:26:48.336063 Iteration 900 mean accuracy 0.920920990074
>>> 2017-04-22 07:26:48.336293 Iteration 900 mean IU 0.856075169234
>>> 2017-04-22 07:26:48.336407 Iteration 900 fwavacc 0.856621303404
>>> 2017-04-22 07:29:26.690329 Begin seg tests
>>> 2017-04-22 07:29:32.784875 Iteration 1000 loss 26325.9128418
>>> 2017-04-22 07:29:32.784980 Iteration 1000 overall accuracy 0.944113333333
>>> 2017-04-22 07:29:32.785040 Iteration 1000 mean accuracy 0.944512275442
>>> 2017-04-22 07:29:32.785249 Iteration 1000 mean IU 0.892525089862
>>> 2017-04-22 07:29:32.785365 Iteration 1000 fwavacc 0.89437315517
>>> 2017-04-22 07:32:06.630871 Begin seg tests
>>> 2017-04-22 07:32:13.712136 Iteration 1100 loss 34737.519043
>>> 2017-04-22 07:32:13.712I0422 07:32:15.804818  2460 solver.cpp:228] Iteration 1100, loss = 18670
I0422 07:32:15.804913  2460 solver.cpp:244]     Train net output #0: loss = 18670 (* 1 = 18670 loss)
I0422 07:32:15.804927  2460 sgd_solver.cpp:106] Iteration 1100, lr = 1e-10
I0422 07:32:45.773493  2460 solver.cpp:228] Iteration 1120, loss = 31203.5
I0422 07:32:45.773598  2460 solver.cpp:244]     Train net output #0: loss = 45510 (* 1 = 45510 loss)
I0422 07:32:45.773613  2460 sgd_solver.cpp:106] Iteration 1120, lr = 1e-10
I0422 07:33:15.749083  2460 solver.cpp:228] Iteration 1140, loss = 22061.8
I0422 07:33:15.749179  2460 solver.cpp:244]     Train net output #0: loss = 10137.6 (* 1 = 10137.6 loss)
I0422 07:33:15.749195  2460 sgd_solver.cpp:106] Iteration 1140, lr = 1e-10
I0422 07:33:45.734967  2460 solver.cpp:228] Iteration 1160, loss = 32822.5
I0422 07:33:45.735074  2460 solver.cpp:244]     Train net output #0: loss = 9575.24 (* 1 = 9575.24 loss)
I0422 07:33:45.735090  2460 sgd_solver.cpp:106] Iteration 1160, lr = 1e-10
I0422 07:34:15.713637  2460 solver.cpp:228] Iteration 1180, loss = 34036.5
I0422 07:34:15.713768  2460 solver.cpp:244]     Train net output #0: loss = 16121.3 (* 1 = 16121.3 loss)
I0422 07:34:15.713785  2460 sgd_solver.cpp:106] Iteration 1180, lr = 1e-10
I0422 07:34:53.351060  2460 solver.cpp:228] Iteration 1200, loss = 117672
I0422 07:34:53.351181  2460 solver.cpp:244]     Train net output #0: loss = 117672 (* 1 = 117672 loss)
I0422 07:34:53.351197  2460 sgd_solver.cpp:106] Iteration 1200, lr = 1e-10
I0422 07:35:23.325640  2460 solver.cpp:228] Iteration 1220, loss = 42759.1
I0422 07:35:23.325755  2460 solver.cpp:244]     Train net output #0: loss = 44010.7 (* 1 = 44010.7 loss)
I0422 07:35:23.325772  2460 sgd_solver.cpp:106] Iteration 1220, lr = 1e-10
I0422 07:35:53.311573  2460 solver.cpp:228] Iteration 1240, loss = 29869.6
I0422 07:35:53.311702  2460 solver.cpp:244]     Train net output #0: loss = 30233.1 (* 1 = 30233.1 loss)
I0422 07:35:53.311717  2460 sgd_solver.cpp:106] Iteration 1240, lr = 1e-10
I0422 07:36:23.291085  2460 solver.cpp:228] Iteration 1260, loss = 33040.3
I0422 07:36:23.291199  2460 solver.cpp:244]     Train net output #0: loss = 35741.2 (* 1 = 35741.2 loss)
I0422 07:36:23.291214  2460 sgd_solver.cpp:106] Iteration 1260, lr = 1e-10
I0422 07:36:53.264889  2460 solver.cpp:228] Iteration 1280, loss = 34552.4
I0422 07:36:53.264997  2460 solver.cpp:244]     Train net output #0: loss = 20090.4 (* 1 = 20090.4 loss)
I0422 07:36:53.265012  2460 sgd_solver.cpp:106] Iteration 1280, lr = 1e-10
I0422 07:37:30.928627  2460 solver.cpp:228] Iteration 1300, loss = 24375.1
I0422 07:37:30.928750  2460 solver.cpp:244]     Train net output #0: loss = 24375.1 (* 1 = 24375.1 loss)
I0422 07:37:30.928766  2460 sgd_solver.cpp:106] Iteration 1300, lr = 1e-10
I0422 07:38:00.914242  2460 solver.cpp:228] Iteration 1320, loss = 38316.4
I0422 07:38:00.914361  2460 solver.cpp:244]     Train net output #0: loss = 22581.7 (* 1 = 22581.7 loss)
I0422 07:38:00.914378  2460 sgd_solver.cpp:106] Iteration 1320, lr = 1e-10
I0422 07:38:30.886591  2460 solver.cpp:228] Iteration 1340, loss = 26374.8
I0422 07:38:30.886706  2460 solver.cpp:244]     Train net output #0: loss = 20716.9 (* 1 = 20716.9 loss)
I0422 07:38:30.886725  2460 sgd_solver.cpp:106] Iteration 1340, lr = 1e-10
I0422 07:39:00.865367  2460 solver.cpp:228] Iteration 1360, loss = 23412.6
I0422 07:39:00.865483  2460 solver.cpp:244]     Train net output #0: loss = 24474.1 (* 1 = 24474.1 loss)
I0422 07:39:00.865499  2460 sgd_solver.cpp:106] Iteration 1360, lr = 1e-10
I0422 07:39:30.843972  2460 solver.cpp:228] Iteration 1380, loss = 33591.3
I0422 07:39:30.844070  2460 solver.cpp:244]     Train net output #0: loss = 55718 (* 1 = 55718 loss)
I0422 07:39:30.844084  2460 sgd_solver.cpp:106] Iteration 1380, lr = 1e-10
I0422 07:40:08.493906  2460 solver.cpp:228] Iteration 1400, loss = 10728.3
I0422 07:40:08.494006  2460 solver.cpp:244]     Train net output #0: loss = 10728.3 (* 1 = 10728.3 loss)
I0422 07:40:08.494022  2460 sgd_solver.cpp:106] Iteration 1400, lr = 1e-10
I0422 07:40:38.474548  2460 solver.cpp:228] Iteration 1420, loss = 29055.6
I0422 07:40:38.474659  2460 solver.cpp:244]     Train net output #0: loss = 66807.9 (* 1 = 66807.9 loss)
I0422 07:40:38.474674  2460 sgd_solver.cpp:106] Iteration 1420, lr = 1e-10
I0422 07:41:08.442126  2460 solver.cpp:228] Iteration 1440, loss = 34327
I0422 07:41:08.442229  2460 solver.cpp:244]     Train net output #0: loss = 48597.3 (* 1 = 48597.3 loss)
I0422 07:41:08.442245  2460 sgd_solver.cpp:106] Iteration 1440, lr = 1e-10
I0422 07:41:38.418823  2460 solver.cpp:228] Iteration 1460, loss = 26938.1
I0422 07:41:38.418941  2460 solver.cpp:244]     Train net output #0: loss = 9407.99 (* 1 = 9407.99 loss)
I0422 07:41:38.418956  2460 sgd_solver.cpp:106] Iteration 1460, lr = 1e-10
I0422 07:42:08.394430  2460 solver.cpp:228] Iteration 1480, loss = 22166.2
I0422 07:42:08.394541  2460 solver.cpp:244]     Train net output #0: loss = 22860.4 (* 1 = 22860.4 loss)
I0422 07:42:08.394556  2460 sgd_solver.cpp:106] Iteration 1480, lr = 1e-10
I0422 07:42:46.052719  2460 solver.cpp:228] Iteration 1500, loss = 11243.4
I0422 07:42:46.052824  2460 solver.cpp:244]     Train net output #0: loss = 11243.4 (* 1 = 11243.4 loss)
I0422 07:42:46.052841  2460 sgd_solver.cpp:106] Iteration 1500, lr = 1e-10
I0422 07:43:16.032488  2460 solver.cpp:228] Iteration 1520, loss = 25937.7
I0422 07:43:16.032587  2460 solver.cpp:244]     Train net output #0: loss = 22336.2 (* 1 = 22336.2 loss)
I0422 07:43:16.032601  2460 sgd_solver.cpp:106] Iteration 1520, lr = 1e-10
I0422 07:43:45.999161  2460 solver.cpp:228] Iteration 1540, loss = 35983.8
I0422 07:43:45.999269  2460 solver.cpp:244]     Train net output #0: loss = 12959.3 (* 1 = 12959.3 loss)
I0422 07:43:45.999284  2460 sgd_solver.cpp:106] Iteration 1540, lr = 1e-10
I0422 07:44:15.976635  2460 solver.cpp:228] Iteration 1560, loss = 28112.4
I0422 07:44:15.976743  2460 solver.cpp:244]     Train net output #0: loss = 18643.5 (* 1 = 18643.5 loss)
I0422 07:44:15.976759  2460 sgd_solver.cpp:106] Iteration 1560, lr = 1e-10
I0422 07:44:45.956063  2460 solver.cpp:228] Iteration 1580, loss = 23472.9
I0422 07:44:45.956166  2460 solver.cpp:244]     Train net output #0: loss = 14098.2 (* 1 = 14098.2 loss)
I0422 07:44:45.956182  2460 sgd_solver.cpp:106] Iteration 1580, lr = 1e-10
I0422 07:45:23.603734  2460 solver.cpp:228] Iteration 1600, loss = 10915.8
I0422 07:45:23.603840  2460 solver.cpp:244]     Train net output #0: loss = 10915.8 (* 1 = 10915.8 loss)
I0422 07:45:23.603857  2460 sgd_solver.cpp:106] Iteration 1600, lr = 1e-10
I0422 07:45:53.584388  2460 solver.cpp:228] Iteration 1620, loss = 23143.8
I0422 07:45:53.584492  2460 solver.cpp:244]     Train net output #0: loss = 55591.7 (* 1 = 55591.7 loss)
I0422 07:45:53.584508  2460 sgd_solver.cpp:106] Iteration 1620, lr = 1e-10
I0422 07:46:23.563927  2460 solver.cpp:228] Iteration 1640, loss = 19530.5
I0422 07:46:23.564031  2460 solver.cpp:244]     Train net output #0: loss = 10378.2 (* 1 = 10378.2 loss)
I0422 07:46:23.564047  2460 sgd_solver.cpp:106] Iteration 1640, lr = 1e-10
I0422 07:46:53.537101  2460 solver.cpp:228] Iteration 1660, loss = 19798.7
I0422 07:46:53.537225  2460 solver.cpp:244]     Train net output #0: loss = 27959.4 (* 1 = 27959.4 loss)
I0422 07:46:53.537240  2460 sgd_solver.cpp:106] Iteration 1660, lr = 1e-10
I0422 07:47:23.509395  2460 solver.cpp:228] Iteration 1680, loss = 30034.6
I0422 07:47:23.509500  2460 solver.cpp:244]     Train net output #0: loss = 17064.7 (* 1 = 17064.7 loss)
I0422 07:47:23.509516  2460 sgd_solver.cpp:106] Iteration 1680, lr = 1e-10
I0422 07:48:01.162500  2460 solver.cpp:228] Iteration 1700, loss = 17318
I0422 07:48:01.162613  2460 solver.cpp:244]     Train net output #0: loss = 17318 (* 1 = 17318 loss)
I0422 07:48:01.162636  2460 sgd_solver.cpp:106] Iteration 1700, lr = 1e-10
I0422 07:48:31.142463  2460 solver.cpp:228] Iteration 1720, loss = 17827.1
I0422 07:48:31.142567  2460 solver.cpp:244]     Train net output #0: loss = 15886.4 (* 1 = 15886.4 loss)
I0422 07:48:31.142583  2460 sgd_solver.cpp:106] Iteration 1720, lr = 1e-10
I0422 07:49:01.110836  2460 solver.cpp:228] Iteration 1740, loss = 21741.9
I0422 07:49:01.110954  2460 solver.cpp:244]     Train net output #0: loss = 17564.6 (* 1 = 17564.6 loss)
I0422 07:49:01.110970  2460 sgd_solver.cpp:106] Iteration 1740, lr = 1e-10
I0422 07:49:31.084369  2460 solver.cpp:228] Iteration 1760, loss = 28492
I0422 07:49:31.084481  2460 solver.cpp:244]     Train net output #0: loss = 17125.7 (* 1 = 17125.7 loss)
I0422 07:49:31.084506  2460 sgd_solver.cpp:106] Iteration 1760, lr = 1e-10
I0422 07:50:01.058497  2460 solver.cpp:228] Iteration 1780, loss = 24386.1
I0422 07:50:01.058605  2460 solver.cpp:244]     Train net output #0: loss = 11183 (* 1 = 11183 loss)
I0422 07:50:01.058625  2460 sgd_solver.cpp:106] Iteration 1780, lr = 1e-10
I0422 07:50:38.709004  2460 solver.cpp:228] Iteration 1800, loss = 37208.9
I0422 07:50:38.709110  2460 solver.cpp:244]     Train net output #0: loss = 37208.9 (* 1 = 37208.9 loss)
I0422 07:50:38.709125  2460 sgd_solver.cpp:106] Iteration 1800, lr = 1e-10
I0422 07:51:08.683089  2460 solver.cpp:228] Iteration 1820, loss = 30648.4
I0422 07:51:08.683192  2460 solver.cpp:244]     Train net output #0: loss = 20703.1 (* 1 = 20703.1 loss)
I0422 07:51:08.683205  2460 sgd_solver.cpp:106] Iteration 1820, lr = 1e-10
I0422 07:51:38.661504  2460 solver.cpp:228] Iteration 1840, loss = 19326.4
I0422 07:51:38.661613  2460 solver.cpp:244]     Train net output #0: loss = 32498 (* 1 = 32498 loss)
I0422 07:51:38.661629  2460 sgd_solver.cpp:106] Iteration 1840, lr = 1e-10
I0422 07:52:08.643029  2460 solver.cpp:228] Iteration 1860, loss = 29540.1
I0422 07:52:08.643129  2460 solver.cpp:244]     Train net output #0: loss = 66926.4 (* 1 = 66926.4 loss)
I0422 07:52:08.643146  2460 sgd_solver.cpp:106] Iteration 1860, lr = 1e-10
I0422 07:52:38.619963  2460 solver.cpp:228] Iteration 1880, loss = 35784.1
I0422 07:52:38.620064  2460 solver.cpp:244]     Train net output #0: loss = 16186 (* 1 = 16186 loss)
I0422 07:52:38.620079  2460 sgd_solver.cpp:106] Iteration 1880, lr = 1e-10
I0422 07:53:16.256873  2460 solver.cpp:228] Iteration 1900, loss = 35126.6
I0422 07:53:16.256976  2460 solver.cpp:244]     Train net output #0: loss = 35126.6 (* 1 = 35126.6 loss)
I0422 07:53:16.256999  2460 sgd_solver.cpp:106] Iteration 1900, lr = 1e-10
I0422 07:53:46.242683  2460 solver.cpp:228] Iteration 1920, loss = 30179.9
I0422 07:53:46.242802  2460 solver.cpp:244]     Train net output #0: loss = 7393.76 (* 1 = 7393.76 loss)
I0422 07:53:46.242818  2460 sgd_solver.cpp:106] Iteration 1920, lr = 1e-10
I0422 07:54:16.232628  2460 solver.cpp:228] Iteration 1940, loss = 24042.5
I0422 07:54:16.232746  2460 solver.cpp:244]     Train net output #0: loss = 13666.2 (* 1 = 13666.2 loss)
I0422 07:54:16.232761  2460 sgd_solver.cpp:106] Iteration 1940, lr = 1e-10
I0422 07:54:46.208791  2460 solver.cpp:228] Iteration 1960, loss = 32676.7
I0422 07:54:46.208897  2460 solver.cpp:244]     Train net output #0: loss = 23552.7 (* 1 = 23552.7 loss)
I0422 07:54:46.208914  2460 sgd_solver.cpp:106] Iteration 1960, lr = 1e-10
I0422 07:55:16.185894  2460 solver.cpp:228] Iteration 1980, loss = 32479.4
I0422 07:55:16.186005  2460 solver.cpp:244]     Train net output #0: loss = 16432.5 (* 1 = 16432.5 loss)
I0422 07:55:16.186022  2460 sgd_solver.cpp:106] Iteration 1980, lr = 1e-10
I0422 07:55:44.667086  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_2000.caffemodel
I0422 07:55:47.936162  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_2000.solverstate
I0422 07:55:57.110579  2460 solver.cpp:228] Iteration 2000, loss = 17985.5
I0422 07:55:57.551044  2460 solver.cpp:244]     Train net output #0: loss = 17985.5 (* 1 = 17985.5 loss)
I0422 07:55:57.551091  2460 sgd_solver.cpp:106] Iteration 2000, lr = 1e-10
I0422 07:56:27.088361  2460 solver.cpp:228] Iteration 2020, loss = 28005.7
I0422 07:56:27.088475  2460 solver.cpp:244]     Train net output #0: loss = 11967.1 (* 1 = 11967.1 loss)
I0422 07:56:27.088491  2460 sgd_solver.cpp:106] Iteration 2020, lr = 1e-10
I0422 07:56:57.070498  2460 solver.cpp:228] Iteration 2040, loss = 28551.4
I0422 07:56:57.070608  2460 solver.cpp:244]     Train net output #0: loss = 11610.3 (* 1 = 11610.3 loss)
I0422 07:56:57.070631  2460 sgd_solver.cpp:106] Iteration 2040, lr = 1e-10
I0422 07:57:27.056941  2460 solver.cpp:228] Iteration 2060, loss = 29656.9
I0422 07:57:27.057067  2460 solver.cpp:244]     Train net output #0: loss = 12355.2 (* 1 = 12355.2 loss)
I0422 07:57:27.057086  2460 sgd_solver.cpp:106] Iteration 2060, lr = 1e-10
I0422 07:57:57.037716  2460 solver.cpp:228] Iteration 2080, loss = 20302.2
I0422 07:57:57.037824  2460 solver.cpp:244]     Train net output #0: loss = 32483.2 (* 1 = 32483.2 loss)
I0422 07:57:57.037848  2460 sgd_solver.cpp:106] Iteration 2080, lr = 1e-10
253 Iteration 1100 overall accuracy 0.925507555556
>>> 2017-04-22 07:32:13.712397 Iteration 1100 mean accuracy 0.919054137369
>>> 2017-04-22 07:32:13.712624 Iteration 1100 mean IU 0.857240884472
>>> 2017-04-22 07:32:13.712750 Iteration 1100 fwavacc 0.860919926611
>>> 2017-04-22 07:34:44.188755 Begin seg tests
>>> 2017-04-22 07:34:51.266769 Iteration 1200 loss 43098.7739258
>>> 2017-04-22 07:34:51.266858 Iteration 1200 overall accuracy 0.913095555556
>>> 2017-04-22 07:34:51.266914 Iteration 1200 mean accuracy 0.906818421594
>>> 2017-04-22 07:34:51.267108 Iteration 1200 mean IU 0.836928711357
>>> 2017-04-22 07:34:51.267212 Iteration 1200 fwavacc 0.838959013936
>>> 2017-04-22 07:37:21.746474 Begin seg tests
>>> 2017-04-22 07:37:28.834341 Iteration 1300 loss 23127.2137858
>>> 2017-04-22 07:37:28.834442 Iteration 1300 overall accuracy 0.958315555556
>>> 2017-04-22 07:37:28.834501 Iteration 1300 mean accuracy 0.958322513354
>>> 2017-04-22 07:37:28.834704 Iteration 1300 mean IU 0.919940719912
>>> 2017-04-22 07:37:28.834814 Iteration 1300 fwavacc 0.919970063999
>>> 2017-04-22 07:39:59.320341 Begin seg tests
>>> 2017-04-22 07:40:06.405192 Iteration 1400 loss 20661.4295247
>>> 2017-04-22 07:40:06.405304 Iteration 1400 overall accuracy 0.955067555556
>>> 2017-04-22 07:40:06.405372 Iteration 1400 mean accuracy 0.955080158602
>>> 2017-04-22 07:40:06.405575 Iteration 1400 mean IU 0.91398330302
>>> 2017-04-22 07:40:06.405684 Iteration 1400 fwavacc 0.914001821716
>>> 2017-04-22 07:42:36.873317 Begin seg tests
>>> 2017-04-22 07:42:43.962760 Iteration 1500 loss 18514.59139
>>> 2017-04-22 07:42:43.962858 Iteration 1500 overall accuracy 0.961526222222
>>> 2017-04-22 07:42:43.962916 Iteration 1500 mean accuracy 0.959273584902
>>> 2017-04-22 07:42:43.963163 Iteration 1500 mean IU 0.922714989403
>>> 2017-04-22 07:42:43.963277 Iteration 1500 fwavacc 0.925989766212
>>> 2017-04-22 07:45:14.430513 Begin seg tests
>>> 2017-04-22 07:45:21.516431 Iteration 1600 loss 26546.3604329
>>> 2017-04-22 07:45:21.516542 Iteration 1600 overall accuracy 0.941110222222
>>> 2017-04-22 07:45:21.516600 Iteration 1600 mean accuracy 0.940664943633
>>> 2017-04-22 07:45:21.516809 Iteration 1600 mean IU 0.888598574426
>>> 2017-04-22 07:45:21.516915 Iteration 1600 fwavacc 0.888713053553
>>> 2017-04-22 07:47:51.984800 Begin seg tests
>>> 2017-04-22 07:47:59.069047 Iteration 1700 loss 21642.3420003
>>> 2017-04-22 07:47:59.069149 Iteration 1700 overall accuracy 0.953280888889
>>> 2017-04-22 07:47:59.069207 Iteration 1700 mean accuracy 0.956686376111
>>> 2017-04-22 07:47:59.069422 Iteration 1700 mean IU 0.907872919902
>>> 2017-04-22 07:47:59.069532 Iteration 1700 fwavacc 0.911276855002
>>> 2017-04-22 07:50:29.543184 Begin seg tests
>>> 2017-04-22 07:50:36.623232 Iteration 1800 loss 22477.7508138
>>> 2017-04-22 07:50:36.623332 Iteration 1800 overall accuracy 0.949286666667
>>> 2017-04-22 07:50:36.623392 Iteration 1800 mean accuracy 0.946234679617
>>> 2017-04-22 07:50:36.623616 Iteration 1800 mean IU 0.900758536606
>>> 2017-04-22 07:50:36.623760 Iteration 1800 fwavacc 0.90344509064
>>> 2017-04-22 07:53:07.097679 Begin seg tests
>>> 2017-04-22 07:53:14.175335 Iteration 1900 loss 21574.141276
>>> 2017-04-22 07:53:14.175448 Iteration 1900 overall accuracy 0.958347555556
>>> 2017-04-22 07:53:14.175507 Iteration 1900 mean accuracy 0.960362230882
>>> 2017-04-22 07:53:14.175743 Iteration 1900 mean IU 0.917252289921
>>> 2017-04-22 07:53:14.175854 Iteration 1900 fwavacc 0.920413345719
>>> 2017-04-22 07:55:48.912021 Begin seg tests
>>> 2017-04-22 07:55:54.990483 Iteration 2000 loss 25905.4374186
>>> 2017-04-22 07:55:54.990588 Iteration 2000 overall accuracy 0.939294666667
>>> 2017-04-22 07:55:54.990659 Iteration 2000 mean accuracy 0.931303786063
>>> 2017-04-22 07:55:54.990865 Iteration 2000 mean IU 0.881141295092
>>> 2017-04-22 07:55:54.990969 Iteration 2000 fwavacc 0.884980920912
>>> 2017-04-22 07:58:25.522630 Begin seg tests
>>> 2017-04-22 07:58:32.607199 Iteration 2100 loss 15778.7869873
>>> 2017-04-22 07:58:32.607298 Iteration 2100 overall accuracy 0.966686222222
>>> 2017-04-22 07:58I0422 07:58:34.726475  2460 solver.cpp:228] Iteration 2100, loss = 20068.9
I0422 07:58:34.726575  2460 solver.cpp:244]     Train net output #0: loss = 20068.9 (* 1 = 20068.9 loss)
I0422 07:58:34.726590  2460 sgd_solver.cpp:106] Iteration 2100, lr = 1e-10
I0422 07:59:04.704546  2460 solver.cpp:228] Iteration 2120, loss = 21614.4
I0422 07:59:04.704663  2460 solver.cpp:244]     Train net output #0: loss = 13579.8 (* 1 = 13579.8 loss)
I0422 07:59:04.704679  2460 sgd_solver.cpp:106] Iteration 2120, lr = 1e-10
I0422 07:59:34.684927  2460 solver.cpp:228] Iteration 2140, loss = 20125
I0422 07:59:34.685044  2460 solver.cpp:244]     Train net output #0: loss = 80198.9 (* 1 = 80198.9 loss)
I0422 07:59:34.685060  2460 sgd_solver.cpp:106] Iteration 2140, lr = 1e-10
I0422 08:00:04.659739  2460 solver.cpp:228] Iteration 2160, loss = 24983.6
I0422 08:00:04.659847  2460 solver.cpp:244]     Train net output #0: loss = 10013.9 (* 1 = 10013.9 loss)
I0422 08:00:04.659862  2460 sgd_solver.cpp:106] Iteration 2160, lr = 1e-10
I0422 08:00:34.634093  2460 solver.cpp:228] Iteration 2180, loss = 20066.3
I0422 08:00:34.634191  2460 solver.cpp:244]     Train net output #0: loss = 10482.6 (* 1 = 10482.6 loss)
I0422 08:00:34.634207  2460 sgd_solver.cpp:106] Iteration 2180, lr = 1e-10
I0422 08:01:12.265650  2460 solver.cpp:228] Iteration 2200, loss = 24729.8
I0422 08:01:12.265766  2460 solver.cpp:244]     Train net output #0: loss = 24729.8 (* 1 = 24729.8 loss)
I0422 08:01:12.265782  2460 sgd_solver.cpp:106] Iteration 2200, lr = 1e-10
I0422 08:01:42.240918  2460 solver.cpp:228] Iteration 2220, loss = 34393.9
I0422 08:01:42.241030  2460 solver.cpp:244]     Train net output #0: loss = 30363.7 (* 1 = 30363.7 loss)
I0422 08:01:42.241055  2460 sgd_solver.cpp:106] Iteration 2220, lr = 1e-10
I0422 08:02:12.214934  2460 solver.cpp:228] Iteration 2240, loss = 25049.8
I0422 08:02:12.215049  2460 solver.cpp:244]     Train net output #0: loss = 43337.9 (* 1 = 43337.9 loss)
I0422 08:02:12.215065  2460 sgd_solver.cpp:106] Iteration 2240, lr = 1e-10
I0422 08:02:42.194424  2460 solver.cpp:228] Iteration 2260, loss = 22456.5
I0422 08:02:42.194530  2460 solver.cpp:244]     Train net output #0: loss = 34731.2 (* 1 = 34731.2 loss)
I0422 08:02:42.194546  2460 sgd_solver.cpp:106] Iteration 2260, lr = 1e-10
I0422 08:03:12.155395  2460 solver.cpp:228] Iteration 2280, loss = 30278.5
I0422 08:03:12.155508  2460 solver.cpp:244]     Train net output #0: loss = 13335 (* 1 = 13335 loss)
I0422 08:03:12.155536  2460 sgd_solver.cpp:106] Iteration 2280, lr = 1e-10
I0422 08:03:49.863440  2460 solver.cpp:228] Iteration 2300, loss = 7159.74
I0422 08:03:49.863539  2460 solver.cpp:244]     Train net output #0: loss = 7159.74 (* 1 = 7159.74 loss)
I0422 08:03:49.863555  2460 sgd_solver.cpp:106] Iteration 2300, lr = 1e-10
I0422 08:04:19.832878  2460 solver.cpp:228] Iteration 2320, loss = 22777.4
I0422 08:04:19.832972  2460 solver.cpp:244]     Train net output #0: loss = 20341.3 (* 1 = 20341.3 loss)
I0422 08:04:19.832986  2460 sgd_solver.cpp:106] Iteration 2320, lr = 1e-10
I0422 08:04:49.805229  2460 solver.cpp:228] Iteration 2340, loss = 28658.2
I0422 08:04:49.805343  2460 solver.cpp:244]     Train net output #0: loss = 21019.3 (* 1 = 21019.3 loss)
I0422 08:04:49.805358  2460 sgd_solver.cpp:106] Iteration 2340, lr = 1e-10
I0422 08:05:19.782049  2460 solver.cpp:228] Iteration 2360, loss = 27491.8
I0422 08:05:19.782153  2460 solver.cpp:244]     Train net output #0: loss = 27769.5 (* 1 = 27769.5 loss)
I0422 08:05:19.782169  2460 sgd_solver.cpp:106] Iteration 2360, lr = 1e-10
I0422 08:05:49.760797  2460 solver.cpp:228] Iteration 2380, loss = 24304.2
I0422 08:05:49.760903  2460 solver.cpp:244]     Train net output #0: loss = 14697.1 (* 1 = 14697.1 loss)
I0422 08:05:49.760918  2460 sgd_solver.cpp:106] Iteration 2380, lr = 1e-10
I0422 08:06:27.429998  2460 solver.cpp:228] Iteration 2400, loss = 19058.2
I0422 08:06:27.430119  2460 solver.cpp:244]     Train net output #0: loss = 19058.2 (* 1 = 19058.2 loss)
I0422 08:06:27.430138  2460 sgd_solver.cpp:106] Iteration 2400, lr = 1e-10
I0422 08:06:57.403251  2460 solver.cpp:228] Iteration 2420, loss = 30500.8
I0422 08:06:57.403352  2460 solver.cpp:244]     Train net output #0: loss = 35123.9 (* 1 = 35123.9 loss)
I0422 08:06:57.403368  2460 sgd_solver.cpp:106] Iteration 2420, lr = 1e-10
I0422 08:07:27.375434  2460 solver.cpp:228] Iteration 2440, loss = 27943.7
I0422 08:07:27.375550  2460 solver.cpp:244]     Train net output #0: loss = 16867.6 (* 1 = 16867.6 loss)
I0422 08:07:27.375564  2460 sgd_solver.cpp:106] Iteration 2440, lr = 1e-10
I0422 08:07:57.353816  2460 solver.cpp:228] Iteration 2460, loss = 27494.7
I0422 08:07:57.353927  2460 solver.cpp:244]     Train net output #0: loss = 10207.9 (* 1 = 10207.9 loss)
I0422 08:07:57.353951  2460 sgd_solver.cpp:106] Iteration 2460, lr = 1e-10
I0422 08:08:27.337218  2460 solver.cpp:228] Iteration 2480, loss = 21680.5
I0422 08:08:27.337324  2460 solver.cpp:244]     Train net output #0: loss = 50694.5 (* 1 = 50694.5 loss)
I0422 08:08:27.337339  2460 sgd_solver.cpp:106] Iteration 2480, lr = 1e-10
I0422 08:09:05.035727  2460 solver.cpp:228] Iteration 2500, loss = 128924
I0422 08:09:05.035837  2460 solver.cpp:244]     Train net output #0: loss = 128924 (* 1 = 128924 loss)
I0422 08:09:05.035864  2460 sgd_solver.cpp:106] Iteration 2500, lr = 1e-10
I0422 08:09:35.015967  2460 solver.cpp:228] Iteration 2520, loss = 17979
I0422 08:09:37.005007  2460 solver.cpp:244]     Train net output #0: loss = 13085.5 (* 1 = 13085.5 loss)
I0422 08:09:37.005040  2460 sgd_solver.cpp:106] Iteration 2520, lr = 1e-10
I0422 08:10:06.013236  2460 solver.cpp:228] Iteration 2540, loss = 26163
I0422 08:10:06.013339  2460 solver.cpp:244]     Train net output #0: loss = 29015.7 (* 1 = 29015.7 loss)
I0422 08:10:06.013357  2460 sgd_solver.cpp:106] Iteration 2540, lr = 1e-10
I0422 08:10:35.985206  2460 solver.cpp:228] Iteration 2560, loss = 17969.3
I0422 08:10:35.985316  2460 solver.cpp:244]     Train net output #0: loss = 17296.7 (* 1 = 17296.7 loss)
I0422 08:10:35.985332  2460 sgd_solver.cpp:106] Iteration 2560, lr = 1e-10
I0422 08:11:05.967708  2460 solver.cpp:228] Iteration 2580, loss = 20052.5
I0422 08:11:05.967805  2460 solver.cpp:244]     Train net output #0: loss = 54720.4 (* 1 = 54720.4 loss)
I0422 08:11:05.967820  2460 sgd_solver.cpp:106] Iteration 2580, lr = 1e-10
I0422 08:11:43.654193  2460 solver.cpp:228] Iteration 2600, loss = 19184.6
I0422 08:11:43.654309  2460 solver.cpp:244]     Train net output #0: loss = 19184.6 (* 1 = 19184.6 loss)
I0422 08:11:43.654325  2460 sgd_solver.cpp:106] Iteration 2600, lr = 1e-10
I0422 08:12:13.629467  2460 solver.cpp:228] Iteration 2620, loss = 16337.1
I0422 08:12:13.629570  2460 solver.cpp:244]     Train net output #0: loss = 12381.1 (* 1 = 12381.1 loss)
I0422 08:12:13.629585  2460 sgd_solver.cpp:106] Iteration 2620, lr = 1e-10
I0422 08:12:43.604743  2460 solver.cpp:228] Iteration 2640, loss = 22753.5
I0422 08:12:43.604853  2460 solver.cpp:244]     Train net output #0: loss = 52266.8 (* 1 = 52266.8 loss)
I0422 08:12:43.604878  2460 sgd_solver.cpp:106] Iteration 2640, lr = 1e-10
I0422 08:13:13.576581  2460 solver.cpp:228] Iteration 2660, loss = 19332.3
I0422 08:13:13.576691  2460 solver.cpp:244]     Train net output #0: loss = 29059.7 (* 1 = 29059.7 loss)
I0422 08:13:13.576707  2460 sgd_solver.cpp:106] Iteration 2660, lr = 1e-10
I0422 08:13:43.547369  2460 solver.cpp:228] Iteration 2680, loss = 23220.8
I0422 08:13:43.547494  2460 solver.cpp:244]     Train net output #0: loss = 52827.8 (* 1 = 52827.8 loss)
I0422 08:13:43.547518  2460 sgd_solver.cpp:106] Iteration 2680, lr = 1e-10
I0422 08:14:21.210670  2460 solver.cpp:228] Iteration 2700, loss = 28190.1
I0422 08:14:21.210782  2460 solver.cpp:244]     Train net output #0: loss = 28190.1 (* 1 = 28190.1 loss)
I0422 08:14:21.210796  2460 sgd_solver.cpp:106] Iteration 2700, lr = 1e-10
I0422 08:14:51.184252  2460 solver.cpp:228] Iteration 2720, loss = 15932.9
I0422 08:14:51.184351  2460 solver.cpp:244]     Train net output #0: loss = 9257.57 (* 1 = 9257.57 loss)
I0422 08:14:51.184365  2460 sgd_solver.cpp:106] Iteration 2720, lr = 1e-10
I0422 08:15:21.166502  2460 solver.cpp:228] Iteration 2740, loss = 20859.7
I0422 08:15:21.166609  2460 solver.cpp:244]     Train net output #0: loss = 10731.5 (* 1 = 10731.5 loss)
I0422 08:15:21.166623  2460 sgd_solver.cpp:106] Iteration 2740, lr = 1e-10
I0422 08:15:51.142675  2460 solver.cpp:228] Iteration 2760, loss = 30657.8
I0422 08:15:51.142778  2460 solver.cpp:244]     Train net output #0: loss = 21226.5 (* 1 = 21226.5 loss)
I0422 08:15:51.142793  2460 sgd_solver.cpp:106] Iteration 2760, lr = 1e-10
I0422 08:16:21.132344  2460 solver.cpp:228] Iteration 2780, loss = 18766.3
I0422 08:16:21.132462  2460 solver.cpp:244]     Train net output #0: loss = 31102.2 (* 1 = 31102.2 loss)
I0422 08:16:21.132477  2460 sgd_solver.cpp:106] Iteration 2780, lr = 1e-10
I0422 08:16:58.806114  2460 solver.cpp:228] Iteration 2800, loss = 12528
I0422 08:16:58.806234  2460 solver.cpp:244]     Train net output #0: loss = 12528 (* 1 = 12528 loss)
I0422 08:16:58.806257  2460 sgd_solver.cpp:106] Iteration 2800, lr = 1e-10
I0422 08:17:28.779340  2460 solver.cpp:228] Iteration 2820, loss = 17662.3
I0422 08:17:28.779451  2460 solver.cpp:244]     Train net output #0: loss = 25455.1 (* 1 = 25455.1 loss)
I0422 08:17:28.779466  2460 sgd_solver.cpp:106] Iteration 2820, lr = 1e-10
I0422 08:17:58.753571  2460 solver.cpp:228] Iteration 2840, loss = 26250.3
I0422 08:17:58.753676  2460 solver.cpp:244]     Train net output #0: loss = 16105.3 (* 1 = 16105.3 loss)
I0422 08:17:58.753702  2460 sgd_solver.cpp:106] Iteration 2840, lr = 1e-10
I0422 08:18:28.730010  2460 solver.cpp:228] Iteration 2860, loss = 24294.6
I0422 08:18:28.730125  2460 solver.cpp:244]     Train net output #0: loss = 26325.3 (* 1 = 26325.3 loss)
I0422 08:18:28.730139  2460 sgd_solver.cpp:106] Iteration 2860, lr = 1e-10
I0422 08:18:58.720824  2460 solver.cpp:228] Iteration 2880, loss = 18653.3
I0422 08:18:58.720927  2460 solver.cpp:244]     Train net output #0: loss = 6653.3 (* 1 = 6653.3 loss)
I0422 08:18:58.720942  2460 sgd_solver.cpp:106] Iteration 2880, lr = 1e-10
I0422 08:19:36.391460  2460 solver.cpp:228] Iteration 2900, loss = 16646.1
I0422 08:19:36.391564  2460 solver.cpp:244]     Train net output #0: loss = 16646.1 (* 1 = 16646.1 loss)
I0422 08:19:36.391584  2460 sgd_solver.cpp:106] Iteration 2900, lr = 1e-10
I0422 08:20:06.378509  2460 solver.cpp:228] Iteration 2920, loss = 22044.9
I0422 08:20:06.378617  2460 solver.cpp:244]     Train net output #0: loss = 8379.61 (* 1 = 8379.61 loss)
I0422 08:20:06.378644  2460 sgd_solver.cpp:106] Iteration 2920, lr = 1e-10
I0422 08:20:36.350836  2460 solver.cpp:228] Iteration 2940, loss = 16340.7
I0422 08:20:36.350960  2460 solver.cpp:244]     Train net output #0: loss = 8197.23 (* 1 = 8197.23 loss)
I0422 08:20:36.350975  2460 sgd_solver.cpp:106] Iteration 2940, lr = 1e-10
I0422 08:21:06.331054  2460 solver.cpp:228] Iteration 2960, loss = 18699.6
I0422 08:21:06.331164  2460 solver.cpp:244]     Train net output #0: loss = 10013.7 (* 1 = 10013.7 loss)
I0422 08:21:06.331181  2460 sgd_solver.cpp:106] Iteration 2960, lr = 1e-10
I0422 08:21:36.317647  2460 solver.cpp:228] Iteration 2980, loss = 25386.7
I0422 08:21:36.317754  2460 solver.cpp:244]     Train net output #0: loss = 10592 (* 1 = 10592 loss)
I0422 08:21:36.317770  2460 sgd_solver.cpp:106] Iteration 2980, lr = 1e-10
I0422 08:22:04.794858  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_3000.caffemodel
I0422 08:22:08.057288  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_3000.solverstate
I0422 08:22:17.275014  2460 solver.cpp:228] Iteration 3000, loss = 14558.1
I0422 08:22:17.275125  2460 solver.cpp:244]     Train net output #0: loss = 14558.1 (* 1 = 14558.1 loss)
I0422 08:22:17.275140  2460 sgd_solver.cpp:106] Iteration 3000, lr = 1e-10
I0422 08:22:47.255800  2460 solver.cpp:228] Iteration 3020, loss = 16363.5
I0422 08:22:47.255935  2460 solver.cpp:244]     Train net output #0: loss = 19176.2 (* 1 = 19176.2 loss)
I0422 08:22:47.255951  2460 sgd_solver.cpp:106] Iteration 3020, lr = 1e-10
I0422 08:23:17.224059  2460 solver.cpp:228] Iteration 3040, loss = 22728
I0422 08:23:17.224181  2460 solver.cpp:244]     Train net output #0: loss = 24946.6 (* 1 = 24946.6 loss)
I0422 08:23:17.224197  2460 sgd_solver.cpp:106] Iteration 3040, lr = 1e-10
I0422 08:23:47.200628  2460 solver.cpp:228] Iteration 3060, loss = 29163.4
I0422 08:23:47.200726  2460 solver.cpp:244]     Train net output #0: loss = 29287.3 (* 1 = 29287.3 loss)
I0422 08:23:47.200747  2460 sgd_solver.cpp:106] Iteration 3060, lr = 1e-10
I0422 08:24:17.167289  2460 solver.cpp:228] Iteration 3080, loss = 15043.8
I0422 08:24:17.167393  2460 solver.cpp:244]     Train net output #0: loss = 17482.1 (* 1 = 17482.1 loss)
I0422 08:24:17.167409  2460 sgd_solver.cpp:106] Iteration 3080, lr = 1e-10
:32.607357 Iteration 2100 mean accuracy 0.964270265597
>>> 2017-04-22 07:58:32.607656 Iteration 2100 mean IU 0.934650660163
>>> 2017-04-22 07:58:32.607762 Iteration 2100 fwavacc 0.935390204205
>>> 2017-04-22 08:01:03.114302 Begin seg tests
>>> 2017-04-22 08:01:10.197423 Iteration 2200 loss 24486.2108561
>>> 2017-04-22 08:01:10.197530 Iteration 2200 overall accuracy 0.945396
>>> 2017-04-22 08:01:10.197598 Iteration 2200 mean accuracy 0.941388442134
>>> 2017-04-22 08:01:10.197849 Iteration 2200 mean IU 0.89496342911
>>> 2017-04-22 08:01:10.197956 Iteration 2200 fwavacc 0.895958142906
>>> 2017-04-22 08:03:40.641784 Begin seg tests
>>> 2017-04-22 08:03:47.726765 Iteration 2300 loss 21309.9353841
>>> 2017-04-22 08:03:47.726865 Iteration 2300 overall accuracy 0.950751111111
>>> 2017-04-22 08:03:47.726924 Iteration 2300 mean accuracy 0.952335274877
>>> 2017-04-22 08:03:47.727149 Iteration 2300 mean IU 0.905834024224
>>> 2017-04-22 08:03:47.727276 Iteration 2300 fwavacc 0.906261803755
>>> 2017-04-22 08:06:18.238081 Begin seg tests
>>> 2017-04-22 08:06:25.321316 Iteration 2400 loss 19726.1333822
>>> 2017-04-22 08:06:25.321420 Iteration 2400 overall accuracy 0.956459555556
>>> 2017-04-22 08:06:25.321488 Iteration 2400 mean accuracy 0.954538262348
>>> 2017-04-22 08:06:25.321734 Iteration 2400 mean IU 0.915810922525
>>> 2017-04-22 08:06:25.321839 Iteration 2400 fwavacc 0.916412832033
>>> 2017-04-22 08:08:55.823953 Begin seg tests
>>> 2017-04-22 08:09:02.912660 Iteration 2500 loss 21658.7704264
>>> 2017-04-22 08:09:02.912772 Iteration 2500 overall accuracy 0.949859111111
>>> 2017-04-22 08:09:02.912839 Iteration 2500 mean accuracy 0.946051034333
>>> 2017-04-22 08:09:02.913049 Iteration 2500 mean IU 0.902415750781
>>> 2017-04-22 08:09:02.913153 Iteration 2500 fwavacc 0.904300729504
>>> 2017-04-22 08:11:34.457185 Begin seg tests
>>> 2017-04-22 08:11:41.539200 Iteration 2600 loss 18178.5891113
>>> 2017-04-22 08:11:41.539291 Iteration 2600 overall accuracy 0.958756
>>> 2017-04-22 08:11:41.539347 Iteration 2600 mean accuracy 0.95893780232
>>> 2017-04-22 08:11:41.539540 Iteration 2600 mean IU 0.919824905001
>>> 2017-04-22 08:11:41.539669 Iteration 2600 fwavacc 0.920875112719
>>> 2017-04-22 08:14:12.031003 Begin seg tests
>>> 2017-04-22 08:14:19.114232 Iteration 2700 loss 14294.7113444
>>> 2017-04-22 08:14:19.114368 Iteration 2700 overall accuracy 0.970565777778
>>> 2017-04-22 08:14:19.114458 Iteration 2700 mean accuracy 0.970988127382
>>> 2017-04-22 08:14:19.114782 Iteration 2700 mean IU 0.942708150118
>>> 2017-04-22 08:14:19.114970 Iteration 2700 fwavacc 0.942841150789
>>> 2017-04-22 08:16:49.615847 Begin seg tests
>>> 2017-04-22 08:16:56.696987 Iteration 2800 loss 17913.4505208
>>> 2017-04-22 08:16:56.697102 Iteration 2800 overall accuracy 0.961128
>>> 2017-04-22 08:16:56.697168 Iteration 2800 mean accuracy 0.961143000375
>>> 2017-04-22 08:16:56.697391 Iteration 2800 mean IU 0.924122783933
>>> 2017-04-22 08:16:56.697496 Iteration 2800 fwavacc 0.925250365588
>>> 2017-04-22 08:19:27.199930 Begin seg tests
>>> 2017-04-22 08:19:34.280480 Iteration 2900 loss 12029.7788086
>>> 2017-04-22 08:19:34.280584 Iteration 2900 overall accuracy 0.975272
>>> 2017-04-22 08:19:34.280651 Iteration 2900 mean accuracy 0.975080688998
>>> 2017-04-22 08:19:34.280871 Iteration 2900 mean IU 0.951703114197
>>> 2017-04-22 08:19:34.280980 Iteration 2900 fwavacc 0.951720984762
>>> 2017-04-22 08:22:09.036682 Begin seg tests
>>> 2017-04-22 08:22:15.120666 Iteration 3000 loss 20024.651001
>>> 2017-04-22 08:22:15.120772 Iteration 3000 overall accuracy 0.954046222222
>>> 2017-04-22 08:22:15.120832 Iteration 3000 mean accuracy 0.953517138486
>>> 2017-04-22 08:22:15.121055 Iteration 3000 mean IU 0.909518627111
>>> 2017-04-22 08:22:15.121197 Iteration 3000 fwavacc 0.912337051901
>>> 2017-04-22 08:24:45.657974 Begin seg tests
>>> 2017-04-22 08:24:52.743171 Iteration 3100 loss 16719.1233724
>>> 2017-04-22 08:24:52.743286 Iteration 3100 overall accuracy 0.962995555556
>>> 2017-04-22 08:24:52.743353 Iteration 3100 mean accuracy 0.961565746045
>>> 2017-04-22 08:24:52.743575 IteratiI0422 08:24:54.862601  2460 solver.cpp:228] Iteration 3100, loss = 10023.2
I0422 08:24:54.862702  2460 solver.cpp:244]     Train net output #0: loss = 10023.2 (* 1 = 10023.2 loss)
I0422 08:24:54.862717  2460 sgd_solver.cpp:106] Iteration 3100, lr = 1e-10
I0422 08:25:24.834147  2460 solver.cpp:228] Iteration 3120, loss = 17801.2
I0422 08:25:24.834259  2460 solver.cpp:244]     Train net output #0: loss = 27243.2 (* 1 = 27243.2 loss)
I0422 08:25:24.834275  2460 sgd_solver.cpp:106] Iteration 3120, lr = 1e-10
I0422 08:25:54.798213  2460 solver.cpp:228] Iteration 3140, loss = 19007.3
I0422 08:25:54.798327  2460 solver.cpp:244]     Train net output #0: loss = 22199.5 (* 1 = 22199.5 loss)
I0422 08:25:54.798343  2460 sgd_solver.cpp:106] Iteration 3140, lr = 1e-10
I0422 08:26:24.774408  2460 solver.cpp:228] Iteration 3160, loss = 16512.1
I0422 08:26:24.774511  2460 solver.cpp:244]     Train net output #0: loss = 27969.3 (* 1 = 27969.3 loss)
I0422 08:26:24.774526  2460 sgd_solver.cpp:106] Iteration 3160, lr = 1e-10
I0422 08:26:54.760887  2460 solver.cpp:228] Iteration 3180, loss = 14184.3
I0422 08:26:54.760999  2460 solver.cpp:244]     Train net output #0: loss = 18903.5 (* 1 = 18903.5 loss)
I0422 08:26:54.761021  2460 sgd_solver.cpp:106] Iteration 3180, lr = 1e-10
I0422 08:27:32.429491  2460 solver.cpp:228] Iteration 3200, loss = 14731.1
I0422 08:27:32.429610  2460 solver.cpp:244]     Train net output #0: loss = 14731.1 (* 1 = 14731.1 loss)
I0422 08:27:32.429625  2460 sgd_solver.cpp:106] Iteration 3200, lr = 1e-10
I0422 08:28:02.412153  2460 solver.cpp:228] Iteration 3220, loss = 15352.9
I0422 08:28:02.412261  2460 solver.cpp:244]     Train net output #0: loss = 23712.7 (* 1 = 23712.7 loss)
I0422 08:28:02.412278  2460 sgd_solver.cpp:106] Iteration 3220, lr = 1e-10
I0422 08:28:32.384264  2460 solver.cpp:228] Iteration 3240, loss = 21557.8
I0422 08:28:32.384377  2460 solver.cpp:244]     Train net output #0: loss = 72499 (* 1 = 72499 loss)
I0422 08:28:32.384392  2460 sgd_solver.cpp:106] Iteration 3240, lr = 1e-10
I0422 08:29:02.367957  2460 solver.cpp:228] Iteration 3260, loss = 21424.8
I0422 08:29:02.368065  2460 solver.cpp:244]     Train net output #0: loss = 14367.6 (* 1 = 14367.6 loss)
I0422 08:29:02.368080  2460 sgd_solver.cpp:106] Iteration 3260, lr = 1e-10
I0422 08:29:32.344126  2460 solver.cpp:228] Iteration 3280, loss = 19009.4
I0422 08:29:32.344233  2460 solver.cpp:244]     Train net output #0: loss = 13671.5 (* 1 = 13671.5 loss)
I0422 08:29:32.344250  2460 sgd_solver.cpp:106] Iteration 3280, lr = 1e-10
I0422 08:30:10.023779  2460 solver.cpp:228] Iteration 3300, loss = 7775.55
I0422 08:30:10.023891  2460 solver.cpp:244]     Train net output #0: loss = 7775.55 (* 1 = 7775.55 loss)
I0422 08:30:10.023911  2460 sgd_solver.cpp:106] Iteration 3300, lr = 1e-10
I0422 08:30:40.000355  2460 solver.cpp:228] Iteration 3320, loss = 20123.7
I0422 08:30:40.000463  2460 solver.cpp:244]     Train net output #0: loss = 24938.5 (* 1 = 24938.5 loss)
I0422 08:30:40.000478  2460 sgd_solver.cpp:106] Iteration 3320, lr = 1e-10
I0422 08:31:09.969121  2460 solver.cpp:228] Iteration 3340, loss = 22918.2
I0422 08:31:09.969231  2460 solver.cpp:244]     Train net output #0: loss = 59494.7 (* 1 = 59494.7 loss)
I0422 08:31:09.969255  2460 sgd_solver.cpp:106] Iteration 3340, lr = 1e-10
I0422 08:31:39.946418  2460 solver.cpp:228] Iteration 3360, loss = 23047.3
I0422 08:31:39.946526  2460 solver.cpp:244]     Train net output #0: loss = 7584.9 (* 1 = 7584.9 loss)
I0422 08:31:39.946542  2460 sgd_solver.cpp:106] Iteration 3360, lr = 1e-10
I0422 08:32:09.917276  2460 solver.cpp:228] Iteration 3380, loss = 18798.4
I0422 08:32:09.917371  2460 solver.cpp:244]     Train net output #0: loss = 20859.3 (* 1 = 20859.3 loss)
I0422 08:32:09.917387  2460 sgd_solver.cpp:106] Iteration 3380, lr = 1e-10
I0422 08:32:47.567690  2460 solver.cpp:228] Iteration 3400, loss = 17730.4
I0422 08:32:47.567798  2460 solver.cpp:244]     Train net output #0: loss = 17730.4 (* 1 = 17730.4 loss)
I0422 08:32:47.567814  2460 sgd_solver.cpp:106] Iteration 3400, lr = 1e-10
I0422 08:33:17.539273  2460 solver.cpp:228] Iteration 3420, loss = 15043.6
I0422 08:33:17.539386  2460 solver.cpp:244]     Train net output #0: loss = 7765.49 (* 1 = 7765.49 loss)
I0422 08:33:17.539402  2460 sgd_solver.cpp:106] Iteration 3420, lr = 1e-10
I0422 08:33:47.511293  2460 solver.cpp:228] Iteration 3440, loss = 16381
I0422 08:33:47.511389  2460 solver.cpp:244]     Train net output #0: loss = 11339 (* 1 = 11339 loss)
I0422 08:33:47.511404  2460 sgd_solver.cpp:106] Iteration 3440, lr = 1e-10
I0422 08:34:17.478936  2460 solver.cpp:228] Iteration 3460, loss = 20973.5
I0422 08:34:17.479039  2460 solver.cpp:244]     Train net output #0: loss = 9934.03 (* 1 = 9934.03 loss)
I0422 08:34:17.479055  2460 sgd_solver.cpp:106] Iteration 3460, lr = 1e-10
I0422 08:34:47.453938  2460 solver.cpp:228] Iteration 3480, loss = 20671.9
I0422 08:34:47.454061  2460 solver.cpp:244]     Train net output #0: loss = 9237.06 (* 1 = 9237.06 loss)
I0422 08:34:47.454083  2460 sgd_solver.cpp:106] Iteration 3480, lr = 1e-10
I0422 08:35:25.142577  2460 solver.cpp:228] Iteration 3500, loss = 19040.3
I0422 08:35:25.142673  2460 solver.cpp:244]     Train net output #0: loss = 19040.3 (* 1 = 19040.3 loss)
I0422 08:35:25.142688  2460 sgd_solver.cpp:106] Iteration 3500, lr = 1e-10
I0422 08:35:55.118415  2460 solver.cpp:228] Iteration 3520, loss = 16067.5
I0422 08:35:55.118526  2460 solver.cpp:244]     Train net output #0: loss = 10716 (* 1 = 10716 loss)
I0422 08:35:55.118551  2460 sgd_solver.cpp:106] Iteration 3520, lr = 1e-10
I0422 08:36:25.101999  2460 solver.cpp:228] Iteration 3540, loss = 17480.3
I0422 08:36:25.102123  2460 solver.cpp:244]     Train net output #0: loss = 11124.4 (* 1 = 11124.4 loss)
I0422 08:36:25.102138  2460 sgd_solver.cpp:106] Iteration 3540, lr = 1e-10
I0422 08:36:55.079411  2460 solver.cpp:228] Iteration 3560, loss = 21462.2
I0422 08:36:55.079536  2460 solver.cpp:244]     Train net output #0: loss = 19149.2 (* 1 = 19149.2 loss)
I0422 08:36:55.079555  2460 sgd_solver.cpp:106] Iteration 3560, lr = 1e-10
I0422 08:37:25.058457  2460 solver.cpp:228] Iteration 3580, loss = 16759
I0422 08:37:25.058585  2460 solver.cpp:244]     Train net output #0: loss = 13878.3 (* 1 = 13878.3 loss)
I0422 08:37:25.058604  2460 sgd_solver.cpp:106] Iteration 3580, lr = 1e-10
I0422 08:38:02.754284  2460 solver.cpp:228] Iteration 3600, loss = 15903.4
I0422 08:38:02.754389  2460 solver.cpp:244]     Train net output #0: loss = 15903.4 (* 1 = 15903.4 loss)
I0422 08:38:02.754405  2460 sgd_solver.cpp:106] Iteration 3600, lr = 1e-10
I0422 08:38:32.731937  2460 solver.cpp:228] Iteration 3620, loss = 18291.4
I0422 08:38:32.732049  2460 solver.cpp:244]     Train net output #0: loss = 10045.5 (* 1 = 10045.5 loss)
I0422 08:38:32.732074  2460 sgd_solver.cpp:106] Iteration 3620, lr = 1e-10
I0422 08:39:02.709463  2460 solver.cpp:228] Iteration 3640, loss = 15175.4
I0422 08:39:02.709559  2460 solver.cpp:244]     Train net output #0: loss = 14322.7 (* 1 = 14322.7 loss)
I0422 08:39:02.709574  2460 sgd_solver.cpp:106] Iteration 3640, lr = 1e-10
I0422 08:39:32.688086  2460 solver.cpp:228] Iteration 3660, loss = 16873.2
I0422 08:39:32.688194  2460 solver.cpp:244]     Train net output #0: loss = 13897.6 (* 1 = 13897.6 loss)
I0422 08:39:32.688210  2460 sgd_solver.cpp:106] Iteration 3660, lr = 1e-10
I0422 08:40:02.667881  2460 solver.cpp:228] Iteration 3680, loss = 15632.1
I0422 08:40:02.667996  2460 solver.cpp:244]     Train net output #0: loss = 7085.02 (* 1 = 7085.02 loss)
I0422 08:40:02.668016  2460 sgd_solver.cpp:106] Iteration 3680, lr = 1e-10
I0422 08:40:40.340081  2460 solver.cpp:228] Iteration 3700, loss = 31140.7
I0422 08:40:40.340186  2460 solver.cpp:244]     Train net output #0: loss = 31140.7 (* 1 = 31140.7 loss)
I0422 08:40:40.340200  2460 sgd_solver.cpp:106] Iteration 3700, lr = 1e-10
I0422 08:41:10.309700  2460 solver.cpp:228] Iteration 3720, loss = 15808.9
I0422 08:41:10.309797  2460 solver.cpp:244]     Train net output #0: loss = 13608.5 (* 1 = 13608.5 loss)
I0422 08:41:10.309813  2460 sgd_solver.cpp:106] Iteration 3720, lr = 1e-10
I0422 08:41:40.283016  2460 solver.cpp:228] Iteration 3740, loss = 18581.2
I0422 08:41:40.283129  2460 solver.cpp:244]     Train net output #0: loss = 11303.3 (* 1 = 11303.3 loss)
I0422 08:41:40.283145  2460 sgd_solver.cpp:106] Iteration 3740, lr = 1e-10
I0422 08:42:10.256525  2460 solver.cpp:228] Iteration 3760, loss = 16904.7
I0422 08:42:10.256625  2460 solver.cpp:244]     Train net output #0: loss = 10985.4 (* 1 = 10985.4 loss)
I0422 08:42:10.256640  2460 sgd_solver.cpp:106] Iteration 3760, lr = 1e-10
I0422 08:42:40.235594  2460 solver.cpp:228] Iteration 3780, loss = 16861.9
I0422 08:42:40.235718  2460 solver.cpp:244]     Train net output #0: loss = 12061.5 (* 1 = 12061.5 loss)
I0422 08:42:40.235735  2460 sgd_solver.cpp:106] Iteration 3780, lr = 1e-10
I0422 08:43:17.918860  2460 solver.cpp:228] Iteration 3800, loss = 54624.1
I0422 08:43:17.918963  2460 solver.cpp:244]     Train net output #0: loss = 54624.1 (* 1 = 54624.1 loss)
I0422 08:43:17.918980  2460 sgd_solver.cpp:106] Iteration 3800, lr = 1e-10
I0422 08:43:47.891543  2460 solver.cpp:228] Iteration 3820, loss = 16747.8
I0422 08:43:47.891665  2460 solver.cpp:244]     Train net output #0: loss = 17889.2 (* 1 = 17889.2 loss)
I0422 08:43:47.891690  2460 sgd_solver.cpp:106] Iteration 3820, lr = 1e-10
I0422 08:44:17.869299  2460 solver.cpp:228] Iteration 3840, loss = 15147.8
I0422 08:44:17.869412  2460 solver.cpp:244]     Train net output #0: loss = 9799.69 (* 1 = 9799.69 loss)
I0422 08:44:17.869427  2460 sgd_solver.cpp:106] Iteration 3840, lr = 1e-10
I0422 08:44:47.853019  2460 solver.cpp:228] Iteration 3860, loss = 17901.2
I0422 08:44:47.853129  2460 solver.cpp:244]     Train net output #0: loss = 13810.6 (* 1 = 13810.6 loss)
I0422 08:44:47.853144  2460 sgd_solver.cpp:106] Iteration 3860, lr = 1e-10
I0422 08:45:17.827224  2460 solver.cpp:228] Iteration 3880, loss = 16231.4
I0422 08:45:17.827339  2460 solver.cpp:244]     Train net output #0: loss = 7724.1 (* 1 = 7724.1 loss)
I0422 08:45:17.827355  2460 sgd_solver.cpp:106] Iteration 3880, lr = 1e-10
I0422 08:45:55.526635  2460 solver.cpp:228] Iteration 3900, loss = 11851.4
I0422 08:45:55.526746  2460 solver.cpp:244]     Train net output #0: loss = 11851.4 (* 1 = 11851.4 loss)
I0422 08:45:55.526768  2460 sgd_solver.cpp:106] Iteration 3900, lr = 1e-10
I0422 08:46:25.504585  2460 solver.cpp:228] Iteration 3920, loss = 17598.9
I0422 08:46:25.504709  2460 solver.cpp:244]     Train net output #0: loss = 26483.6 (* 1 = 26483.6 loss)
I0422 08:46:25.504732  2460 sgd_solver.cpp:106] Iteration 3920, lr = 1e-10
I0422 08:46:55.478464  2460 solver.cpp:228] Iteration 3940, loss = 15463.6
I0422 08:46:55.478571  2460 solver.cpp:244]     Train net output #0: loss = 16517 (* 1 = 16517 loss)
I0422 08:46:55.478587  2460 sgd_solver.cpp:106] Iteration 3940, lr = 1e-10
I0422 08:47:25.449832  2460 solver.cpp:228] Iteration 3960, loss = 18015.1
I0422 08:47:25.449957  2460 solver.cpp:244]     Train net output #0: loss = 20027 (* 1 = 20027 loss)
I0422 08:47:25.449983  2460 sgd_solver.cpp:106] Iteration 3960, lr = 1e-10
I0422 08:47:55.431170  2460 solver.cpp:228] Iteration 3980, loss = 19785.6
I0422 08:47:55.431264  2460 solver.cpp:244]     Train net output #0: loss = 10424.5 (* 1 = 10424.5 loss)
I0422 08:47:55.431282  2460 sgd_solver.cpp:106] Iteration 3980, lr = 1e-10
I0422 08:48:23.915101  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_4000.caffemodel
I0422 08:48:27.183872  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_4000.solverstate
I0422 08:48:36.447494  2460 solver.cpp:228] Iteration 4000, loss = 10946.8
I0422 08:48:36.447597  2460 solver.cpp:244]     Train net output #0: loss = 10946.8 (* 1 = 10946.8 loss)
I0422 08:48:36.447634  2460 sgd_solver.cpp:106] Iteration 4000, lr = 1e-10
I0422 08:49:06.417438  2460 solver.cpp:228] Iteration 4020, loss = 16980.9
I0422 08:49:06.417532  2460 solver.cpp:244]     Train net output #0: loss = 7689.74 (* 1 = 7689.74 loss)
I0422 08:49:06.417548  2460 sgd_solver.cpp:106] Iteration 4020, lr = 1e-10
I0422 08:49:36.395717  2460 solver.cpp:228] Iteration 4040, loss = 18180.8
I0422 08:49:36.395823  2460 solver.cpp:244]     Train net output #0: loss = 20311.5 (* 1 = 20311.5 loss)
I0422 08:49:36.395838  2460 sgd_solver.cpp:106] Iteration 4040, lr = 1e-10
I0422 08:50:06.374591  2460 solver.cpp:228] Iteration 4060, loss = 18190.1
I0422 08:50:06.374721  2460 solver.cpp:244]     Train net output #0: loss = 43115.1 (* 1 = 43115.1 loss)
I0422 08:50:06.374737  2460 sgd_solver.cpp:106] Iteration 4060, lr = 1e-10
I0422 08:50:36.356204  2460 solver.cpp:228] Iteration 4080, loss = 13882.8
I0422 08:50:36.356325  2460 solver.cpp:244]     Train net output #0: loss = 8857.21 (* 1 = 8857.21 loss)
I0422 08:50:36.356341  2460 sgd_solver.cpp:106] Iteration 4080, lr = 1e-10
on 3100 mean IU 0.92819030091
>>> 2017-04-22 08:24:52.743840 Iteration 3100 fwavacc 0.928526328729
>>> 2017-04-22 08:27:23.233892 Begin seg tests
>>> 2017-04-22 08:27:30.316545 Iteration 3200 loss 17815.7278239
>>> 2017-04-22 08:27:30.316641 Iteration 3200 overall accuracy 0.966337777778
>>> 2017-04-22 08:27:30.316698 Iteration 3200 mean accuracy 0.96436691366
>>> 2017-04-22 08:27:30.316902 Iteration 3200 mean IU 0.933728272484
>>> 2017-04-22 08:27:30.317006 Iteration 3200 fwavacc 0.934806813509
>>> 2017-04-22 08:30:00.826147 Begin seg tests
>>> 2017-04-22 08:30:07.910782 Iteration 3300 loss 13128.9952799
>>> 2017-04-22 08:30:07.910880 Iteration 3300 overall accuracy 0.974621777778
>>> 2017-04-22 08:30:07.910942 Iteration 3300 mean accuracy 0.974224789414
>>> 2017-04-22 08:30:07.911155 Iteration 3300 mean IU 0.950305277329
>>> 2017-04-22 08:30:07.911270 Iteration 3300 fwavacc 0.950488498558
>>> 2017-04-22 08:32:38.390129 Begin seg tests
>>> 2017-04-22 08:32:45.470259 Iteration 3400 loss 19591.2390951
>>> 2017-04-22 08:32:45.470352 Iteration 3400 overall accuracy 0.957945333333
>>> 2017-04-22 08:32:45.470409 Iteration 3400 mean accuracy 0.959878109582
>>> 2017-04-22 08:32:45.470604 Iteration 3400 mean IU 0.91903407989
>>> 2017-04-22 08:32:45.470719 Iteration 3400 fwavacc 0.919414244847
>>> 2017-04-22 08:35:15.939069 Begin seg tests
>>> 2017-04-22 08:35:23.025137 Iteration 3500 loss 18715.3152669
>>> 2017-04-22 08:35:23.025257 Iteration 3500 overall accuracy 0.960114666667
>>> 2017-04-22 08:35:23.025325 Iteration 3500 mean accuracy 0.956317710837
>>> 2017-04-22 08:35:23.025536 Iteration 3500 mean IU 0.920321946703
>>> 2017-04-22 08:35:23.025645 Iteration 3500 fwavacc 0.92322491729
>>> 2017-04-22 08:37:53.544306 Begin seg tests
>>> 2017-04-22 08:38:00.631259 Iteration 3600 loss 15139.1005046
>>> 2017-04-22 08:38:00.631370 Iteration 3600 overall accuracy 0.969229333333
>>> 2017-04-22 08:38:00.631436 Iteration 3600 mean accuracy 0.965075531734
>>> 2017-04-22 08:38:00.631678 Iteration 3600 mean IU 0.934000376824
>>> 2017-04-22 08:38:00.631793 Iteration 3600 fwavacc 0.940446939755
>>> 2017-04-22 08:40:31.148053 Begin seg tests
>>> 2017-04-22 08:40:38.230009 Iteration 3700 loss 14278.9525553
>>> 2017-04-22 08:40:38.230099 Iteration 3700 overall accuracy 0.970861777778
>>> 2017-04-22 08:40:38.230154 Iteration 3700 mean accuracy 0.969483431589
>>> 2017-04-22 08:40:38.230345 Iteration 3700 mean IU 0.940697837937
>>> 2017-04-22 08:40:38.230450 Iteration 3700 fwavacc 0.943456413828
>>> 2017-04-22 08:43:08.710246 Begin seg tests
>>> 2017-04-22 08:43:15.791817 Iteration 3800 loss 14940.8697103
>>> 2017-04-22 08:43:15.791930 Iteration 3800 overall accuracy 0.967849333333
>>> 2017-04-22 08:43:15.791989 Iteration 3800 mean accuracy 0.967830912214
>>> 2017-04-22 08:43:15.792197 Iteration 3800 mean IU 0.937695995689
>>> 2017-04-22 08:43:15.792301 Iteration 3800 fwavacc 0.937697472393
>>> 2017-04-22 08:45:46.305305 Begin seg tests
>>> 2017-04-22 08:45:53.387955 Iteration 3900 loss 12179.3892822
>>> 2017-04-22 08:45:53.388051 Iteration 3900 overall accuracy 0.974539111111
>>> 2017-04-22 08:45:53.388114 Iteration 3900 mean accuracy 0.974463536483
>>> 2017-04-22 08:45:53.388325 Iteration 3900 mean IU 0.950198017589
>>> 2017-04-22 08:45:53.388443 Iteration 3900 fwavacc 0.950346345394
>>> 2017-04-22 08:48:28.195690 Begin seg tests
>>> 2017-04-22 08:48:34.281106 Iteration 4000 loss 15952.0330811
>>> 2017-04-22 08:48:34.281243 Iteration 4000 overall accuracy 0.965659555556
>>> 2017-04-22 08:48:34.281312 Iteration 4000 mean accuracy 0.965228024427
>>> 2017-04-22 08:48:34.281582 Iteration 4000 mean IU 0.933437583185
>>> 2017-04-22 08:48:34.281700 Iteration 4000 fwavacc 0.933576193719
>>> 2017-04-22 08:51:04.831698 Begin seg tests
>>> 2017-04-22 08:51:11.910685 Iteration 4100 loss 14037.4761556
>>> 2017-04-22 08:51:11.910782 Iteration 4100 overall accuracy 0.969789777778
>>> 2017-04-22 08:51:11.910842 Iteration 4100 mean accuracy 0.969746634847
>>> 2017-04-22 08:51:11.911046 Iteration 4100 mean IU 0.941311352087
>>> 2017-04-22 08:51:11.911166 IteratioI0422 08:51:14.006256  2460 solver.cpp:228] Iteration 4100, loss = 17790.2
I0422 08:51:14.006359  2460 solver.cpp:244]     Train net output #0: loss = 17790.2 (* 1 = 17790.2 loss)
I0422 08:51:14.006373  2460 sgd_solver.cpp:106] Iteration 4100, lr = 1e-10
I0422 08:51:43.975881  2460 solver.cpp:228] Iteration 4120, loss = 14247.8
I0422 08:51:43.975999  2460 solver.cpp:244]     Train net output #0: loss = 12599 (* 1 = 12599 loss)
I0422 08:51:43.976019  2460 sgd_solver.cpp:106] Iteration 4120, lr = 1e-10
I0422 08:52:13.958432  2460 solver.cpp:228] Iteration 4140, loss = 13021.7
I0422 08:52:13.958552  2460 solver.cpp:244]     Train net output #0: loss = 11973.6 (* 1 = 11973.6 loss)
I0422 08:52:13.958571  2460 sgd_solver.cpp:106] Iteration 4140, lr = 1e-10
I0422 08:52:43.933051  2460 solver.cpp:228] Iteration 4160, loss = 14424.8
I0422 08:52:43.933151  2460 solver.cpp:244]     Train net output #0: loss = 13373.6 (* 1 = 13373.6 loss)
I0422 08:52:43.933167  2460 sgd_solver.cpp:106] Iteration 4160, lr = 1e-10
I0422 08:53:13.913929  2460 solver.cpp:228] Iteration 4180, loss = 13584.7
I0422 08:53:13.914048  2460 solver.cpp:244]     Train net output #0: loss = 8252.71 (* 1 = 8252.71 loss)
I0422 08:53:13.914063  2460 sgd_solver.cpp:106] Iteration 4180, lr = 1e-10
I0422 08:53:51.585611  2460 solver.cpp:228] Iteration 4200, loss = 11734.8
I0422 08:53:51.585718  2460 solver.cpp:244]     Train net output #0: loss = 11734.8 (* 1 = 11734.8 loss)
I0422 08:53:51.585738  2460 sgd_solver.cpp:106] Iteration 4200, lr = 1e-10
I0422 08:54:21.561064  2460 solver.cpp:228] Iteration 4220, loss = 13948.9
I0422 08:54:21.561187  2460 solver.cpp:244]     Train net output #0: loss = 13617.8 (* 1 = 13617.8 loss)
I0422 08:54:21.561210  2460 sgd_solver.cpp:106] Iteration 4220, lr = 1e-10
I0422 08:54:51.552088  2460 solver.cpp:228] Iteration 4240, loss = 14931.2
I0422 08:54:51.552188  2460 solver.cpp:244]     Train net output #0: loss = 15586.4 (* 1 = 15586.4 loss)
I0422 08:54:51.552203  2460 sgd_solver.cpp:106] Iteration 4240, lr = 1e-10
I0422 08:55:21.529548  2460 solver.cpp:228] Iteration 4260, loss = 15053.6
I0422 08:55:21.529650  2460 solver.cpp:244]     Train net output #0: loss = 15175.5 (* 1 = 15175.5 loss)
I0422 08:55:21.529666  2460 sgd_solver.cpp:106] Iteration 4260, lr = 1e-10
I0422 08:55:51.506995  2460 solver.cpp:228] Iteration 4280, loss = 18496.1
I0422 08:55:51.507093  2460 solver.cpp:244]     Train net output #0: loss = 9481.12 (* 1 = 9481.12 loss)
I0422 08:55:51.507109  2460 sgd_solver.cpp:106] Iteration 4280, lr = 1e-10
I0422 08:56:29.164254  2460 solver.cpp:228] Iteration 4300, loss = 9037.28
I0422 08:56:29.164372  2460 solver.cpp:244]     Train net output #0: loss = 9037.28 (* 1 = 9037.28 loss)
I0422 08:56:29.164388  2460 sgd_solver.cpp:106] Iteration 4300, lr = 1e-10
I0422 08:56:59.145759  2460 solver.cpp:228] Iteration 4320, loss = 14400.8
I0422 08:56:59.145875  2460 solver.cpp:244]     Train net output #0: loss = 12825.5 (* 1 = 12825.5 loss)
I0422 08:56:59.145890  2460 sgd_solver.cpp:106] Iteration 4320, lr = 1e-10
I0422 08:57:29.117914  2460 solver.cpp:228] Iteration 4340, loss = 15951.3
I0422 08:57:29.118034  2460 solver.cpp:244]     Train net output #0: loss = 11269.8 (* 1 = 11269.8 loss)
I0422 08:57:29.118052  2460 sgd_solver.cpp:106] Iteration 4340, lr = 1e-10
I0422 08:57:59.098377  2460 solver.cpp:228] Iteration 4360, loss = 15273.4
I0422 08:57:59.098476  2460 solver.cpp:244]     Train net output #0: loss = 17488.5 (* 1 = 17488.5 loss)
I0422 08:57:59.098491  2460 sgd_solver.cpp:106] Iteration 4360, lr = 1e-10
I0422 08:58:29.068742  2460 solver.cpp:228] Iteration 4380, loss = 13737.3
I0422 08:58:29.068850  2460 solver.cpp:244]     Train net output #0: loss = 11967 (* 1 = 11967 loss)
I0422 08:58:29.068866  2460 sgd_solver.cpp:106] Iteration 4380, lr = 1e-10
I0422 08:59:06.758363  2460 solver.cpp:228] Iteration 4400, loss = 15513.2
I0422 08:59:06.758473  2460 solver.cpp:244]     Train net output #0: loss = 15513.2 (* 1 = 15513.2 loss)
I0422 08:59:06.758489  2460 sgd_solver.cpp:106] Iteration 4400, lr = 1e-10
I0422 08:59:36.735461  2460 solver.cpp:228] Iteration 4420, loss = 15059.8
I0422 08:59:36.735569  2460 solver.cpp:244]     Train net output #0: loss = 9194.22 (* 1 = 9194.22 loss)
I0422 08:59:36.735584  2460 sgd_solver.cpp:106] Iteration 4420, lr = 1e-10
I0422 09:00:06.714071  2460 solver.cpp:228] Iteration 4440, loss = 18298.3
I0422 09:00:06.714171  2460 solver.cpp:244]     Train net output #0: loss = 20607.9 (* 1 = 20607.9 loss)
I0422 09:00:06.714187  2460 sgd_solver.cpp:106] Iteration 4440, lr = 1e-10
I0422 09:00:36.690799  2460 solver.cpp:228] Iteration 4460, loss = 17584.9
I0422 09:00:36.690907  2460 solver.cpp:244]     Train net output #0: loss = 16275.7 (* 1 = 16275.7 loss)
I0422 09:00:36.690922  2460 sgd_solver.cpp:106] Iteration 4460, lr = 1e-10
I0422 09:01:06.672519  2460 solver.cpp:228] Iteration 4480, loss = 16297.1
I0422 09:01:06.672631  2460 solver.cpp:244]     Train net output #0: loss = 6069.34 (* 1 = 6069.34 loss)
I0422 09:01:06.672646  2460 sgd_solver.cpp:106] Iteration 4480, lr = 1e-10
I0422 09:01:44.320379  2460 solver.cpp:228] Iteration 4500, loss = 16484.5
I0422 09:01:44.320538  2460 solver.cpp:244]     Train net output #0: loss = 16484.5 (* 1 = 16484.5 loss)
I0422 09:01:44.320562  2460 sgd_solver.cpp:106] Iteration 4500, lr = 1e-10
I0422 09:02:14.303130  2460 solver.cpp:228] Iteration 4520, loss = 18618.6
I0422 09:02:14.303226  2460 solver.cpp:244]     Train net output #0: loss = 16113.6 (* 1 = 16113.6 loss)
I0422 09:02:14.303241  2460 sgd_solver.cpp:106] Iteration 4520, lr = 1e-10
I0422 09:02:44.283376  2460 solver.cpp:228] Iteration 4540, loss = 14335.2
I0422 09:02:44.283476  2460 solver.cpp:244]     Train net output #0: loss = 19248.5 (* 1 = 19248.5 loss)
I0422 09:02:44.283493  2460 sgd_solver.cpp:106] Iteration 4540, lr = 1e-10
I0422 09:03:14.261350  2460 solver.cpp:228] Iteration 4560, loss = 13312.7
I0422 09:03:14.261451  2460 solver.cpp:244]     Train net output #0: loss = 10578.4 (* 1 = 10578.4 loss)
I0422 09:03:14.261466  2460 sgd_solver.cpp:106] Iteration 4560, lr = 1e-10
I0422 09:03:44.240263  2460 solver.cpp:228] Iteration 4580, loss = 17168.3
I0422 09:03:44.240375  2460 solver.cpp:244]     Train net output #0: loss = 15796.3 (* 1 = 15796.3 loss)
I0422 09:03:44.240391  2460 sgd_solver.cpp:106] Iteration 4580, lr = 1e-10
I0422 09:04:21.914773  2460 solver.cpp:228] Iteration 4600, loss = 10292.6
I0422 09:04:21.914891  2460 solver.cpp:244]     Train net output #0: loss = 10292.6 (* 1 = 10292.6 loss)
I0422 09:04:21.914906  2460 sgd_solver.cpp:106] Iteration 4600, lr = 1e-10
I0422 09:04:51.894865  2460 solver.cpp:228] Iteration 4620, loss = 15684.2
I0422 09:04:51.894980  2460 solver.cpp:244]     Train net output #0: loss = 52832.8 (* 1 = 52832.8 loss)
I0422 09:04:51.894995  2460 sgd_solver.cpp:106] Iteration 4620, lr = 1e-10
I0422 09:05:21.874716  2460 solver.cpp:228] Iteration 4640, loss = 14148.8
I0422 09:05:21.874845  2460 solver.cpp:244]     Train net output #0: loss = 16490.7 (* 1 = 16490.7 loss)
I0422 09:05:21.874882  2460 sgd_solver.cpp:106] Iteration 4640, lr = 1e-10
I0422 09:05:51.847293  2460 solver.cpp:228] Iteration 4660, loss = 14952.8
I0422 09:05:51.847385  2460 solver.cpp:244]     Train net output #0: loss = 31937.6 (* 1 = 31937.6 loss)
I0422 09:05:51.847400  2460 sgd_solver.cpp:106] Iteration 4660, lr = 1e-10
I0422 09:06:21.820523  2460 solver.cpp:228] Iteration 4680, loss = 15316.8
I0422 09:06:21.820633  2460 solver.cpp:244]     Train net output #0: loss = 12744.2 (* 1 = 12744.2 loss)
I0422 09:06:21.820649  2460 sgd_solver.cpp:106] Iteration 4680, lr = 1e-10
I0422 09:06:59.488101  2460 solver.cpp:228] Iteration 4700, loss = 20561.6
I0422 09:06:59.488204  2460 solver.cpp:244]     Train net output #0: loss = 20561.6 (* 1 = 20561.6 loss)
I0422 09:06:59.488220  2460 sgd_solver.cpp:106] Iteration 4700, lr = 1e-10
I0422 09:07:29.466795  2460 solver.cpp:228] Iteration 4720, loss = 18446.3
I0422 09:07:29.466902  2460 solver.cpp:244]     Train net output #0: loss = 18749.6 (* 1 = 18749.6 loss)
I0422 09:07:29.466918  2460 sgd_solver.cpp:106] Iteration 4720, lr = 1e-10
I0422 09:07:59.456076  2460 solver.cpp:228] Iteration 4740, loss = 16529.3
I0422 09:07:59.456197  2460 solver.cpp:244]     Train net output #0: loss = 10846.3 (* 1 = 10846.3 loss)
I0422 09:07:59.456220  2460 sgd_solver.cpp:106] Iteration 4740, lr = 1e-10
I0422 09:08:29.436842  2460 solver.cpp:228] Iteration 4760, loss = 17493.2
I0422 09:08:29.436945  2460 solver.cpp:244]     Train net output #0: loss = 31033 (* 1 = 31033 loss)
I0422 09:08:29.436960  2460 sgd_solver.cpp:106] Iteration 4760, lr = 1e-10
I0422 09:08:59.411003  2460 solver.cpp:228] Iteration 4780, loss = 13274.6
I0422 09:08:59.411103  2460 solver.cpp:244]     Train net output #0: loss = 9442.29 (* 1 = 9442.29 loss)
I0422 09:08:59.411118  2460 sgd_solver.cpp:106] Iteration 4780, lr = 1e-10
I0422 09:09:37.069717  2460 solver.cpp:228] Iteration 4800, loss = 10383.6
I0422 09:09:37.069820  2460 solver.cpp:244]     Train net output #0: loss = 10383.6 (* 1 = 10383.6 loss)
I0422 09:09:37.069835  2460 sgd_solver.cpp:106] Iteration 4800, lr = 1e-10
I0422 09:10:07.052316  2460 solver.cpp:228] Iteration 4820, loss = 24548.1
I0422 09:10:07.052434  2460 solver.cpp:244]     Train net output #0: loss = 11188.5 (* 1 = 11188.5 loss)
I0422 09:10:07.052485  2460 sgd_solver.cpp:106] Iteration 4820, lr = 1e-10
I0422 09:10:37.035361  2460 solver.cpp:228] Iteration 4840, loss = 11559.3
I0422 09:10:37.035477  2460 solver.cpp:244]     Train net output #0: loss = 16631.9 (* 1 = 16631.9 loss)
I0422 09:10:37.035493  2460 sgd_solver.cpp:106] Iteration 4840, lr = 1e-10
I0422 09:11:07.009660  2460 solver.cpp:228] Iteration 4860, loss = 18414.7
I0422 09:11:07.009763  2460 solver.cpp:244]     Train net output #0: loss = 35186 (* 1 = 35186 loss)
I0422 09:11:07.009779  2460 sgd_solver.cpp:106] Iteration 4860, lr = 1e-10
I0422 09:11:36.989657  2460 solver.cpp:228] Iteration 4880, loss = 15144.2
I0422 09:11:36.989778  2460 solver.cpp:244]     Train net output #0: loss = 11770.3 (* 1 = 11770.3 loss)
I0422 09:11:36.989794  2460 sgd_solver.cpp:106] Iteration 4880, lr = 1e-10
I0422 09:12:14.667328  2460 solver.cpp:228] Iteration 4900, loss = 7110.95
I0422 09:12:14.667443  2460 solver.cpp:244]     Train net output #0: loss = 7110.95 (* 1 = 7110.95 loss)
I0422 09:12:14.667459  2460 sgd_solver.cpp:106] Iteration 4900, lr = 1e-10
I0422 09:12:44.645869  2460 solver.cpp:228] Iteration 4920, loss = 19811.8
I0422 09:12:44.645983  2460 solver.cpp:244]     Train net output #0: loss = 14786.5 (* 1 = 14786.5 loss)
I0422 09:12:44.646006  2460 sgd_solver.cpp:106] Iteration 4920, lr = 1e-10
I0422 09:13:14.608121  2460 solver.cpp:228] Iteration 4940, loss = 15212.1
I0422 09:13:14.608230  2460 solver.cpp:244]     Train net output #0: loss = 18679.4 (* 1 = 18679.4 loss)
I0422 09:13:14.608247  2460 sgd_solver.cpp:106] Iteration 4940, lr = 1e-10
I0422 09:13:44.581683  2460 solver.cpp:228] Iteration 4960, loss = 15588.6
I0422 09:13:44.581796  2460 solver.cpp:244]     Train net output #0: loss = 85583.5 (* 1 = 85583.5 loss)
I0422 09:13:44.581823  2460 sgd_solver.cpp:106] Iteration 4960, lr = 1e-10
I0422 09:14:14.549810  2460 solver.cpp:228] Iteration 4980, loss = 14167.2
I0422 09:14:14.549916  2460 solver.cpp:244]     Train net output #0: loss = 8174.33 (* 1 = 8174.33 loss)
I0422 09:14:14.549932  2460 sgd_solver.cpp:106] Iteration 4980, lr = 1e-10
I0422 09:14:43.026470  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_5000.caffemodel
I0422 09:14:46.307018  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_5000.solverstate
I0422 09:14:55.445935  2460 solver.cpp:228] Iteration 5000, loss = 8162.2
I0422 09:14:55.446061  2460 solver.cpp:244]     Train net output #0: loss = 8162.2 (* 1 = 8162.2 loss)
I0422 09:14:55.446079  2460 sgd_solver.cpp:106] Iteration 5000, lr = 1e-10
I0422 09:15:25.423478  2460 solver.cpp:228] Iteration 5020, loss = 18615.7
I0422 09:15:25.423588  2460 solver.cpp:244]     Train net output #0: loss = 20555.4 (* 1 = 20555.4 loss)
I0422 09:15:25.423604  2460 sgd_solver.cpp:106] Iteration 5020, lr = 1e-10
I0422 09:15:55.409175  2460 solver.cpp:228] Iteration 5040, loss = 13822.8
I0422 09:15:55.409281  2460 solver.cpp:244]     Train net output #0: loss = 10868.2 (* 1 = 10868.2 loss)
I0422 09:15:55.409297  2460 sgd_solver.cpp:106] Iteration 5040, lr = 1e-10
I0422 09:16:25.384443  2460 solver.cpp:228] Iteration 5060, loss = 17658.5
I0422 09:16:25.384546  2460 solver.cpp:244]     Train net output #0: loss = 14012.6 (* 1 = 14012.6 loss)
I0422 09:16:25.384562  2460 sgd_solver.cpp:106] Iteration 5060, lr = 1e-10
I0422 09:16:55.362818  2460 solver.cpp:228] Iteration 5080, loss = 13701.2
I0422 09:16:55.362939  2460 solver.cpp:244]     Train net output #0: loss = 11165.4 (* 1 = 11165.4 loss)
I0422 09:16:55.362956  2460 sgd_solver.cpp:106] Iteration 5080, lr = 1e-10
I0422 09:17:33.022456  2460 solver.cpp:228] Iteration 5100, loss = 7940.77
I0422 09:17:33.022567  2460 solver.cpp:244]     Train net output #0: loss = 7940.77 (* 1 = 7940.77 loss)
I0422 09:17:33.022581  2460 sgd_solver.cpp:106] Iteration 5100, lr = 1e-10
I0422 09:18:03.003991  2460 solver.cpp:228] Iteration 5120, loss = 17068.6
I0422 09:18:03.004124  2460 solver.cpp:244]     Train net output #0: loss = 37042.3 (* 1 = 37042.3 loss)
I0422 09:18:03.004142  2460 sgd_solver.cpp:106] Iteration 5120, lr = 1e-10
I0422 09:18:32.992440  2460 solver.cpp:228] Iteration 5140, loss = 15256.2
I0422 09:18:32.992552  2460 solver.cpp:244]     Train net output #0: loss = 31349.2 (* 1 = 31349.2 loss)
I0422 09:18:32.992576  2460 sgd_solver.cpp:106] Iteration 5140, lr = 1e-10
I0422 09:19:02.980073  2460 solver.cpp:228] Iteration 5160, loss = 14325.4
I0422 09:19:02.980197  2460 solver.cpp:244]     Train net output #0: loss = 15074.2 (* 1 = 15074.2 loss)
I0422 09:19:02.980212  2460 sgd_solver.cpp:106] Iteration 5160, lr = 1e-10
I0422 09:19:32.964987  2460 solver.cpp:228] Iteration 5180, loss = 14430.4
I0422 09:19:32.965104  2460 solver.cpp:244]     Train net output #0: loss = 30733.9 (* 1 = 30733.9 loss)
I0422 09:19:32.965121  2460 sgd_solver.cpp:106] Iteration 5180, lr = 1e-10
n 4100 fwavacc 0.941351218768
>>> 2017-04-22 08:53:42.397592 Begin seg tests
>>> 2017-04-22 08:53:49.484046 Iteration 4200 loss 17473.7364095
>>> 2017-04-22 08:53:49.484136 Iteration 4200 overall accuracy 0.960104
>>> 2017-04-22 08:53:49.484191 Iteration 4200 mean accuracy 0.957943043407
>>> 2017-04-22 08:53:49.484384 Iteration 4200 mean IU 0.922123279934
>>> 2017-04-22 08:53:49.484488 Iteration 4200 fwavacc 0.92317606204
>>> 2017-04-22 08:56:19.990376 Begin seg tests
>>> 2017-04-22 08:56:27.076347 Iteration 4300 loss 17185.9832357
>>> 2017-04-22 08:56:27.076457 Iteration 4300 overall accuracy 0.965014222222
>>> 2017-04-22 08:56:27.076520 Iteration 4300 mean accuracy 0.964302238306
>>> 2017-04-22 08:56:27.076723 Iteration 4300 mean IU 0.932113403757
>>> 2017-04-22 08:56:27.076827 Iteration 4300 fwavacc 0.932356440962
>>> 2017-04-22 08:58:57.566250 Begin seg tests
>>> 2017-04-22 08:59:04.652981 Iteration 4400 loss 12176.7239583
>>> 2017-04-22 08:59:04.653093 Iteration 4400 overall accuracy 0.973700888889
>>> 2017-04-22 08:59:04.653153 Iteration 4400 mean accuracy 0.973582904272
>>> 2017-04-22 08:59:04.653361 Iteration 4400 mean IU 0.948520270831
>>> 2017-04-22 08:59:04.653481 Iteration 4400 fwavacc 0.948755965015
>>> 2017-04-22 09:01:35.151802 Begin seg tests
>>> 2017-04-22 09:01:42.228319 Iteration 4500 loss 10889.8800863
>>> 2017-04-22 09:01:42.228416 Iteration 4500 overall accuracy 0.977928444444
>>> 2017-04-22 09:01:42.228474 Iteration 4500 mean accuracy 0.978470394862
>>> 2017-04-22 09:01:42.228684 Iteration 4500 mean IU 0.955392847124
>>> 2017-04-22 09:01:42.228789 Iteration 4500 fwavacc 0.956896598497
>>> 2017-04-22 09:04:12.726119 Begin seg tests
>>> 2017-04-22 09:04:19.819691 Iteration 4600 loss 20570.5274251
>>> 2017-04-22 09:04:19.819802 Iteration 4600 overall accuracy 0.955991555556
>>> 2017-04-22 09:04:19.819860 Iteration 4600 mean accuracy 0.956615702837
>>> 2017-04-22 09:04:19.820064 Iteration 4600 mean IU 0.913885466387
>>> 2017-04-22 09:04:19.820167 Iteration 4600 fwavacc 0.915908680366
>>> 2017-04-22 09:06:50.306432 Begin seg tests
>>> 2017-04-22 09:06:57.394453 Iteration 4700 loss 12005.9154053
>>> 2017-04-22 09:06:57.394561 Iteration 4700 overall accuracy 0.973988
>>> 2017-04-22 09:06:57.394621 Iteration 4700 mean accuracy 0.973837043916
>>> 2017-04-22 09:06:57.394834 Iteration 4700 mean IU 0.949240620134
>>> 2017-04-22 09:06:57.394945 Iteration 4700 fwavacc 0.949289019505
>>> 2017-04-22 09:09:27.883219 Begin seg tests
>>> 2017-04-22 09:09:34.966744 Iteration 4800 loss 13084.6936035
>>> 2017-04-22 09:09:34.966842 Iteration 4800 overall accuracy 0.971319111111
>>> 2017-04-22 09:09:34.966911 Iteration 4800 mean accuracy 0.972837266452
>>> 2017-04-22 09:09:34.967136 Iteration 4800 mean IU 0.943558153243
>>> 2017-04-22 09:09:34.967244 Iteration 4800 fwavacc 0.944351925125
>>> 2017-04-22 09:12:05.484503 Begin seg tests
>>> 2017-04-22 09:12:12.568584 Iteration 4900 loss 13429.4706217
>>> 2017-04-22 09:12:12.568693 Iteration 4900 overall accuracy 0.970456444444
>>> 2017-04-22 09:12:12.568755 Iteration 4900 mean accuracy 0.969967368985
>>> 2017-04-22 09:12:12.568985 Iteration 4900 mean IU 0.942494471612
>>> 2017-04-22 09:12:12.569111 Iteration 4900 fwavacc 0.942573764505
>>> 2017-04-22 09:14:47.285203 Begin seg tests
>>> 2017-04-22 09:14:53.374191 Iteration 5000 loss 11006.2381185
>>> 2017-04-22 09:14:53.374315 Iteration 5000 overall accuracy 0.975668444444
>>> 2017-04-22 09:14:53.374390 Iteration 5000 mean accuracy 0.975663848534
>>> 2017-04-22 09:14:53.374691 Iteration 5000 mean IU 0.952470274472
>>> 2017-04-22 09:14:53.374819 Iteration 5000 fwavacc 0.952493721688
>>> 2017-04-22 09:17:23.836996 Begin seg tests
>>> 2017-04-22 09:17:30.927281 Iteration 5100 loss 13163.8086751
>>> 2017-04-22 09:17:30.927392 Iteration 5100 overall accuracy 0.972408
>>> 2017-04-22 09:17:30.927461 Iteration 5100 mean accuracy 0.972063348416
>>> 2017-04-22 09:17:30.927698 Iteration 5100 mean IU 0.946177378494
>>> 2017-04-22 09:17:30.927805 Iteration 5100 fwavacc 0.94628284902
>>> 2017-04-22 09:20:01.444475 Begin seg tests
>>> 2017-I0422 09:20:10.643908  2460 solver.cpp:228] Iteration 5200, loss = 8633.03
I0422 09:20:10.643999  2460 solver.cpp:244]     Train net output #0: loss = 8633.03 (* 1 = 8633.03 loss)
I0422 09:20:10.644016  2460 sgd_solver.cpp:106] Iteration 5200, lr = 1e-10
I0422 09:20:40.613334  2460 solver.cpp:228] Iteration 5220, loss = 14675.9
I0422 09:20:40.613457  2460 solver.cpp:244]     Train net output #0: loss = 8143 (* 1 = 8143 loss)
I0422 09:20:40.613472  2460 sgd_solver.cpp:106] Iteration 5220, lr = 1e-10
I0422 09:21:10.578794  2460 solver.cpp:228] Iteration 5240, loss = 12497.8
I0422 09:21:10.578892  2460 solver.cpp:244]     Train net output #0: loss = 8376.33 (* 1 = 8376.33 loss)
I0422 09:21:10.578908  2460 sgd_solver.cpp:106] Iteration 5240, lr = 1e-10
I0422 09:21:40.556654  2460 solver.cpp:228] Iteration 5260, loss = 14694.6
I0422 09:21:40.556766  2460 solver.cpp:244]     Train net output #0: loss = 17440.8 (* 1 = 17440.8 loss)
I0422 09:21:40.556780  2460 sgd_solver.cpp:106] Iteration 5260, lr = 1e-10
I0422 09:22:10.541971  2460 solver.cpp:228] Iteration 5280, loss = 14765.8
I0422 09:22:10.542093  2460 solver.cpp:244]     Train net output #0: loss = 10762.5 (* 1 = 10762.5 loss)
I0422 09:22:10.542107  2460 sgd_solver.cpp:106] Iteration 5280, lr = 1e-10
I0422 09:22:48.190583  2460 solver.cpp:228] Iteration 5300, loss = 10828
I0422 09:22:48.190690  2460 solver.cpp:244]     Train net output #0: loss = 10828 (* 1 = 10828 loss)
I0422 09:22:48.190707  2460 sgd_solver.cpp:106] Iteration 5300, lr = 1e-10
I0422 09:23:18.166477  2460 solver.cpp:228] Iteration 5320, loss = 14019.3
I0422 09:23:18.166585  2460 solver.cpp:244]     Train net output #0: loss = 12574.8 (* 1 = 12574.8 loss)
I0422 09:23:18.166604  2460 sgd_solver.cpp:106] Iteration 5320, lr = 1e-10
I0422 09:23:48.152530  2460 solver.cpp:228] Iteration 5340, loss = 19316.9
I0422 09:23:48.152638  2460 solver.cpp:244]     Train net output #0: loss = 39936.1 (* 1 = 39936.1 loss)
I0422 09:23:48.152662  2460 sgd_solver.cpp:106] Iteration 5340, lr = 1e-10
I0422 09:24:18.119139  2460 solver.cpp:228] Iteration 5360, loss = 15095.3
I0422 09:24:18.119238  2460 solver.cpp:244]     Train net output #0: loss = 28052.2 (* 1 = 28052.2 loss)
I0422 09:24:18.119253  2460 sgd_solver.cpp:106] Iteration 5360, lr = 1e-10
I0422 09:24:48.105293  2460 solver.cpp:228] Iteration 5380, loss = 14089.2
I0422 09:24:48.105402  2460 solver.cpp:244]     Train net output #0: loss = 10166.7 (* 1 = 10166.7 loss)
I0422 09:24:48.105423  2460 sgd_solver.cpp:106] Iteration 5380, lr = 1e-10
I0422 09:25:25.775673  2460 solver.cpp:228] Iteration 5400, loss = 13158
I0422 09:25:25.775786  2460 solver.cpp:244]     Train net output #0: loss = 13158 (* 1 = 13158 loss)
I0422 09:25:25.775800  2460 sgd_solver.cpp:106] Iteration 5400, lr = 1e-10
I0422 09:25:55.752869  2460 solver.cpp:228] Iteration 5420, loss = 16638.9
I0422 09:25:55.752972  2460 solver.cpp:244]     Train net output #0: loss = 43910.3 (* 1 = 43910.3 loss)
I0422 09:25:55.752988  2460 sgd_solver.cpp:106] Iteration 5420, lr = 1e-10
I0422 09:26:25.727548  2460 solver.cpp:228] Iteration 5440, loss = 12647.7
I0422 09:26:25.727699  2460 solver.cpp:244]     Train net output #0: loss = 12112.2 (* 1 = 12112.2 loss)
I0422 09:26:25.727766  2460 sgd_solver.cpp:106] Iteration 5440, lr = 1e-10
I0422 09:26:55.712246  2460 solver.cpp:228] Iteration 5460, loss = 13280.9
I0422 09:26:55.712360  2460 solver.cpp:244]     Train net output #0: loss = 12687.6 (* 1 = 12687.6 loss)
I0422 09:26:55.712375  2460 sgd_solver.cpp:106] Iteration 5460, lr = 1e-10
I0422 09:27:25.692980  2460 solver.cpp:228] Iteration 5480, loss = 12183.7
I0422 09:27:25.693090  2460 solver.cpp:244]     Train net output #0: loss = 10061.4 (* 1 = 10061.4 loss)
I0422 09:27:25.693106  2460 sgd_solver.cpp:106] Iteration 5480, lr = 1e-10
I0422 09:28:03.336760  2460 solver.cpp:228] Iteration 5500, loss = 8898.46
I0422 09:28:03.336866  2460 solver.cpp:244]     Train net output #0: loss = 8898.46 (* 1 = 8898.46 loss)
I0422 09:28:03.336882  2460 sgd_solver.cpp:106] Iteration 5500, lr = 1e-10
I0422 09:28:33.306135  2460 solver.cpp:228] Iteration 5520, loss = 14197
I0422 09:28:33.306236  2460 solver.cpp:244]     Train net output #0: loss = 10862.3 (* 1 = 10862.3 loss)
I0422 09:28:33.306251  2460 sgd_solver.cpp:106] Iteration 5520, lr = 1e-10
I0422 09:29:03.274372  2460 solver.cpp:228] Iteration 5540, loss = 12532.7
I0422 09:29:03.274482  2460 solver.cpp:244]     Train net output #0: loss = 10235.2 (* 1 = 10235.2 loss)
I0422 09:29:03.274498  2460 sgd_solver.cpp:106] Iteration 5540, lr = 1e-10
I0422 09:29:33.251488  2460 solver.cpp:228] Iteration 5560, loss = 14938.9
I0422 09:29:33.251612  2460 solver.cpp:244]     Train net output #0: loss = 14082 (* 1 = 14082 loss)
I0422 09:29:33.251646  2460 sgd_solver.cpp:106] Iteration 5560, lr = 1e-10
I0422 09:30:03.229740  2460 solver.cpp:228] Iteration 5580, loss = 12915.9
I0422 09:30:03.229848  2460 solver.cpp:244]     Train net output #0: loss = 7091.98 (* 1 = 7091.98 loss)
I0422 09:30:03.229864  2460 sgd_solver.cpp:106] Iteration 5580, lr = 1e-10
I0422 09:30:40.869485  2460 solver.cpp:228] Iteration 5600, loss = 8862.25
I0422 09:30:40.869595  2460 solver.cpp:244]     Train net output #0: loss = 8862.25 (* 1 = 8862.25 loss)
I0422 09:30:40.869617  2460 sgd_solver.cpp:106] Iteration 5600, lr = 1e-10
I0422 09:31:10.853133  2460 solver.cpp:228] Iteration 5620, loss = 14313.5
I0422 09:31:10.853247  2460 solver.cpp:244]     Train net output #0: loss = 22776.7 (* 1 = 22776.7 loss)
I0422 09:31:10.853261  2460 sgd_solver.cpp:106] Iteration 5620, lr = 1e-10
I0422 09:31:40.834846  2460 solver.cpp:228] Iteration 5640, loss = 11792.7
I0422 09:31:40.834959  2460 solver.cpp:244]     Train net output #0: loss = 10358.4 (* 1 = 10358.4 loss)
I0422 09:31:40.834975  2460 sgd_solver.cpp:106] Iteration 5640, lr = 1e-10
I0422 09:32:10.812999  2460 solver.cpp:228] Iteration 5660, loss = 14582.8
I0422 09:32:10.813097  2460 solver.cpp:244]     Train net output #0: loss = 10436.3 (* 1 = 10436.3 loss)
I0422 09:32:10.813112  2460 sgd_solver.cpp:106] Iteration 5660, lr = 1e-10
I0422 09:32:40.787259  2460 solver.cpp:228] Iteration 5680, loss = 11533.3
I0422 09:32:40.787359  2460 solver.cpp:244]     Train net output #0: loss = 7774.12 (* 1 = 7774.12 loss)
I0422 09:32:40.787376  2460 sgd_solver.cpp:106] Iteration 5680, lr = 1e-10
I0422 09:33:18.470892  2460 solver.cpp:228] Iteration 5700, loss = 6976.02
I0422 09:33:18.470998  2460 solver.cpp:244]     Train net output #0: loss = 6976.02 (* 1 = 6976.02 loss)
I0422 09:33:18.471012  2460 sgd_solver.cpp:106] Iteration 5700, lr = 1e-10
I0422 09:33:48.450831  2460 solver.cpp:228] Iteration 5720, loss = 13404
I0422 09:33:48.450940  2460 solver.cpp:244]     Train net output #0: loss = 14261.8 (* 1 = 14261.8 loss)
I0422 09:33:48.450956  2460 sgd_solver.cpp:106] Iteration 5720, lr = 1e-10
I0422 09:34:18.424439  2460 solver.cpp:228] Iteration 5740, loss = 12981.5
I0422 09:34:18.424551  2460 solver.cpp:244]     Train net output #0: loss = 6569.45 (* 1 = 6569.45 loss)
I0422 09:34:18.424567  2460 sgd_solver.cpp:106] Iteration 5740, lr = 1e-10
I0422 09:34:48.399821  2460 solver.cpp:228] Iteration 5760, loss = 13765.4
I0422 09:34:48.399938  2460 solver.cpp:244]     Train net output #0: loss = 9497.09 (* 1 = 9497.09 loss)
I0422 09:34:48.399982  2460 sgd_solver.cpp:106] Iteration 5760, lr = 1e-10
I0422 09:35:18.376206  2460 solver.cpp:228] Iteration 5780, loss = 11525.4
I0422 09:35:18.376318  2460 solver.cpp:244]     Train net output #0: loss = 11932.2 (* 1 = 11932.2 loss)
I0422 09:35:18.376332  2460 sgd_solver.cpp:106] Iteration 5780, lr = 1e-10
I0422 09:35:56.087505  2460 solver.cpp:228] Iteration 5800, loss = 8785.58
I0422 09:35:56.087607  2460 solver.cpp:244]     Train net output #0: loss = 8785.58 (* 1 = 8785.58 loss)
I0422 09:35:56.087642  2460 sgd_solver.cpp:106] Iteration 5800, lr = 1e-10
I0422 09:36:26.073405  2460 solver.cpp:228] Iteration 5820, loss = 17112.4
I0422 09:36:26.073506  2460 solver.cpp:244]     Train net output #0: loss = 6884.82 (* 1 = 6884.82 loss)
I0422 09:36:26.073521  2460 sgd_solver.cpp:106] Iteration 5820, lr = 1e-10
I0422 09:36:56.055420  2460 solver.cpp:228] Iteration 5840, loss = 12439.5
I0422 09:36:56.055532  2460 solver.cpp:244]     Train net output #0: loss = 9215.48 (* 1 = 9215.48 loss)
I0422 09:36:56.055547  2460 sgd_solver.cpp:106] Iteration 5840, lr = 1e-10
I0422 09:37:26.026125  2460 solver.cpp:228] Iteration 5860, loss = 13744.9
I0422 09:37:26.026218  2460 solver.cpp:244]     Train net output #0: loss = 13283.7 (* 1 = 13283.7 loss)
I0422 09:37:26.026233  2460 sgd_solver.cpp:106] Iteration 5860, lr = 1e-10
I0422 09:37:56.004135  2460 solver.cpp:228] Iteration 5880, loss = 11269.8
I0422 09:37:56.004238  2460 solver.cpp:244]     Train net output #0: loss = 15229.9 (* 1 = 15229.9 loss)
I0422 09:37:56.004253  2460 sgd_solver.cpp:106] Iteration 5880, lr = 1e-10
I0422 09:38:33.690846  2460 solver.cpp:228] Iteration 5900, loss = 12248.3
I0422 09:38:33.690963  2460 solver.cpp:244]     Train net output #0: loss = 12248.3 (* 1 = 12248.3 loss)
I0422 09:38:33.690979  2460 sgd_solver.cpp:106] Iteration 5900, lr = 1e-10
I0422 09:39:03.671339  2460 solver.cpp:228] Iteration 5920, loss = 11045.6
I0422 09:39:03.671442  2460 solver.cpp:244]     Train net output #0: loss = 13234.4 (* 1 = 13234.4 loss)
I0422 09:39:03.671456  2460 sgd_solver.cpp:106] Iteration 5920, lr = 1e-10
I0422 09:39:33.649322  2460 solver.cpp:228] Iteration 5940, loss = 15753.4
I0422 09:39:33.649432  2460 solver.cpp:244]     Train net output #0: loss = 16476 (* 1 = 16476 loss)
I0422 09:39:33.649453  2460 sgd_solver.cpp:106] Iteration 5940, lr = 1e-10
I0422 09:40:03.617640  2460 solver.cpp:228] Iteration 5960, loss = 11273.8
I0422 09:40:03.617751  2460 solver.cpp:244]     Train net output #0: loss = 10907.2 (* 1 = 10907.2 loss)
I0422 09:40:03.617768  2460 sgd_solver.cpp:106] Iteration 5960, lr = 1e-10
I0422 09:40:33.606166  2460 solver.cpp:228] Iteration 5980, loss = 13639.1
I0422 09:40:33.606259  2460 solver.cpp:244]     Train net output #0: loss = 5693.34 (* 1 = 5693.34 loss)
I0422 09:40:33.606276  2460 sgd_solver.cpp:106] Iteration 5980, lr = 1e-10
I0422 09:41:02.081627  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_6000.caffemodel
I0422 09:41:05.327577  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_6000.solverstate
I0422 09:41:14.455446  2460 solver.cpp:228] Iteration 6000, loss = 5734.36
I0422 09:41:14.455557  2460 solver.cpp:244]     Train net output #0: loss = 5734.36 (* 1 = 5734.36 loss)
I0422 09:41:14.455572  2460 sgd_solver.cpp:106] Iteration 6000, lr = 1e-10
I0422 09:41:44.436219  2460 solver.cpp:228] Iteration 6020, loss = 14552.9
I0422 09:41:44.436327  2460 solver.cpp:244]     Train net output #0: loss = 11379.6 (* 1 = 11379.6 loss)
I0422 09:41:44.436343  2460 sgd_solver.cpp:106] Iteration 6020, lr = 1e-10
I0422 09:42:14.410874  2460 solver.cpp:228] Iteration 6040, loss = 12840.8
I0422 09:42:14.410993  2460 solver.cpp:244]     Train net output #0: loss = 14738.5 (* 1 = 14738.5 loss)
I0422 09:42:14.411012  2460 sgd_solver.cpp:106] Iteration 6040, lr = 1e-10
I0422 09:42:44.382423  2460 solver.cpp:228] Iteration 6060, loss = 12973.1
I0422 09:42:44.382517  2460 solver.cpp:244]     Train net output #0: loss = 9732.34 (* 1 = 9732.34 loss)
I0422 09:42:44.382552  2460 sgd_solver.cpp:106] Iteration 6060, lr = 1e-10
I0422 09:43:14.359494  2460 solver.cpp:228] Iteration 6080, loss = 12131.8
I0422 09:43:14.359593  2460 solver.cpp:244]     Train net output #0: loss = 14607.7 (* 1 = 14607.7 loss)
I0422 09:43:14.359609  2460 sgd_solver.cpp:106] Iteration 6080, lr = 1e-10
I0422 09:43:52.047608  2460 solver.cpp:228] Iteration 6100, loss = 6341.66
I0422 09:43:52.047737  2460 solver.cpp:244]     Train net output #0: loss = 6341.66 (* 1 = 6341.66 loss)
I0422 09:43:52.047754  2460 sgd_solver.cpp:106] Iteration 6100, lr = 1e-10
I0422 09:44:22.016460  2460 solver.cpp:228] Iteration 6120, loss = 11825.6
I0422 09:44:22.016569  2460 solver.cpp:244]     Train net output #0: loss = 12042.1 (* 1 = 12042.1 loss)
I0422 09:44:22.016584  2460 sgd_solver.cpp:106] Iteration 6120, lr = 1e-10
I0422 09:44:51.990722  2460 solver.cpp:228] Iteration 6140, loss = 12643.7
I0422 09:44:51.990844  2460 solver.cpp:244]     Train net output #0: loss = 17676.7 (* 1 = 17676.7 loss)
I0422 09:44:51.990859  2460 sgd_solver.cpp:106] Iteration 6140, lr = 1e-10
I0422 09:45:21.966660  2460 solver.cpp:228] Iteration 6160, loss = 13488.3
I0422 09:45:21.966778  2460 solver.cpp:244]     Train net output #0: loss = 16049.6 (* 1 = 16049.6 loss)
I0422 09:45:21.966794  2460 sgd_solver.cpp:106] Iteration 6160, lr = 1e-10
I0422 09:45:51.944463  2460 solver.cpp:228] Iteration 6180, loss = 14150.9
I0422 09:45:51.944586  2460 solver.cpp:244]     Train net output #0: loss = 6993.5 (* 1 = 6993.5 loss)
I0422 09:45:51.944602  2460 sgd_solver.cpp:106] Iteration 6180, lr = 1e-10
04-22 09:20:08.535960 Iteration 5200 loss 13027.8318278
>>> 2017-04-22 09:20:08.536113 Iteration 5200 overall accuracy 0.972818222222
>>> 2017-04-22 09:20:08.536171 Iteration 5200 mean accuracy 0.973799345123
>>> 2017-04-22 09:20:08.536365 Iteration 5200 mean IU 0.946453270127
>>> 2017-04-22 09:20:08.536469 Iteration 5200 fwavacc 0.947156803018
>>> 2017-04-22 09:22:39.024527 Begin seg tests
>>> 2017-04-22 09:22:46.110253 Iteration 5300 loss 12824.9897054
>>> 2017-04-22 09:22:46.110357 Iteration 5300 overall accuracy 0.972838666667
>>> 2017-04-22 09:22:46.110415 Iteration 5300 mean accuracy 0.971812504909
>>> 2017-04-22 09:22:46.110618 Iteration 5300 mean IU 0.946503729822
>>> 2017-04-22 09:22:46.110722 Iteration 5300 fwavacc 0.947089731367
>>> 2017-04-22 09:25:16.583136 Begin seg tests
>>> 2017-04-22 09:25:23.673247 Iteration 5400 loss 12849.8522135
>>> 2017-04-22 09:25:23.673346 Iteration 5400 overall accuracy 0.972752
>>> 2017-04-22 09:25:23.673413 Iteration 5400 mean accuracy 0.971361005633
>>> 2017-04-22 09:25:23.673630 Iteration 5400 mean IU 0.946493992775
>>> 2017-04-22 09:25:23.673735 Iteration 5400 fwavacc 0.946886894525
>>> 2017-04-22 09:27:54.175067 Begin seg tests
>>> 2017-04-22 09:28:01.262051 Iteration 5500 loss 10564.1619059
>>> 2017-04-22 09:28:01.262154 Iteration 5500 overall accuracy 0.978374666667
>>> 2017-04-22 09:28:01.262217 Iteration 5500 mean accuracy 0.977638664177
>>> 2017-04-22 09:28:01.262429 Iteration 5500 mean IU 0.956926170928
>>> 2017-04-22 09:28:01.262534 Iteration 5500 fwavacc 0.957665689627
>>> 2017-04-22 09:30:31.703309 Begin seg tests
>>> 2017-04-22 09:30:38.783870 Iteration 5600 loss 11816.1853434
>>> 2017-04-22 09:30:38.783973 Iteration 5600 overall accuracy 0.974393777778
>>> 2017-04-22 09:30:38.784031 Iteration 5600 mean accuracy 0.972897866912
>>> 2017-04-22 09:30:38.784234 Iteration 5600 mean IU 0.948820684775
>>> 2017-04-22 09:30:38.784344 Iteration 5600 fwavacc 0.950054191872
>>> 2017-04-22 09:33:09.269544 Begin seg tests
>>> 2017-04-22 09:33:16.360495 Iteration 5700 loss 13507.2283529
>>> 2017-04-22 09:33:16.360587 Iteration 5700 overall accuracy 0.971062222222
>>> 2017-04-22 09:33:16.360643 Iteration 5700 mean accuracy 0.969086255316
>>> 2017-04-22 09:33:16.360851 Iteration 5700 mean IU 0.938086726107
>>> 2017-04-22 09:33:16.360957 Iteration 5700 fwavacc 0.943980686828
>>> 2017-04-22 09:35:46.863389 Begin seg tests
>>> 2017-04-22 09:35:53.951313 Iteration 5800 loss 13058.2592367
>>> 2017-04-22 09:35:53.951403 Iteration 5800 overall accuracy 0.971094222222
>>> 2017-04-22 09:35:53.951459 Iteration 5800 mean accuracy 0.971826374165
>>> 2017-04-22 09:35:53.951679 Iteration 5800 mean IU 0.942749774638
>>> 2017-04-22 09:35:53.951784 Iteration 5800 fwavacc 0.943914370063
>>> 2017-04-22 09:38:24.479240 Begin seg tests
>>> 2017-04-22 09:38:31.556180 Iteration 5900 loss 11874.7754313
>>> 2017-04-22 09:38:31.556300 Iteration 5900 overall accuracy 0.974397777778
>>> 2017-04-22 09:38:31.556359 Iteration 5900 mean accuracy 0.972993900059
>>> 2017-04-22 09:38:31.556564 Iteration 5900 mean IU 0.948693219577
>>> 2017-04-22 09:38:31.556672 Iteration 5900 fwavacc 0.950074429807
>>> 2017-04-22 09:41:06.305869 Begin seg tests
>>> 2017-04-22 09:41:12.400430 Iteration 6000 loss 10926.5583089
>>> 2017-04-22 09:41:12.400522 Iteration 6000 overall accuracy 0.977118222222
>>> 2017-04-22 09:41:12.400580 Iteration 6000 mean accuracy 0.977188632301
>>> 2017-04-22 09:41:12.400775 Iteration 6000 mean IU 0.954867821727
>>> 2017-04-22 09:41:12.400879 Iteration 6000 fwavacc 0.955281600972
>>> 2017-04-22 09:43:42.841962 Begin seg tests
>>> 2017-04-22 09:43:49.925959 Iteration 6100 loss 13944.7298991
>>> 2017-04-22 09:43:49.926069 Iteration 6100 overall accuracy 0.969571555556
>>> 2017-04-22 09:43:49.926131 Iteration 6100 mean accuracy 0.966976380996
>>> 2017-04-22 09:43:49.926351 Iteration 6100 mean IU 0.937583205587
>>> 2017-04-22 09:43:49.926456 Iteration 6100 fwavacc 0.940995701676
>>> 2017-04-22 09:46:20.428360 Begin seg tests
>>> 2017-04-22 09:46:27.517705 Iteration 6200 loss 11701.4907227
>>> 2017-04-22 0I0422 09:46:29.626675  2460 solver.cpp:228] Iteration 6200, loss = 18812.9
I0422 09:46:29.626778  2460 solver.cpp:244]     Train net output #0: loss = 18812.9 (* 1 = 18812.9 loss)
I0422 09:46:29.626794  2460 sgd_solver.cpp:106] Iteration 6200, lr = 1e-10
I0422 09:46:59.594261  2460 solver.cpp:228] Iteration 6220, loss = 14036.7
I0422 09:46:59.594393  2460 solver.cpp:244]     Train net output #0: loss = 9293.65 (* 1 = 9293.65 loss)
I0422 09:46:59.594408  2460 sgd_solver.cpp:106] Iteration 6220, lr = 1e-10
I0422 09:47:29.568125  2460 solver.cpp:228] Iteration 6240, loss = 17278.5
I0422 09:47:29.568241  2460 solver.cpp:244]     Train net output #0: loss = 9690.4 (* 1 = 9690.4 loss)
I0422 09:47:29.568265  2460 sgd_solver.cpp:106] Iteration 6240, lr = 1e-10
I0422 09:47:59.542979  2460 solver.cpp:228] Iteration 6260, loss = 15477.9
I0422 09:47:59.543085  2460 solver.cpp:244]     Train net output #0: loss = 11830.8 (* 1 = 11830.8 loss)
I0422 09:47:59.543102  2460 sgd_solver.cpp:106] Iteration 6260, lr = 1e-10
I0422 09:48:29.521939  2460 solver.cpp:228] Iteration 6280, loss = 16113.3
I0422 09:48:29.522070  2460 solver.cpp:244]     Train net output #0: loss = 28781.9 (* 1 = 28781.9 loss)
I0422 09:48:29.522085  2460 sgd_solver.cpp:106] Iteration 6280, lr = 1e-10
I0422 09:49:07.204511  2460 solver.cpp:228] Iteration 6300, loss = 12053.8
I0422 09:49:07.204614  2460 solver.cpp:244]     Train net output #0: loss = 12053.8 (* 1 = 12053.8 loss)
I0422 09:49:07.204630  2460 sgd_solver.cpp:106] Iteration 6300, lr = 1e-10
I0422 09:49:37.180423  2460 solver.cpp:228] Iteration 6320, loss = 12282.4
I0422 09:49:37.180516  2460 solver.cpp:244]     Train net output #0: loss = 12342.8 (* 1 = 12342.8 loss)
I0422 09:49:37.180533  2460 sgd_solver.cpp:106] Iteration 6320, lr = 1e-10
I0422 09:50:07.161516  2460 solver.cpp:228] Iteration 6340, loss = 14616.9
I0422 09:50:07.161628  2460 solver.cpp:244]     Train net output #0: loss = 7935.97 (* 1 = 7935.97 loss)
I0422 09:50:07.161643  2460 sgd_solver.cpp:106] Iteration 6340, lr = 1e-10
I0422 09:50:37.146201  2460 solver.cpp:228] Iteration 6360, loss = 13288.1
I0422 09:50:37.146324  2460 solver.cpp:244]     Train net output #0: loss = 25646.2 (* 1 = 25646.2 loss)
I0422 09:50:37.146345  2460 sgd_solver.cpp:106] Iteration 6360, lr = 1e-10
I0422 09:51:07.128993  2460 solver.cpp:228] Iteration 6380, loss = 14842.3
I0422 09:51:07.129094  2460 solver.cpp:244]     Train net output #0: loss = 44565.5 (* 1 = 44565.5 loss)
I0422 09:51:07.129137  2460 sgd_solver.cpp:106] Iteration 6380, lr = 1e-10
I0422 09:51:44.798228  2460 solver.cpp:228] Iteration 6400, loss = 13440.2
I0422 09:51:44.798328  2460 solver.cpp:244]     Train net output #0: loss = 13440.2 (* 1 = 13440.2 loss)
I0422 09:51:44.798344  2460 sgd_solver.cpp:106] Iteration 6400, lr = 1e-10
I0422 09:52:14.784907  2460 solver.cpp:228] Iteration 6420, loss = 13265.2
I0422 09:52:14.785023  2460 solver.cpp:244]     Train net output #0: loss = 16949.1 (* 1 = 16949.1 loss)
I0422 09:52:14.785038  2460 sgd_solver.cpp:106] Iteration 6420, lr = 1e-10
I0422 09:52:44.762954  2460 solver.cpp:228] Iteration 6440, loss = 11461.7
I0422 09:52:44.763059  2460 solver.cpp:244]     Train net output #0: loss = 8514.58 (* 1 = 8514.58 loss)
I0422 09:52:44.763075  2460 sgd_solver.cpp:106] Iteration 6440, lr = 1e-10
I0422 09:53:14.735034  2460 solver.cpp:228] Iteration 6460, loss = 13644.7
I0422 09:53:14.735159  2460 solver.cpp:244]     Train net output #0: loss = 8342.75 (* 1 = 8342.75 loss)
I0422 09:53:14.735178  2460 sgd_solver.cpp:106] Iteration 6460, lr = 1e-10
I0422 09:53:44.707340  2460 solver.cpp:228] Iteration 6480, loss = 11552.9
I0422 09:53:44.707451  2460 solver.cpp:244]     Train net output #0: loss = 8963.27 (* 1 = 8963.27 loss)
I0422 09:53:44.707468  2460 sgd_solver.cpp:106] Iteration 6480, lr = 1e-10
I0422 09:54:22.388981  2460 solver.cpp:228] Iteration 6500, loss = 30413.4
I0422 09:54:22.389107  2460 solver.cpp:244]     Train net output #0: loss = 30413.4 (* 1 = 30413.4 loss)
I0422 09:54:22.389129  2460 sgd_solver.cpp:106] Iteration 6500, lr = 1e-10
I0422 09:54:52.371264  2460 solver.cpp:228] Iteration 6520, loss = 14232.9
I0422 09:54:52.371393  2460 solver.cpp:244]     Train net output #0: loss = 19243.4 (* 1 = 19243.4 loss)
I0422 09:54:52.371415  2460 sgd_solver.cpp:106] Iteration 6520, lr = 1e-10
I0422 09:55:22.343752  2460 solver.cpp:228] Iteration 6540, loss = 15655
I0422 09:55:22.343865  2460 solver.cpp:244]     Train net output #0: loss = 9454.79 (* 1 = 9454.79 loss)
I0422 09:55:22.343879  2460 sgd_solver.cpp:106] Iteration 6540, lr = 1e-10
I0422 09:55:52.323738  2460 solver.cpp:228] Iteration 6560, loss = 14245.5
I0422 09:55:52.323843  2460 solver.cpp:244]     Train net output #0: loss = 24310.5 (* 1 = 24310.5 loss)
I0422 09:55:52.323858  2460 sgd_solver.cpp:106] Iteration 6560, lr = 1e-10
I0422 09:56:22.299780  2460 solver.cpp:228] Iteration 6580, loss = 12404.8
I0422 09:56:22.299887  2460 solver.cpp:244]     Train net output #0: loss = 11724.3 (* 1 = 11724.3 loss)
I0422 09:56:22.299911  2460 sgd_solver.cpp:106] Iteration 6580, lr = 1e-10
I0422 09:57:00.023989  2460 solver.cpp:228] Iteration 6600, loss = 9251.79
I0422 09:57:00.024090  2460 solver.cpp:244]     Train net output #0: loss = 9251.79 (* 1 = 9251.79 loss)
I0422 09:57:00.024106  2460 sgd_solver.cpp:106] Iteration 6600, lr = 1e-10
I0422 09:57:30.004070  2460 solver.cpp:228] Iteration 6620, loss = 13792.9
I0422 09:57:30.004190  2460 solver.cpp:244]     Train net output #0: loss = 16383.6 (* 1 = 16383.6 loss)
I0422 09:57:30.004205  2460 sgd_solver.cpp:106] Iteration 6620, lr = 1e-10
I0422 09:57:59.986423  2460 solver.cpp:228] Iteration 6640, loss = 13336.2
I0422 09:57:59.986531  2460 solver.cpp:244]     Train net output #0: loss = 8969.54 (* 1 = 8969.54 loss)
I0422 09:57:59.986546  2460 sgd_solver.cpp:106] Iteration 6640, lr = 1e-10
I0422 09:58:29.960399  2460 solver.cpp:228] Iteration 6660, loss = 13308.6
I0422 09:58:29.960512  2460 solver.cpp:244]     Train net output #0: loss = 8022.06 (* 1 = 8022.06 loss)
I0422 09:58:29.960527  2460 sgd_solver.cpp:106] Iteration 6660, lr = 1e-10
I0422 09:58:59.944270  2460 solver.cpp:228] Iteration 6680, loss = 13700.8
I0422 09:58:59.944378  2460 solver.cpp:244]     Train net output #0: loss = 9224.33 (* 1 = 9224.33 loss)
I0422 09:58:59.944393  2460 sgd_solver.cpp:106] Iteration 6680, lr = 1e-10
I0422 09:59:37.640913  2460 solver.cpp:228] Iteration 6700, loss = 8366.28
I0422 09:59:37.641017  2460 solver.cpp:244]     Train net output #0: loss = 8366.28 (* 1 = 8366.28 loss)
I0422 09:59:37.641068  2460 sgd_solver.cpp:106] Iteration 6700, lr = 1e-10
I0422 10:00:07.609426  2460 solver.cpp:228] Iteration 6720, loss = 11758.8
I0422 10:00:07.609534  2460 solver.cpp:244]     Train net output #0: loss = 6942.48 (* 1 = 6942.48 loss)
I0422 10:00:07.609549  2460 sgd_solver.cpp:106] Iteration 6720, lr = 1e-10
I0422 10:00:37.594547  2460 solver.cpp:228] Iteration 6740, loss = 11068.8
I0422 10:00:37.594648  2460 solver.cpp:244]     Train net output #0: loss = 11812.1 (* 1 = 11812.1 loss)
I0422 10:00:37.594663  2460 sgd_solver.cpp:106] Iteration 6740, lr = 1e-10
I0422 10:01:07.571641  2460 solver.cpp:228] Iteration 6760, loss = 13193.2
I0422 10:01:07.571743  2460 solver.cpp:244]     Train net output #0: loss = 8565.83 (* 1 = 8565.83 loss)
I0422 10:01:07.571758  2460 sgd_solver.cpp:106] Iteration 6760, lr = 1e-10
I0422 10:01:37.550966  2460 solver.cpp:228] Iteration 6780, loss = 12546.2
I0422 10:01:37.551064  2460 solver.cpp:244]     Train net output #0: loss = 10864.1 (* 1 = 10864.1 loss)
I0422 10:01:37.551079  2460 sgd_solver.cpp:106] Iteration 6780, lr = 1e-10
I0422 10:02:15.220932  2460 solver.cpp:228] Iteration 6800, loss = 11962.7
I0422 10:02:15.221042  2460 solver.cpp:244]     Train net output #0: loss = 11962.7 (* 1 = 11962.7 loss)
I0422 10:02:15.221057  2460 sgd_solver.cpp:106] Iteration 6800, lr = 1e-10
I0422 10:02:45.194159  2460 solver.cpp:228] Iteration 6820, loss = 11622.9
I0422 10:02:45.194259  2460 solver.cpp:244]     Train net output #0: loss = 11706.1 (* 1 = 11706.1 loss)
I0422 10:02:45.194274  2460 sgd_solver.cpp:106] Iteration 6820, lr = 1e-10
I0422 10:03:15.162189  2460 solver.cpp:228] Iteration 6840, loss = 13454.9
I0422 10:03:15.162312  2460 solver.cpp:244]     Train net output #0: loss = 10415.8 (* 1 = 10415.8 loss)
I0422 10:03:15.162328  2460 sgd_solver.cpp:106] Iteration 6840, lr = 1e-10
I0422 10:03:45.142999  2460 solver.cpp:228] Iteration 6860, loss = 10429.8
I0422 10:03:45.143105  2460 solver.cpp:244]     Train net output #0: loss = 5796.03 (* 1 = 5796.03 loss)
I0422 10:03:45.143121  2460 sgd_solver.cpp:106] Iteration 6860, lr = 1e-10
I0422 10:04:15.122380  2460 solver.cpp:228] Iteration 6880, loss = 12416.1
I0422 10:04:15.122494  2460 solver.cpp:244]     Train net output #0: loss = 8559.81 (* 1 = 8559.81 loss)
I0422 10:04:15.122509  2460 sgd_solver.cpp:106] Iteration 6880, lr = 1e-10
I0422 10:04:52.786087  2460 solver.cpp:228] Iteration 6900, loss = 12054
I0422 10:04:52.786186  2460 solver.cpp:244]     Train net output #0: loss = 12054 (* 1 = 12054 loss)
I0422 10:04:52.786201  2460 sgd_solver.cpp:106] Iteration 6900, lr = 1e-10
I0422 10:05:22.759693  2460 solver.cpp:228] Iteration 6920, loss = 14766.7
I0422 10:05:22.759804  2460 solver.cpp:244]     Train net output #0: loss = 10497 (* 1 = 10497 loss)
I0422 10:05:22.759819  2460 sgd_solver.cpp:106] Iteration 6920, lr = 1e-10
I0422 10:05:52.742166  2460 solver.cpp:228] Iteration 6940, loss = 12677.3
I0422 10:05:52.742274  2460 solver.cpp:244]     Train net output #0: loss = 19826.2 (* 1 = 19826.2 loss)
I0422 10:05:52.742288  2460 sgd_solver.cpp:106] Iteration 6940, lr = 1e-10
I0422 10:06:22.723062  2460 solver.cpp:228] Iteration 6960, loss = 14310.3
I0422 10:06:22.723171  2460 solver.cpp:244]     Train net output #0: loss = 10362.3 (* 1 = 10362.3 loss)
I0422 10:06:22.723187  2460 sgd_solver.cpp:106] Iteration 6960, lr = 1e-10
I0422 10:06:52.691728  2460 solver.cpp:228] Iteration 6980, loss = 10581.1
I0422 10:06:52.691829  2460 solver.cpp:244]     Train net output #0: loss = 10011 (* 1 = 10011 loss)
I0422 10:06:52.691844  2460 sgd_solver.cpp:106] Iteration 6980, lr = 1e-10
I0422 10:07:21.166975  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_7000.caffemodel
I0422 10:07:24.449539  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_7000.solverstate
I0422 10:07:33.622032  2460 solver.cpp:228] Iteration 7000, loss = 7942.54
I0422 10:07:33.622139  2460 solver.cpp:244]     Train net output #0: loss = 7942.54 (* 1 = 7942.54 loss)
I0422 10:07:33.622182  2460 sgd_solver.cpp:106] Iteration 7000, lr = 1e-10
I0422 10:08:03.599997  2460 solver.cpp:228] Iteration 7020, loss = 12828.6
I0422 10:08:03.600122  2460 solver.cpp:244]     Train net output #0: loss = 13995 (* 1 = 13995 loss)
I0422 10:08:03.600141  2460 sgd_solver.cpp:106] Iteration 7020, lr = 1e-10
I0422 10:08:33.572710  2460 solver.cpp:228] Iteration 7040, loss = 13004.6
I0422 10:08:33.572818  2460 solver.cpp:244]     Train net output #0: loss = 12873.4 (* 1 = 12873.4 loss)
I0422 10:08:33.572834  2460 sgd_solver.cpp:106] Iteration 7040, lr = 1e-10
I0422 10:09:03.555563  2460 solver.cpp:228] Iteration 7060, loss = 12053.8
I0422 10:09:03.555685  2460 solver.cpp:244]     Train net output #0: loss = 9449.17 (* 1 = 9449.17 loss)
I0422 10:09:03.555702  2460 sgd_solver.cpp:106] Iteration 7060, lr = 1e-10
I0422 10:09:33.536550  2460 solver.cpp:228] Iteration 7080, loss = 12107.3
I0422 10:09:33.536659  2460 solver.cpp:244]     Train net output #0: loss = 11074 (* 1 = 11074 loss)
I0422 10:09:33.536674  2460 sgd_solver.cpp:106] Iteration 7080, lr = 1e-10
I0422 10:10:11.198151  2460 solver.cpp:228] Iteration 7100, loss = 11776.5
I0422 10:10:11.198266  2460 solver.cpp:244]     Train net output #0: loss = 11776.5 (* 1 = 11776.5 loss)
I0422 10:10:11.198290  2460 sgd_solver.cpp:106] Iteration 7100, lr = 1e-10
I0422 10:10:41.179466  2460 solver.cpp:228] Iteration 7120, loss = 13372
I0422 10:10:41.179589  2460 solver.cpp:244]     Train net output #0: loss = 9236.74 (* 1 = 9236.74 loss)
I0422 10:10:41.179610  2460 sgd_solver.cpp:106] Iteration 7120, lr = 1e-10
I0422 10:11:11.160811  2460 solver.cpp:228] Iteration 7140, loss = 10240.6
I0422 10:11:11.160928  2460 solver.cpp:244]     Train net output #0: loss = 9823.92 (* 1 = 9823.92 loss)
I0422 10:11:11.160943  2460 sgd_solver.cpp:106] Iteration 7140, lr = 1e-10
I0422 10:11:41.126679  2460 solver.cpp:228] Iteration 7160, loss = 10428.1
I0422 10:11:41.126792  2460 solver.cpp:244]     Train net output #0: loss = 6294.32 (* 1 = 6294.32 loss)
I0422 10:11:41.126807  2460 sgd_solver.cpp:106] Iteration 7160, lr = 1e-10
I0422 10:12:11.112797  2460 solver.cpp:228] Iteration 7180, loss = 15654.9
I0422 10:12:11.112922  2460 solver.cpp:244]     Train net output #0: loss = 10107.7 (* 1 = 10107.7 loss)
I0422 10:12:11.112943  2460 sgd_solver.cpp:106] Iteration 7180, lr = 1e-10
9:46:27.517795 Iteration 6200 overall accuracy 0.974872
>>> 2017-04-22 09:46:27.517917 Iteration 6200 mean accuracy 0.974169651484
>>> 2017-04-22 09:46:27.518115 Iteration 6200 mean IU 0.947769480741
>>> 2017-04-22 09:46:27.518224 Iteration 6200 fwavacc 0.951107430629
>>> 2017-04-22 09:48:58.001902 Begin seg tests
>>> 2017-04-22 09:49:05.085395 Iteration 6300 loss 10271.0195312
>>> 2017-04-22 09:49:05.085494 Iteration 6300 overall accuracy 0.978604
>>> 2017-04-22 09:49:05.085561 Iteration 6300 mean accuracy 0.978610209126
>>> 2017-04-22 09:49:05.085784 Iteration 6300 mean IU 0.958103984662
>>> 2017-04-22 09:49:05.085887 Iteration 6300 fwavacc 0.958104596182
>>> 2017-04-22 09:51:35.599763 Begin seg tests
>>> 2017-04-22 09:51:42.687664 Iteration 6400 loss 10757.71639
>>> 2017-04-22 09:51:42.687770 Iteration 6400 overall accuracy 0.977129333333
>>> 2017-04-22 09:51:42.687828 Iteration 6400 mean accuracy 0.977146985369
>>> 2017-04-22 09:51:42.688052 Iteration 6400 mean IU 0.955281405255
>>> 2017-04-22 09:51:42.688157 Iteration 6400 fwavacc 0.955281342182
>>> 2017-04-22 09:54:13.190239 Begin seg tests
>>> 2017-04-22 09:54:20.267112 Iteration 6500 loss 9706.16312663
>>> 2017-04-22 09:54:20.267214 Iteration 6500 overall accuracy 0.979885777778
>>> 2017-04-22 09:54:20.267273 Iteration 6500 mean accuracy 0.979643835505
>>> 2017-04-22 09:54:20.267489 Iteration 6500 mean IU 0.960504520127
>>> 2017-04-22 09:54:20.267592 Iteration 6500 fwavacc 0.960555792316
>>> 2017-04-22 09:56:50.785918 Begin seg tests
>>> 2017-04-22 09:56:57.872252 Iteration 6600 loss 11785.6552734
>>> 2017-04-22 09:56:57.872349 Iteration 6600 overall accuracy 0.974443555556
>>> 2017-04-22 09:56:57.872407 Iteration 6600 mean accuracy 0.974569686972
>>> 2017-04-22 09:56:57.872610 Iteration 6600 mean IU 0.950153720904
>>> 2017-04-22 09:56:57.872719 Iteration 6600 fwavacc 0.950165189878
>>> 2017-04-22 09:59:28.423024 Begin seg tests
>>> 2017-04-22 09:59:35.505611 Iteration 6700 loss 10447.9702148
>>> 2017-04-22 09:59:35.505710 Iteration 6700 overall accuracy 0.976944
>>> 2017-04-22 09:59:35.505768 Iteration 6700 mean accuracy 0.97654283909
>>> 2017-04-22 09:59:35.505977 Iteration 6700 mean IU 0.954821472983
>>> 2017-04-22 09:59:35.506090 Iteration 6700 fwavacc 0.95491009266
>>> 2017-04-22 10:02:06.037233 Begin seg tests
>>> 2017-04-22 10:02:13.123116 Iteration 6800 loss 13932.1958822
>>> 2017-04-22 10:02:13.123221 Iteration 6800 overall accuracy 0.968632888889
>>> 2017-04-22 10:02:13.123280 Iteration 6800 mean accuracy 0.967927132886
>>> 2017-04-22 10:02:13.123486 Iteration 6800 mean IU 0.938627883246
>>> 2017-04-22 10:02:13.123589 Iteration 6800 fwavacc 0.939163931528
>>> 2017-04-22 10:04:43.602436 Begin seg tests
>>> 2017-04-22 10:04:50.694808 Iteration 6900 loss 13071.1592611
>>> 2017-04-22 10:04:50.694910 Iteration 6900 overall accuracy 0.972396444444
>>> 2017-04-22 10:04:50.694977 Iteration 6900 mean accuracy 0.973512471373
>>> 2017-04-22 10:04:50.695196 Iteration 6900 mean IU 0.9443730665
>>> 2017-04-22 10:04:50.695313 Iteration 6900 fwavacc 0.946440288991
>>> 2017-04-22 10:07:25.421816 Begin seg tests
>>> 2017-04-22 10:07:31.509708 Iteration 7000 loss 13205.1429443
>>> 2017-04-22 10:07:31.509815 Iteration 7000 overall accuracy 0.971649777778
>>> 2017-04-22 10:07:31.509882 Iteration 7000 mean accuracy 0.969372470573
>>> 2017-04-22 10:07:31.510175 Iteration 7000 mean IU 0.942822687837
>>> 2017-04-22 10:07:31.510289 Iteration 7000 fwavacc 0.944851779382
>>> 2017-04-22 10:10:02.012082 Begin seg tests
>>> 2017-04-22 10:10:09.096026 Iteration 7100 loss 12981.0231527
>>> 2017-04-22 10:10:09.096122 Iteration 7100 overall accuracy 0.971566222222
>>> 2017-04-22 10:10:09.096189 Iteration 7100 mean accuracy 0.971791297935
>>> 2017-04-22 10:10:09.096422 Iteration 7100 mean IU 0.944615306133
>>> 2017-04-22 10:10:09.096526 Iteration 7100 fwavacc 0.944720780878
>>> 2017-04-22 10:12:39.606960 Begin seg tests
>>> 2017-04-22 10:12:46.694765 Iteration 7200 loss 13168.151652
>>> 2017-04-22 10:12:46.694867 Iteration 7200 overall accuracy 0.969809777778
>>> 2017-04-22 10:12:46.69493I0422 10:12:48.802001  2460 solver.cpp:228] Iteration 7200, loss = 11388
I0422 10:12:48.802112  2460 solver.cpp:244]     Train net output #0: loss = 11388 (* 1 = 11388 loss)
I0422 10:12:48.802127  2460 sgd_solver.cpp:106] Iteration 7200, lr = 1e-10
I0422 10:13:18.782894  2460 solver.cpp:228] Iteration 7220, loss = 12141.7
I0422 10:13:18.782997  2460 solver.cpp:244]     Train net output #0: loss = 14903 (* 1 = 14903 loss)
I0422 10:13:18.783013  2460 sgd_solver.cpp:106] Iteration 7220, lr = 1e-10
I0422 10:13:48.752178  2460 solver.cpp:228] Iteration 7240, loss = 10959.2
I0422 10:13:48.752285  2460 solver.cpp:244]     Train net output #0: loss = 10073.1 (* 1 = 10073.1 loss)
I0422 10:13:48.752300  2460 sgd_solver.cpp:106] Iteration 7240, lr = 1e-10
I0422 10:14:18.733820  2460 solver.cpp:228] Iteration 7260, loss = 12920.8
I0422 10:14:18.733932  2460 solver.cpp:244]     Train net output #0: loss = 9276.41 (* 1 = 9276.41 loss)
I0422 10:14:18.733955  2460 sgd_solver.cpp:106] Iteration 7260, lr = 1e-10
I0422 10:14:48.725389  2460 solver.cpp:228] Iteration 7280, loss = 10675.5
I0422 10:14:48.725488  2460 solver.cpp:244]     Train net output #0: loss = 12330.5 (* 1 = 12330.5 loss)
I0422 10:14:48.725503  2460 sgd_solver.cpp:106] Iteration 7280, lr = 1e-10
I0422 10:15:26.392534  2460 solver.cpp:228] Iteration 7300, loss = 6248.82
I0422 10:15:26.392658  2460 solver.cpp:244]     Train net output #0: loss = 6248.82 (* 1 = 6248.82 loss)
I0422 10:15:26.392675  2460 sgd_solver.cpp:106] Iteration 7300, lr = 1e-10
I0422 10:15:56.361893  2460 solver.cpp:228] Iteration 7320, loss = 16434.3
I0422 10:15:56.362004  2460 solver.cpp:244]     Train net output #0: loss = 9433.19 (* 1 = 9433.19 loss)
I0422 10:15:56.362061  2460 sgd_solver.cpp:106] Iteration 7320, lr = 1e-10
I0422 10:16:26.336125  2460 solver.cpp:228] Iteration 7340, loss = 14013.5
I0422 10:16:26.336238  2460 solver.cpp:244]     Train net output #0: loss = 10593.1 (* 1 = 10593.1 loss)
I0422 10:16:26.336253  2460 sgd_solver.cpp:106] Iteration 7340, lr = 1e-10
I0422 10:16:56.312741  2460 solver.cpp:228] Iteration 7360, loss = 11145.1
I0422 10:16:56.312845  2460 solver.cpp:244]     Train net output #0: loss = 10478.9 (* 1 = 10478.9 loss)
I0422 10:16:56.312870  2460 sgd_solver.cpp:106] Iteration 7360, lr = 1e-10
I0422 10:17:26.286459  2460 solver.cpp:228] Iteration 7380, loss = 10979
I0422 10:17:26.286569  2460 solver.cpp:244]     Train net output #0: loss = 7638.79 (* 1 = 7638.79 loss)
I0422 10:17:26.286586  2460 sgd_solver.cpp:106] Iteration 7380, lr = 1e-10
I0422 10:18:03.971011  2460 solver.cpp:228] Iteration 7400, loss = 11786.2
I0422 10:18:03.971110  2460 solver.cpp:244]     Train net output #0: loss = 11786.2 (* 1 = 11786.2 loss)
I0422 10:18:03.971125  2460 sgd_solver.cpp:106] Iteration 7400, lr = 1e-10
I0422 10:18:33.946635  2460 solver.cpp:228] Iteration 7420, loss = 14200.8
I0422 10:18:33.946749  2460 solver.cpp:244]     Train net output #0: loss = 14785 (* 1 = 14785 loss)
I0422 10:18:33.946768  2460 sgd_solver.cpp:106] Iteration 7420, lr = 1e-10
I0422 10:19:03.919706  2460 solver.cpp:228] Iteration 7440, loss = 12187.3
I0422 10:19:03.919812  2460 solver.cpp:244]     Train net output #0: loss = 8506.53 (* 1 = 8506.53 loss)
I0422 10:19:03.919828  2460 sgd_solver.cpp:106] Iteration 7440, lr = 1e-10
I0422 10:19:33.897775  2460 solver.cpp:228] Iteration 7460, loss = 13024.3
I0422 10:19:33.897887  2460 solver.cpp:244]     Train net output #0: loss = 7910.05 (* 1 = 7910.05 loss)
I0422 10:19:33.897902  2460 sgd_solver.cpp:106] Iteration 7460, lr = 1e-10
I0422 10:20:03.866729  2460 solver.cpp:228] Iteration 7480, loss = 10846
I0422 10:20:03.866832  2460 solver.cpp:244]     Train net output #0: loss = 8751.46 (* 1 = 8751.46 loss)
I0422 10:20:03.866848  2460 sgd_solver.cpp:106] Iteration 7480, lr = 1e-10
I0422 10:20:41.571745  2460 solver.cpp:228] Iteration 7500, loss = 7562.9
I0422 10:20:41.571861  2460 solver.cpp:244]     Train net output #0: loss = 7562.9 (* 1 = 7562.9 loss)
I0422 10:20:41.571877  2460 sgd_solver.cpp:106] Iteration 7500, lr = 1e-10
I0422 10:21:11.536461  2460 solver.cpp:228] Iteration 7520, loss = 13135.9
I0422 10:21:11.536578  2460 solver.cpp:244]     Train net output #0: loss = 12482.1 (* 1 = 12482.1 loss)
I0422 10:21:11.536604  2460 sgd_solver.cpp:106] Iteration 7520, lr = 1e-10
I0422 10:21:41.515594  2460 solver.cpp:228] Iteration 7540, loss = 11155.6
I0422 10:21:41.515722  2460 solver.cpp:244]     Train net output #0: loss = 8427.32 (* 1 = 8427.32 loss)
I0422 10:21:41.515745  2460 sgd_solver.cpp:106] Iteration 7540, lr = 1e-10
I0422 10:22:11.500856  2460 solver.cpp:228] Iteration 7560, loss = 13981.5
I0422 10:22:11.500960  2460 solver.cpp:244]     Train net output #0: loss = 6419.25 (* 1 = 6419.25 loss)
I0422 10:22:11.500975  2460 sgd_solver.cpp:106] Iteration 7560, lr = 1e-10
I0422 10:22:41.471314  2460 solver.cpp:228] Iteration 7580, loss = 12496.6
I0422 10:22:41.471436  2460 solver.cpp:244]     Train net output #0: loss = 9511.27 (* 1 = 9511.27 loss)
I0422 10:22:41.471452  2460 sgd_solver.cpp:106] Iteration 7580, lr = 1e-10
I0422 10:23:19.153811  2460 solver.cpp:228] Iteration 7600, loss = 18342
I0422 10:23:19.153925  2460 solver.cpp:244]     Train net output #0: loss = 18342 (* 1 = 18342 loss)
I0422 10:23:19.153940  2460 sgd_solver.cpp:106] Iteration 7600, lr = 1e-10
I0422 10:23:49.139025  2460 solver.cpp:228] Iteration 7620, loss = 11312.7
I0422 10:23:49.139149  2460 solver.cpp:244]     Train net output #0: loss = 17799.5 (* 1 = 17799.5 loss)
I0422 10:23:49.139165  2460 sgd_solver.cpp:106] Iteration 7620, lr = 1e-10
I0422 10:24:19.115265  2460 solver.cpp:228] Iteration 7640, loss = 11867.7
I0422 10:24:19.115377  2460 solver.cpp:244]     Train net output #0: loss = 7461.07 (* 1 = 7461.07 loss)
I0422 10:24:19.115409  2460 sgd_solver.cpp:106] Iteration 7640, lr = 1e-10
I0422 10:24:49.096689  2460 solver.cpp:228] Iteration 7660, loss = 13888.8
I0422 10:24:49.096799  2460 solver.cpp:244]     Train net output #0: loss = 16810.3 (* 1 = 16810.3 loss)
I0422 10:24:49.096815  2460 sgd_solver.cpp:106] Iteration 7660, lr = 1e-10
I0422 10:25:19.077260  2460 solver.cpp:228] Iteration 7680, loss = 12699.9
I0422 10:25:19.077359  2460 solver.cpp:244]     Train net output #0: loss = 6929.87 (* 1 = 6929.87 loss)
I0422 10:25:19.077374  2460 sgd_solver.cpp:106] Iteration 7680, lr = 1e-10
I0422 10:25:56.791677  2460 solver.cpp:228] Iteration 7700, loss = 20505.1
I0422 10:25:56.791780  2460 solver.cpp:244]     Train net output #0: loss = 20505.1 (* 1 = 20505.1 loss)
I0422 10:25:56.791795  2460 sgd_solver.cpp:106] Iteration 7700, lr = 1e-10
I0422 10:26:26.766711  2460 solver.cpp:228] Iteration 7720, loss = 11534
I0422 10:26:26.766819  2460 solver.cpp:244]     Train net output #0: loss = 7402.65 (* 1 = 7402.65 loss)
I0422 10:26:26.766834  2460 sgd_solver.cpp:106] Iteration 7720, lr = 1e-10
I0422 10:26:56.732059  2460 solver.cpp:228] Iteration 7740, loss = 10922.2
I0422 10:26:56.732175  2460 solver.cpp:244]     Train net output #0: loss = 7790.2 (* 1 = 7790.2 loss)
I0422 10:26:56.732200  2460 sgd_solver.cpp:106] Iteration 7740, lr = 1e-10
I0422 10:27:26.703143  2460 solver.cpp:228] Iteration 7760, loss = 9354.86
I0422 10:27:26.703259  2460 solver.cpp:244]     Train net output #0: loss = 10778.7 (* 1 = 10778.7 loss)
I0422 10:27:26.703275  2460 sgd_solver.cpp:106] Iteration 7760, lr = 1e-10
I0422 10:27:56.680140  2460 solver.cpp:228] Iteration 7780, loss = 11272.5
I0422 10:27:56.680260  2460 solver.cpp:244]     Train net output #0: loss = 26715.4 (* 1 = 26715.4 loss)
I0422 10:27:56.680277  2460 sgd_solver.cpp:106] Iteration 7780, lr = 1e-10
I0422 10:28:34.363356  2460 solver.cpp:228] Iteration 7800, loss = 14405.5
I0422 10:28:34.363456  2460 solver.cpp:244]     Train net output #0: loss = 14405.5 (* 1 = 14405.5 loss)
I0422 10:28:34.363471  2460 sgd_solver.cpp:106] Iteration 7800, lr = 1e-10
I0422 10:29:04.337455  2460 solver.cpp:228] Iteration 7820, loss = 14035.1
I0422 10:29:04.337563  2460 solver.cpp:244]     Train net output #0: loss = 9137.54 (* 1 = 9137.54 loss)
I0422 10:29:04.337579  2460 sgd_solver.cpp:106] Iteration 7820, lr = 1e-10
I0422 10:29:34.325788  2460 solver.cpp:228] Iteration 7840, loss = 12878.3
I0422 10:29:34.325898  2460 solver.cpp:244]     Train net output #0: loss = 10871.6 (* 1 = 10871.6 loss)
I0422 10:29:34.325914  2460 sgd_solver.cpp:106] Iteration 7840, lr = 1e-10
I0422 10:30:04.314132  2460 solver.cpp:228] Iteration 7860, loss = 10617.9
I0422 10:30:04.314226  2460 solver.cpp:244]     Train net output #0: loss = 6549.16 (* 1 = 6549.16 loss)
I0422 10:30:04.314240  2460 sgd_solver.cpp:106] Iteration 7860, lr = 1e-10
I0422 10:30:34.294736  2460 solver.cpp:228] Iteration 7880, loss = 10147.8
I0422 10:30:34.294852  2460 solver.cpp:244]     Train net output #0: loss = 7502.77 (* 1 = 7502.77 loss)
I0422 10:30:34.294867  2460 sgd_solver.cpp:106] Iteration 7880, lr = 1e-10
I0422 10:31:12.020992  2460 solver.cpp:228] Iteration 7900, loss = 11451.2
I0422 10:31:12.021090  2460 solver.cpp:244]     Train net output #0: loss = 11451.2 (* 1 = 11451.2 loss)
I0422 10:31:12.021106  2460 sgd_solver.cpp:106] Iteration 7900, lr = 1e-10
I0422 10:31:42.001014  2460 solver.cpp:228] Iteration 7920, loss = 10326.7
I0422 10:31:42.001116  2460 solver.cpp:244]     Train net output #0: loss = 36243.7 (* 1 = 36243.7 loss)
I0422 10:31:42.001132  2460 sgd_solver.cpp:106] Iteration 7920, lr = 1e-10
I0422 10:32:11.973611  2460 solver.cpp:228] Iteration 7940, loss = 11353.7
I0422 10:32:11.973739  2460 solver.cpp:244]     Train net output #0: loss = 5986.23 (* 1 = 5986.23 loss)
I0422 10:32:11.973760  2460 sgd_solver.cpp:106] Iteration 7940, lr = 1e-10
I0422 10:32:41.952024  2460 solver.cpp:228] Iteration 7960, loss = 13269.9
I0422 10:32:41.952136  2460 solver.cpp:244]     Train net output #0: loss = 27559.2 (* 1 = 27559.2 loss)
I0422 10:32:41.952153  2460 sgd_solver.cpp:106] Iteration 7960, lr = 1e-10
I0422 10:33:11.935767  2460 solver.cpp:228] Iteration 7980, loss = 11413
I0422 10:33:11.935874  2460 solver.cpp:244]     Train net output #0: loss = 6517.78 (* 1 = 6517.78 loss)
I0422 10:33:11.935889  2460 sgd_solver.cpp:106] Iteration 7980, lr = 1e-10
I0422 10:33:40.421252  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_8000.caffemodel
I0422 10:33:43.729758  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_8000.solverstate
I0422 10:33:53.164163  2460 solver.cpp:228] Iteration 8000, loss = 8074.11
I0422 10:33:53.164266  2460 solver.cpp:244]     Train net output #0: loss = 8074.11 (* 1 = 8074.11 loss)
I0422 10:33:53.164299  2460 sgd_solver.cpp:106] Iteration 8000, lr = 1e-10
I0422 10:34:23.139874  2460 solver.cpp:228] Iteration 8020, loss = 11521.9
I0422 10:34:23.139981  2460 solver.cpp:244]     Train net output #0: loss = 19589.9 (* 1 = 19589.9 loss)
I0422 10:34:23.140007  2460 sgd_solver.cpp:106] Iteration 8020, lr = 1e-10
I0422 10:34:53.112097  2460 solver.cpp:228] Iteration 8040, loss = 10727.1
I0422 10:34:53.112211  2460 solver.cpp:244]     Train net output #0: loss = 6800.52 (* 1 = 6800.52 loss)
I0422 10:34:53.112226  2460 sgd_solver.cpp:106] Iteration 8040, lr = 1e-10
I0422 10:35:23.090250  2460 solver.cpp:228] Iteration 8060, loss = 10324
I0422 10:35:23.090366  2460 solver.cpp:244]     Train net output #0: loss = 11321.5 (* 1 = 11321.5 loss)
I0422 10:35:23.090381  2460 sgd_solver.cpp:106] Iteration 8060, lr = 1e-10
I0422 10:35:53.071969  2460 solver.cpp:228] Iteration 8080, loss = 11327.7
I0422 10:35:53.072070  2460 solver.cpp:244]     Train net output #0: loss = 7450.64 (* 1 = 7450.64 loss)
I0422 10:35:53.072085  2460 sgd_solver.cpp:106] Iteration 8080, lr = 1e-10
I0422 10:36:30.793448  2460 solver.cpp:228] Iteration 8100, loss = 10149.7
I0422 10:36:30.793553  2460 solver.cpp:244]     Train net output #0: loss = 10149.7 (* 1 = 10149.7 loss)
I0422 10:36:30.793568  2460 sgd_solver.cpp:106] Iteration 8100, lr = 1e-10
I0422 10:37:00.768859  2460 solver.cpp:228] Iteration 8120, loss = 12001.6
I0422 10:37:00.768980  2460 solver.cpp:244]     Train net output #0: loss = 18738.9 (* 1 = 18738.9 loss)
I0422 10:37:00.768996  2460 sgd_solver.cpp:106] Iteration 8120, lr = 1e-10
I0422 10:37:30.743945  2460 solver.cpp:228] Iteration 8140, loss = 11071.3
I0422 10:37:30.744058  2460 solver.cpp:244]     Train net output #0: loss = 11067.5 (* 1 = 11067.5 loss)
I0422 10:37:30.744082  2460 sgd_solver.cpp:106] Iteration 8140, lr = 1e-10
I0422 10:38:00.724182  2460 solver.cpp:228] Iteration 8160, loss = 12489.9
I0422 10:38:00.724300  2460 solver.cpp:244]     Train net output #0: loss = 20256 (* 1 = 20256 loss)
I0422 10:38:00.724318  2460 sgd_solver.cpp:106] Iteration 8160, lr = 1e-10
I0422 10:38:30.698243  2460 solver.cpp:228] Iteration 8180, loss = 9424.72
I0422 10:38:30.698369  2460 solver.cpp:244]     Train net output #0: loss = 9440.41 (* 1 = 9440.41 loss)
I0422 10:38:30.698384  2460 sgd_solver.cpp:106] Iteration 8180, lr = 1e-10
4 Iteration 7200 mean accuracy 0.969685728759
>>> 2017-04-22 10:12:46.695215 Iteration 7200 mean IU 0.937768819002
>>> 2017-04-22 10:12:46.695321 Iteration 7200 fwavacc 0.941607412359
>>> 2017-04-22 10:15:17.200438 Begin seg tests
>>> 2017-04-22 10:15:24.284895 Iteration 7300 loss 10499.8231201
>>> 2017-04-22 10:15:24.284997 Iteration 7300 overall accuracy 0.977864444444
>>> 2017-04-22 10:15:24.285055 Iteration 7300 mean accuracy 0.977427040972
>>> 2017-04-22 10:15:24.285264 Iteration 7300 mean IU 0.956409499686
>>> 2017-04-22 10:15:24.285375 Iteration 7300 fwavacc 0.95668032067
>>> 2017-04-22 10:17:54.768932 Begin seg tests
>>> 2017-04-22 10:18:01.867481 Iteration 7400 loss 9626.44144694
>>> 2017-04-22 10:18:01.867577 Iteration 7400 overall accuracy 0.979710666667
>>> 2017-04-22 10:18:01.867657 Iteration 7400 mean accuracy 0.978016212148
>>> 2017-04-22 10:18:01.867881 Iteration 7400 mean IU 0.959044988683
>>> 2017-04-22 10:18:01.867986 Iteration 7400 fwavacc 0.960207176236
>>> 2017-04-22 10:20:32.343554 Begin seg tests
>>> 2017-04-22 10:20:39.427571 Iteration 7500 loss 10077.7706299
>>> 2017-04-22 10:20:39.427690 Iteration 7500 overall accuracy 0.978965777778
>>> 2017-04-22 10:20:39.427750 Iteration 7500 mean accuracy 0.976054145987
>>> 2017-04-22 10:20:39.427961 Iteration 7500 mean IU 0.956177667265
>>> 2017-04-22 10:20:39.428079 Iteration 7500 fwavacc 0.95878771698
>>> 2017-04-22 10:23:09.959062 Begin seg tests
>>> 2017-04-22 10:23:17.051208 Iteration 7600 loss 14054.4724935
>>> 2017-04-22 10:23:17.051313 Iteration 7600 overall accuracy 0.969714222222
>>> 2017-04-22 10:23:17.051368 Iteration 7600 mean accuracy 0.969654290807
>>> 2017-04-22 10:23:17.051601 Iteration 7600 mean IU 0.941122406676
>>> 2017-04-22 10:23:17.051728 Iteration 7600 fwavacc 0.941210826535
>>> 2017-04-22 10:25:47.579264 Begin seg tests
>>> 2017-04-22 10:25:54.660089 Iteration 7700 loss 13548.4256592
>>> 2017-04-22 10:25:54.660181 Iteration 7700 overall accuracy 0.971744888889
>>> 2017-04-22 10:25:54.660238 Iteration 7700 mean accuracy 0.971266256606
>>> 2017-04-22 10:25:54.660437 Iteration 7700 mean IU 0.944551780901
>>> 2017-04-22 10:25:54.660542 Iteration 7700 fwavacc 0.945044166285
>>> 2017-04-22 10:28:25.160521 Begin seg tests
>>> 2017-04-22 10:28:32.249687 Iteration 7800 loss 12392.8255208
>>> 2017-04-22 10:28:32.249799 Iteration 7800 overall accuracy 0.973248444444
>>> 2017-04-22 10:28:32.249864 Iteration 7800 mean accuracy 0.972250779538
>>> 2017-04-22 10:28:32.250085 Iteration 7800 mean IU 0.947515986642
>>> 2017-04-22 10:28:32.250216 Iteration 7800 fwavacc 0.947851775086
>>> 2017-04-22 10:31:02.780892 Begin seg tests
>>> 2017-04-22 10:31:09.855829 Iteration 7900 loss 10018.0931803
>>> 2017-04-22 10:31:09.855927 Iteration 7900 overall accuracy 0.978854666667
>>> 2017-04-22 10:31:09.855990 Iteration 7900 mean accuracy 0.977550449302
>>> 2017-04-22 10:31:09.856215 Iteration 7900 mean IU 0.957592624334
>>> 2017-04-22 10:31:09.856319 Iteration 7900 fwavacc 0.95857199094
>>> 2017-04-22 10:33:44.717608 Begin seg tests
>>> 2017-04-22 10:33:50.813538 Iteration 8000 loss 10706.8126628
>>> 2017-04-22 10:33:50.813635 Iteration 8000 overall accuracy 0.975461777778
>>> 2017-04-22 10:33:50.813697 Iteration 8000 mean accuracy 0.975629811693
>>> 2017-04-22 10:33:50.813923 Iteration 8000 mean IU 0.951905126427
>>> 2017-04-22 10:33:50.814027 Iteration 8000 fwavacc 0.952116282819
>>> 2017-04-22 10:36:21.546397 Begin seg tests
>>> 2017-04-22 10:36:28.628361 Iteration 8100 loss 9828.87947591
>>> 2017-04-22 10:36:28.628472 Iteration 8100 overall accuracy 0.979005777778
>>> 2017-04-22 10:36:28.628554 Iteration 8100 mean accuracy 0.978934000369
>>> 2017-04-22 10:36:28.628800 Iteration 8100 mean IU 0.958863738512
>>> 2017-04-22 10:36:28.628935 Iteration 8100 fwavacc 0.95887017243
>>> 2017-04-22 10:38:59.181986 Begin seg tests
>>> 2017-04-22 10:39:06.269348 Iteration 8200 loss 10780.6750081
>>> 2017-04-22 10:39:06.269438 Iteration 8200 overall accuracy 0.975813777778
>>> 2017-04-22 10:39:06.269496 Iteration 8200 mean accuracy 0.974659059114
>>> 2017-04-22 10:39:06.I0422 10:39:08.425353  2460 solver.cpp:228] Iteration 8200, loss = 9629.26
I0422 10:39:08.425456  2460 solver.cpp:244]     Train net output #0: loss = 9629.26 (* 1 = 9629.26 loss)
I0422 10:39:08.425470  2460 sgd_solver.cpp:106] Iteration 8200, lr = 1e-10
I0422 10:39:38.399777  2460 solver.cpp:228] Iteration 8220, loss = 13628.7
I0422 10:39:38.399885  2460 solver.cpp:244]     Train net output #0: loss = 9384.43 (* 1 = 9384.43 loss)
I0422 10:39:38.399901  2460 sgd_solver.cpp:106] Iteration 8220, lr = 1e-10
I0422 10:40:08.384352  2460 solver.cpp:228] Iteration 8240, loss = 13220.5
I0422 10:40:08.384456  2460 solver.cpp:244]     Train net output #0: loss = 8904.79 (* 1 = 8904.79 loss)
I0422 10:40:08.384471  2460 sgd_solver.cpp:106] Iteration 8240, lr = 1e-10
I0422 10:40:38.362864  2460 solver.cpp:228] Iteration 8260, loss = 13498.3
I0422 10:40:38.362977  2460 solver.cpp:244]     Train net output #0: loss = 8455.72 (* 1 = 8455.72 loss)
I0422 10:40:38.363028  2460 sgd_solver.cpp:106] Iteration 8260, lr = 1e-10
I0422 10:41:08.337780  2460 solver.cpp:228] Iteration 8280, loss = 12356
I0422 10:41:08.337895  2460 solver.cpp:244]     Train net output #0: loss = 17759.2 (* 1 = 17759.2 loss)
I0422 10:41:08.337911  2460 sgd_solver.cpp:106] Iteration 8280, lr = 1e-10
I0422 10:41:46.064774  2460 solver.cpp:228] Iteration 8300, loss = 5892.04
I0422 10:41:46.064893  2460 solver.cpp:244]     Train net output #0: loss = 5892.04 (* 1 = 5892.04 loss)
I0422 10:41:46.064909  2460 sgd_solver.cpp:106] Iteration 8300, lr = 1e-10
I0422 10:42:16.039373  2460 solver.cpp:228] Iteration 8320, loss = 11551.2
I0422 10:42:16.039497  2460 solver.cpp:244]     Train net output #0: loss = 10899.1 (* 1 = 10899.1 loss)
I0422 10:42:16.039515  2460 sgd_solver.cpp:106] Iteration 8320, lr = 1e-10
I0422 10:42:46.030125  2460 solver.cpp:228] Iteration 8340, loss = 9837.15
I0422 10:42:46.030222  2460 solver.cpp:244]     Train net output #0: loss = 9831.61 (* 1 = 9831.61 loss)
I0422 10:42:46.030239  2460 sgd_solver.cpp:106] Iteration 8340, lr = 1e-10
I0422 10:43:16.005138  2460 solver.cpp:228] Iteration 8360, loss = 11996.5
I0422 10:43:16.005239  2460 solver.cpp:244]     Train net output #0: loss = 16847.2 (* 1 = 16847.2 loss)
I0422 10:43:16.005254  2460 sgd_solver.cpp:106] Iteration 8360, lr = 1e-10
I0422 10:43:45.980901  2460 solver.cpp:228] Iteration 8380, loss = 12768.4
I0422 10:43:45.981014  2460 solver.cpp:244]     Train net output #0: loss = 7722.56 (* 1 = 7722.56 loss)
I0422 10:43:45.981027  2460 sgd_solver.cpp:106] Iteration 8380, lr = 1e-10
I0422 10:44:23.685380  2460 solver.cpp:228] Iteration 8400, loss = 8656.85
I0422 10:44:23.685488  2460 solver.cpp:244]     Train net output #0: loss = 8656.85 (* 1 = 8656.85 loss)
I0422 10:44:23.685504  2460 sgd_solver.cpp:106] Iteration 8400, lr = 1e-10
I0422 10:44:53.660435  2460 solver.cpp:228] Iteration 8420, loss = 11004.3
I0422 10:44:53.660560  2460 solver.cpp:244]     Train net output #0: loss = 10047 (* 1 = 10047 loss)
I0422 10:44:53.660575  2460 sgd_solver.cpp:106] Iteration 8420, lr = 1e-10
I0422 10:45:23.641321  2460 solver.cpp:228] Iteration 8440, loss = 10709.6
I0422 10:45:23.641436  2460 solver.cpp:244]     Train net output #0: loss = 7898.18 (* 1 = 7898.18 loss)
I0422 10:45:23.641450  2460 sgd_solver.cpp:106] Iteration 8440, lr = 1e-10
I0422 10:45:53.623338  2460 solver.cpp:228] Iteration 8460, loss = 12040
I0422 10:45:53.623442  2460 solver.cpp:244]     Train net output #0: loss = 8294.79 (* 1 = 8294.79 loss)
I0422 10:45:53.623458  2460 sgd_solver.cpp:106] Iteration 8460, lr = 1e-10
I0422 10:46:23.597527  2460 solver.cpp:228] Iteration 8480, loss = 11838.4
I0422 10:46:23.597632  2460 solver.cpp:244]     Train net output #0: loss = 10545.8 (* 1 = 10545.8 loss)
I0422 10:46:23.597647  2460 sgd_solver.cpp:106] Iteration 8480, lr = 1e-10
I0422 10:47:01.334369  2460 solver.cpp:228] Iteration 8500, loss = 12182.4
I0422 10:47:01.334484  2460 solver.cpp:244]     Train net output #0: loss = 12182.4 (* 1 = 12182.4 loss)
I0422 10:47:01.334501  2460 sgd_solver.cpp:106] Iteration 8500, lr = 1e-10
I0422 10:47:31.319556  2460 solver.cpp:228] Iteration 8520, loss = 10694.1
I0422 10:47:31.319686  2460 solver.cpp:244]     Train net output #0: loss = 22802.7 (* 1 = 22802.7 loss)
I0422 10:47:31.319710  2460 sgd_solver.cpp:106] Iteration 8520, lr = 1e-10
I0422 10:48:01.299173  2460 solver.cpp:228] Iteration 8540, loss = 11555.8
I0422 10:48:01.299275  2460 solver.cpp:244]     Train net output #0: loss = 10225.2 (* 1 = 10225.2 loss)
I0422 10:48:01.299295  2460 sgd_solver.cpp:106] Iteration 8540, lr = 1e-10
I0422 10:48:31.273993  2460 solver.cpp:228] Iteration 8560, loss = 11794.8
I0422 10:48:31.274094  2460 solver.cpp:244]     Train net output #0: loss = 18655.9 (* 1 = 18655.9 loss)
I0422 10:48:31.274111  2460 sgd_solver.cpp:106] Iteration 8560, lr = 1e-10
I0422 10:49:01.241446  2460 solver.cpp:228] Iteration 8580, loss = 9233.05
I0422 10:49:01.241554  2460 solver.cpp:244]     Train net output #0: loss = 9188.56 (* 1 = 9188.56 loss)
I0422 10:49:01.241569  2460 sgd_solver.cpp:106] Iteration 8580, lr = 1e-10
I0422 10:49:38.980679  2460 solver.cpp:228] Iteration 8600, loss = 22341
I0422 10:49:38.980788  2460 solver.cpp:244]     Train net output #0: loss = 22341 (* 1 = 22341 loss)
I0422 10:49:38.980808  2460 sgd_solver.cpp:106] Iteration 8600, lr = 1e-10
I0422 10:50:08.957614  2460 solver.cpp:228] Iteration 8620, loss = 9933.38
I0422 10:50:08.957715  2460 solver.cpp:244]     Train net output #0: loss = 12875.5 (* 1 = 12875.5 loss)
I0422 10:50:08.957737  2460 sgd_solver.cpp:106] Iteration 8620, lr = 1e-10
I0422 10:50:38.937366  2460 solver.cpp:228] Iteration 8640, loss = 9893.4
I0422 10:50:38.937450  2460 solver.cpp:244]     Train net output #0: loss = 7707.56 (* 1 = 7707.56 loss)
I0422 10:50:38.937463  2460 sgd_solver.cpp:106] Iteration 8640, lr = 1e-10
I0422 10:51:08.913305  2460 solver.cpp:228] Iteration 8660, loss = 10958.1
I0422 10:51:08.913398  2460 solver.cpp:244]     Train net output #0: loss = 7229.48 (* 1 = 7229.48 loss)
I0422 10:51:08.913410  2460 sgd_solver.cpp:106] Iteration 8660, lr = 1e-10
I0422 10:51:38.890094  2460 solver.cpp:228] Iteration 8680, loss = 10766.1
I0422 10:51:38.890182  2460 solver.cpp:244]     Train net output #0: loss = 5843.43 (* 1 = 5843.43 loss)
I0422 10:51:38.890194  2460 sgd_solver.cpp:106] Iteration 8680, lr = 1e-10
I0422 10:52:16.601683  2460 solver.cpp:228] Iteration 8700, loss = 14607.1
I0422 10:52:16.601768  2460 solver.cpp:244]     Train net output #0: loss = 14607.1 (* 1 = 14607.1 loss)
I0422 10:52:16.601781  2460 sgd_solver.cpp:106] Iteration 8700, lr = 1e-10
I0422 10:52:46.577188  2460 solver.cpp:228] Iteration 8720, loss = 10416
I0422 10:52:46.577270  2460 solver.cpp:244]     Train net output #0: loss = 18920 (* 1 = 18920 loss)
I0422 10:52:46.577282  2460 sgd_solver.cpp:106] Iteration 8720, lr = 1e-10
I0422 10:53:16.546103  2460 solver.cpp:228] Iteration 8740, loss = 11615.1
I0422 10:53:16.546190  2460 solver.cpp:244]     Train net output #0: loss = 15682.6 (* 1 = 15682.6 loss)
I0422 10:53:16.546202  2460 sgd_solver.cpp:106] Iteration 8740, lr = 1e-10
I0422 10:53:46.524199  2460 solver.cpp:228] Iteration 8760, loss = 11465.4
I0422 10:53:46.524286  2460 solver.cpp:244]     Train net output #0: loss = 10380.9 (* 1 = 10380.9 loss)
I0422 10:53:46.524299  2460 sgd_solver.cpp:106] Iteration 8760, lr = 1e-10
I0422 10:54:16.496614  2460 solver.cpp:228] Iteration 8780, loss = 16372.9
I0422 10:54:16.496712  2460 solver.cpp:244]     Train net output #0: loss = 13112.5 (* 1 = 13112.5 loss)
I0422 10:54:16.496724  2460 sgd_solver.cpp:106] Iteration 8780, lr = 1e-10
I0422 10:54:54.197494  2460 solver.cpp:228] Iteration 8800, loss = 15109.3
I0422 10:54:54.197582  2460 solver.cpp:244]     Train net output #0: loss = 15109.3 (* 1 = 15109.3 loss)
I0422 10:54:54.197602  2460 sgd_solver.cpp:106] Iteration 8800, lr = 1e-10
I0422 10:55:24.165127  2460 solver.cpp:228] Iteration 8820, loss = 9841.99
I0422 10:55:25.407841  2460 solver.cpp:244]     Train net output #0: loss = 10611.5 (* 1 = 10611.5 loss)
I0422 10:55:25.407882  2460 sgd_solver.cpp:106] Iteration 8820, lr = 1e-10
I0422 10:55:54.413957  2460 solver.cpp:228] Iteration 8840, loss = 9995.79
I0422 10:55:54.414052  2460 solver.cpp:244]     Train net output #0: loss = 15920.3 (* 1 = 15920.3 loss)
I0422 10:55:54.414065  2460 sgd_solver.cpp:106] Iteration 8840, lr = 1e-10
I0422 10:56:24.388769  2460 solver.cpp:228] Iteration 8860, loss = 12952
I0422 10:56:24.388859  2460 solver.cpp:244]     Train net output #0: loss = 37383.9 (* 1 = 37383.9 loss)
I0422 10:56:24.388871  2460 sgd_solver.cpp:106] Iteration 8860, lr = 1e-10
I0422 10:56:54.374557  2460 solver.cpp:228] Iteration 8880, loss = 9073.63
I0422 10:56:54.374644  2460 solver.cpp:244]     Train net output #0: loss = 14748 (* 1 = 14748 loss)
I0422 10:56:54.374665  2460 sgd_solver.cpp:106] Iteration 8880, lr = 1e-10
I0422 10:57:32.079383  2460 solver.cpp:228] Iteration 8900, loss = 8227.37
I0422 10:57:32.079481  2460 solver.cpp:244]     Train net output #0: loss = 8227.37 (* 1 = 8227.37 loss)
I0422 10:57:32.079495  2460 sgd_solver.cpp:106] Iteration 8900, lr = 1e-10
I0422 10:58:02.058687  2460 solver.cpp:228] Iteration 8920, loss = 11557.8
I0422 10:58:02.058785  2460 solver.cpp:244]     Train net output #0: loss = 8699.37 (* 1 = 8699.37 loss)
I0422 10:58:02.058806  2460 sgd_solver.cpp:106] Iteration 8920, lr = 1e-10
I0422 10:58:32.029299  2460 solver.cpp:228] Iteration 8940, loss = 9301.5
I0422 10:58:32.029405  2460 solver.cpp:244]     Train net output #0: loss = 5226.84 (* 1 = 5226.84 loss)
I0422 10:58:32.029417  2460 sgd_solver.cpp:106] Iteration 8940, lr = 1e-10
I0422 10:59:02.003123  2460 solver.cpp:228] Iteration 8960, loss = 11743.4
I0422 10:59:02.003212  2460 solver.cpp:244]     Train net output #0: loss = 12271.8 (* 1 = 12271.8 loss)
I0422 10:59:02.003235  2460 sgd_solver.cpp:106] Iteration 8960, lr = 1e-10
I0422 10:59:31.987066  2460 solver.cpp:228] Iteration 8980, loss = 10212.7
I0422 10:59:31.987161  2460 solver.cpp:244]     Train net output #0: loss = 7220.59 (* 1 = 7220.59 loss)
I0422 10:59:31.987174  2460 sgd_solver.cpp:106] Iteration 8980, lr = 1e-10
I0422 11:00:00.466891  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_9000.caffemodel
I0422 11:00:03.794253  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_9000.solverstate
I0422 11:00:12.978873  2460 solver.cpp:228] Iteration 9000, loss = 12657.1
I0422 11:00:12.978971  2460 solver.cpp:244]     Train net output #0: loss = 12657.1 (* 1 = 12657.1 loss)
I0422 11:00:12.978986  2460 sgd_solver.cpp:106] Iteration 9000, lr = 1e-10
I0422 11:00:42.955888  2460 solver.cpp:228] Iteration 9020, loss = 11563.8
I0422 11:00:42.955996  2460 solver.cpp:244]     Train net output #0: loss = 6712.91 (* 1 = 6712.91 loss)
I0422 11:00:42.956012  2460 sgd_solver.cpp:106] Iteration 9020, lr = 1e-10
I0422 11:01:12.925633  2460 solver.cpp:228] Iteration 9040, loss = 10954.4
I0422 11:01:12.925725  2460 solver.cpp:244]     Train net output #0: loss = 7605.05 (* 1 = 7605.05 loss)
I0422 11:01:12.925739  2460 sgd_solver.cpp:106] Iteration 9040, lr = 1e-10
I0422 11:01:42.905728  2460 solver.cpp:228] Iteration 9060, loss = 10152.2
I0422 11:01:42.905817  2460 solver.cpp:244]     Train net output #0: loss = 12778.1 (* 1 = 12778.1 loss)
I0422 11:01:42.905830  2460 sgd_solver.cpp:106] Iteration 9060, lr = 1e-10
I0422 11:02:12.890406  2460 solver.cpp:228] Iteration 9080, loss = 12325.2
I0422 11:02:12.890497  2460 solver.cpp:244]     Train net output #0: loss = 55190.5 (* 1 = 55190.5 loss)
I0422 11:02:12.890511  2460 sgd_solver.cpp:106] Iteration 9080, lr = 1e-10
I0422 11:02:50.573416  2460 solver.cpp:228] Iteration 9100, loss = 5646.21
I0422 11:02:50.573513  2460 solver.cpp:244]     Train net output #0: loss = 5646.21 (* 1 = 5646.21 loss)
I0422 11:02:50.573529  2460 sgd_solver.cpp:106] Iteration 9100, lr = 1e-10
I0422 11:03:20.550850  2460 solver.cpp:228] Iteration 9120, loss = 14266.6
I0422 11:03:20.550938  2460 solver.cpp:244]     Train net output #0: loss = 14184.2 (* 1 = 14184.2 loss)
I0422 11:03:20.550951  2460 sgd_solver.cpp:106] Iteration 9120, lr = 1e-10
I0422 11:03:50.522487  2460 solver.cpp:228] Iteration 9140, loss = 12373.3
I0422 11:03:50.522577  2460 solver.cpp:244]     Train net output #0: loss = 14382.1 (* 1 = 14382.1 loss)
I0422 11:03:50.522598  2460 sgd_solver.cpp:106] Iteration 9140, lr = 1e-10
I0422 11:04:20.501646  2460 solver.cpp:228] Iteration 9160, loss = 9916.8
I0422 11:04:20.501749  2460 solver.cpp:244]     Train net output #0: loss = 6629.46 (* 1 = 6629.46 loss)
I0422 11:04:20.501767  2460 sgd_solver.cpp:106] Iteration 9160, lr = 1e-10
I0422 11:04:50.485821  2460 solver.cpp:228] Iteration 9180, loss = 11544.5
I0422 11:04:50.485905  2460 solver.cpp:244]     Train net output #0: loss = 5420.17 (* 1 = 5420.17 loss)
I0422 11:04:50.485918  2460 sgd_solver.cpp:106] Iteration 9180, lr = 1e-10
269719 Iteration 8200 mean IU 0.952443150925
>>> 2017-04-22 10:39:06.269912 Iteration 8200 fwavacc 0.95271997854
>>> 2017-04-22 10:41:36.812761 Begin seg tests
>>> 2017-04-22 10:41:43.900824 Iteration 8300 loss 11307.5975342
>>> 2017-04-22 10:41:43.900922 Iteration 8300 overall accuracy 0.975655555556
>>> 2017-04-22 10:41:43.900981 Iteration 8300 mean accuracy 0.975641833849
>>> 2017-04-22 10:41:43.901186 Iteration 8300 mean IU 0.951485688064
>>> 2017-04-22 10:41:43.901291 Iteration 8300 fwavacc 0.952516642801
>>> 2017-04-22 10:44:14.464181 Begin seg tests
>>> 2017-04-22 10:44:21.546078 Iteration 8400 loss 13499.3844808
>>> 2017-04-22 10:44:21.546176 Iteration 8400 overall accuracy 0.970797333333
>>> 2017-04-22 10:44:21.546240 Iteration 8400 mean accuracy 0.970803074293
>>> 2017-04-22 10:44:21.546468 Iteration 8400 mean IU 0.94324036492
>>> 2017-04-22 10:44:21.546579 Iteration 8400 fwavacc 0.943240056133
>>> 2017-04-22 10:46:52.082257 Begin seg tests
>>> 2017-04-22 10:46:59.172770 Iteration 8500 loss 13030.0571696
>>> 2017-04-22 10:46:59.172876 Iteration 8500 overall accuracy 0.971951555556
>>> 2017-04-22 10:46:59.172935 Iteration 8500 mean accuracy 0.971772115255
>>> 2017-04-22 10:46:59.173142 Iteration 8500 mean IU 0.945039585333
>>> 2017-04-22 10:46:59.173246 Iteration 8500 fwavacc 0.945446607349
>>> 2017-04-22 10:49:29.731191 Begin seg tests
>>> 2017-04-22 10:49:36.820184 Iteration 8600 loss 11543.8108317
>>> 2017-04-22 10:49:36.820291 Iteration 8600 overall accuracy 0.975596444444
>>> 2017-04-22 10:49:36.820357 Iteration 8600 mean accuracy 0.976068670313
>>> 2017-04-22 10:49:36.820569 Iteration 8600 mean IU 0.952238807305
>>> 2017-04-22 10:49:36.820674 Iteration 8600 fwavacc 0.952380419121
>>> 2017-04-22 10:52:07.369244 Begin seg tests
>>> 2017-04-22 10:52:14.441335 Iteration 8700 loss 12728.843099
>>> 2017-04-22 10:52:14.441441 Iteration 8700 overall accuracy 0.973751111111
>>> 2017-04-22 10:52:14.441498 Iteration 8700 mean accuracy 0.974511755231
>>> 2017-04-22 10:52:14.441718 Iteration 8700 mean IU 0.947234766765
>>> 2017-04-22 10:52:14.441826 Iteration 8700 fwavacc 0.94896871786
>>> 2017-04-22 10:54:44.975750 Begin seg tests
>>> 2017-04-22 10:54:52.054446 Iteration 8800 loss 13492.4733887
>>> 2017-04-22 10:54:52.054552 Iteration 8800 overall accuracy 0.971485777778
>>> 2017-04-22 10:54:52.054611 Iteration 8800 mean accuracy 0.971543913334
>>> 2017-04-22 10:54:52.054838 Iteration 8800 mean IU 0.944543069709
>>> 2017-04-22 10:54:52.054966 Iteration 8800 fwavacc 0.944540559791
>>> 2017-04-22 10:57:22.858083 Begin seg tests
>>> 2017-04-22 10:57:29.945076 Iteration 8900 loss 11368.7995605
>>> 2017-04-22 10:57:29.945171 Iteration 8900 overall accuracy 0.976442666667
>>> 2017-04-22 10:57:29.945230 Iteration 8900 mean accuracy 0.976650152527
>>> 2017-04-22 10:57:29.945452 Iteration 8900 mean IU 0.953441555299
>>> 2017-04-22 10:57:29.945556 Iteration 8900 fwavacc 0.954004332378
>>> 2017-04-22 11:00:04.762667 Begin seg tests
>>> 2017-04-22 11:00:10.847995 Iteration 9000 loss 12339.3552246
>>> 2017-04-22 11:00:10.848092 Iteration 9000 overall accuracy 0.975351555556
>>> 2017-04-22 11:00:10.848151 Iteration 9000 mean accuracy 0.9751417053
>>> 2017-04-22 11:00:10.848376 Iteration 9000 mean IU 0.951804752512
>>> 2017-04-22 11:00:10.848498 Iteration 9000 fwavacc 0.95188193089
>>> 2017-04-22 11:02:41.367747 Begin seg tests
>>> 2017-04-22 11:02:48.450599 Iteration 9100 loss 9266.23596191
>>> 2017-04-22 11:02:48.450704 Iteration 9100 overall accuracy 0.978996444444
>>> 2017-04-22 11:02:48.450761 Iteration 9100 mean accuracy 0.978170148736
>>> 2017-04-22 11:02:48.450970 Iteration 9100 mean IU 0.958554714308
>>> 2017-04-22 11:02:48.451082 Iteration 9100 fwavacc 0.958832120986
>>> 2017-04-22 11:05:18.957668 Begin seg tests
>>> 2017-04-22 11:05:26.040765 Iteration 9200 loss 9768.03336589
>>> 2017-04-22 11:05:26.040865 Iteration 9200 overall accuracy 0.978552444444
>>> 2017-04-22 11:05:26.040924 Iteration 9200 mean accuracy 0.978407325143
>>> 2017-04-22 11:05:26.041127 Iteration 9200 mean IU 0.957519462245
>>> 2017-04-22 11:05:26.0412I0422 11:05:28.191931  2460 solver.cpp:228] Iteration 9200, loss = 12976.6
I0422 11:05:28.192023  2460 solver.cpp:244]     Train net output #0: loss = 12976.6 (* 1 = 12976.6 loss)
I0422 11:05:28.192035  2460 sgd_solver.cpp:106] Iteration 9200, lr = 1e-10
I0422 11:05:58.161401  2460 solver.cpp:228] Iteration 9220, loss = 10029.9
I0422 11:05:58.161486  2460 solver.cpp:244]     Train net output #0: loss = 8833.53 (* 1 = 8833.53 loss)
I0422 11:05:58.161499  2460 sgd_solver.cpp:106] Iteration 9220, lr = 1e-10
I0422 11:06:28.144911  2460 solver.cpp:228] Iteration 9240, loss = 11181
I0422 11:06:28.145001  2460 solver.cpp:244]     Train net output #0: loss = 6716.93 (* 1 = 6716.93 loss)
I0422 11:06:28.145015  2460 sgd_solver.cpp:106] Iteration 9240, lr = 1e-10
I0422 11:06:58.129878  2460 solver.cpp:228] Iteration 9260, loss = 14633.1
I0422 11:06:58.129967  2460 solver.cpp:244]     Train net output #0: loss = 9854.43 (* 1 = 9854.43 loss)
I0422 11:06:58.129981  2460 sgd_solver.cpp:106] Iteration 9260, lr = 1e-10
I0422 11:07:28.103579  2460 solver.cpp:228] Iteration 9280, loss = 11583.6
I0422 11:07:28.103698  2460 solver.cpp:244]     Train net output #0: loss = 7512.21 (* 1 = 7512.21 loss)
I0422 11:07:28.103710  2460 sgd_solver.cpp:106] Iteration 9280, lr = 1e-10
I0422 11:08:05.797559  2460 solver.cpp:228] Iteration 9300, loss = 9057.01
I0422 11:08:05.797649  2460 solver.cpp:244]     Train net output #0: loss = 9057.01 (* 1 = 9057.01 loss)
I0422 11:08:05.797662  2460 sgd_solver.cpp:106] Iteration 9300, lr = 1e-10
I0422 11:08:35.779824  2460 solver.cpp:228] Iteration 9320, loss = 11851
I0422 11:08:35.779908  2460 solver.cpp:244]     Train net output #0: loss = 9060.82 (* 1 = 9060.82 loss)
I0422 11:08:35.779922  2460 sgd_solver.cpp:106] Iteration 9320, lr = 1e-10
I0422 11:09:05.751648  2460 solver.cpp:228] Iteration 9340, loss = 11809.7
I0422 11:09:05.751736  2460 solver.cpp:244]     Train net output #0: loss = 11016.9 (* 1 = 11016.9 loss)
I0422 11:09:05.751749  2460 sgd_solver.cpp:106] Iteration 9340, lr = 1e-10
I0422 11:09:35.724362  2460 solver.cpp:228] Iteration 9360, loss = 12048.6
I0422 11:09:35.724454  2460 solver.cpp:244]     Train net output #0: loss = 12648.2 (* 1 = 12648.2 loss)
I0422 11:09:35.724467  2460 sgd_solver.cpp:106] Iteration 9360, lr = 1e-10
I0422 11:10:05.688463  2460 solver.cpp:228] Iteration 9380, loss = 9731.56
I0422 11:10:05.688549  2460 solver.cpp:244]     Train net output #0: loss = 9414.84 (* 1 = 9414.84 loss)
I0422 11:10:05.688561  2460 sgd_solver.cpp:106] Iteration 9380, lr = 1e-10
I0422 11:10:43.350360  2460 solver.cpp:228] Iteration 9400, loss = 6593.2
I0422 11:10:43.350438  2460 solver.cpp:244]     Train net output #0: loss = 6593.2 (* 1 = 6593.2 loss)
I0422 11:10:43.350450  2460 sgd_solver.cpp:106] Iteration 9400, lr = 1e-10
I0422 11:11:13.326637  2460 solver.cpp:228] Iteration 9420, loss = 13487
I0422 11:11:13.326738  2460 solver.cpp:244]     Train net output #0: loss = 7796.17 (* 1 = 7796.17 loss)
I0422 11:11:13.326751  2460 sgd_solver.cpp:106] Iteration 9420, lr = 1e-10
I0422 11:11:43.308673  2460 solver.cpp:228] Iteration 9440, loss = 12858.5
I0422 11:11:43.308763  2460 solver.cpp:244]     Train net output #0: loss = 7928.98 (* 1 = 7928.98 loss)
I0422 11:11:43.308775  2460 sgd_solver.cpp:106] Iteration 9440, lr = 1e-10
I0422 11:12:13.289960  2460 solver.cpp:228] Iteration 9460, loss = 13593.8
I0422 11:12:13.290050  2460 solver.cpp:244]     Train net output #0: loss = 15286.9 (* 1 = 15286.9 loss)
I0422 11:12:13.290062  2460 sgd_solver.cpp:106] Iteration 9460, lr = 1e-10
I0422 11:12:43.259817  2460 solver.cpp:228] Iteration 9480, loss = 11581
I0422 11:12:43.259907  2460 solver.cpp:244]     Train net output #0: loss = 8944.76 (* 1 = 8944.76 loss)
I0422 11:12:43.259918  2460 sgd_solver.cpp:106] Iteration 9480, lr = 1e-10
I0422 11:13:20.942587  2460 solver.cpp:228] Iteration 9500, loss = 7935.98
I0422 11:13:20.942674  2460 solver.cpp:244]     Train net output #0: loss = 7935.98 (* 1 = 7935.98 loss)
I0422 11:13:20.942687  2460 sgd_solver.cpp:106] Iteration 9500, lr = 1e-10
I0422 11:13:50.921345  2460 solver.cpp:228] Iteration 9520, loss = 10750
I0422 11:13:50.921432  2460 solver.cpp:244]     Train net output #0: loss = 16831.4 (* 1 = 16831.4 loss)
I0422 11:13:50.921445  2460 sgd_solver.cpp:106] Iteration 9520, lr = 1e-10
I0422 11:14:20.901448  2460 solver.cpp:228] Iteration 9540, loss = 12438.4
I0422 11:14:20.901533  2460 solver.cpp:244]     Train net output #0: loss = 9371.98 (* 1 = 9371.98 loss)
I0422 11:14:20.901547  2460 sgd_solver.cpp:106] Iteration 9540, lr = 1e-10
I0422 11:14:50.881587  2460 solver.cpp:228] Iteration 9560, loss = 14082.8
I0422 11:14:50.881674  2460 solver.cpp:244]     Train net output #0: loss = 4866.13 (* 1 = 4866.13 loss)
I0422 11:14:50.881686  2460 sgd_solver.cpp:106] Iteration 9560, lr = 1e-10
I0422 11:15:20.858180  2460 solver.cpp:228] Iteration 9580, loss = 10494.8
I0422 11:15:20.858259  2460 solver.cpp:244]     Train net output #0: loss = 9482.47 (* 1 = 9482.47 loss)
I0422 11:15:20.858271  2460 sgd_solver.cpp:106] Iteration 9580, lr = 1e-10
I0422 11:15:58.606453  2460 solver.cpp:228] Iteration 9600, loss = 10126.7
I0422 11:15:58.606539  2460 solver.cpp:244]     Train net output #0: loss = 10126.7 (* 1 = 10126.7 loss)
I0422 11:15:58.606552  2460 sgd_solver.cpp:106] Iteration 9600, lr = 1e-10
I0422 11:16:28.584813  2460 solver.cpp:228] Iteration 9620, loss = 11944.3
I0422 11:16:28.584897  2460 solver.cpp:244]     Train net output #0: loss = 12723 (* 1 = 12723 loss)
I0422 11:16:28.584909  2460 sgd_solver.cpp:106] Iteration 9620, lr = 1e-10
I0422 11:16:58.559999  2460 solver.cpp:228] Iteration 9640, loss = 12411.5
I0422 11:16:58.560098  2460 solver.cpp:244]     Train net output #0: loss = 9576.67 (* 1 = 9576.67 loss)
I0422 11:16:58.560111  2460 sgd_solver.cpp:106] Iteration 9640, lr = 1e-10
I0422 11:17:28.535356  2460 solver.cpp:228] Iteration 9660, loss = 10622.2
I0422 11:17:28.535449  2460 solver.cpp:244]     Train net output #0: loss = 22322.6 (* 1 = 22322.6 loss)
I0422 11:17:28.535464  2460 sgd_solver.cpp:106] Iteration 9660, lr = 1e-10
I0422 11:17:58.511359  2460 solver.cpp:228] Iteration 9680, loss = 8655.87
I0422 11:17:58.511461  2460 solver.cpp:244]     Train net output #0: loss = 10217.4 (* 1 = 10217.4 loss)
I0422 11:17:58.511474  2460 sgd_solver.cpp:106] Iteration 9680, lr = 1e-10
I0422 11:18:36.225008  2460 solver.cpp:228] Iteration 9700, loss = 15148.2
I0422 11:18:36.225095  2460 solver.cpp:244]     Train net output #0: loss = 15148.2 (* 1 = 15148.2 loss)
I0422 11:18:36.225108  2460 sgd_solver.cpp:106] Iteration 9700, lr = 1e-10
I0422 11:19:06.209637  2460 solver.cpp:228] Iteration 9720, loss = 10508.7
I0422 11:19:06.209724  2460 solver.cpp:244]     Train net output #0: loss = 8359.44 (* 1 = 8359.44 loss)
I0422 11:19:06.209746  2460 sgd_solver.cpp:106] Iteration 9720, lr = 1e-10
I0422 11:19:36.182727  2460 solver.cpp:228] Iteration 9740, loss = 10767.8
I0422 11:19:36.182816  2460 solver.cpp:244]     Train net output #0: loss = 24768.1 (* 1 = 24768.1 loss)
I0422 11:19:36.182837  2460 sgd_solver.cpp:106] Iteration 9740, lr = 1e-10
I0422 11:20:06.160382  2460 solver.cpp:228] Iteration 9760, loss = 13224
I0422 11:20:06.160478  2460 solver.cpp:244]     Train net output #0: loss = 4271.08 (* 1 = 4271.08 loss)
I0422 11:20:06.160490  2460 sgd_solver.cpp:106] Iteration 9760, lr = 1e-10
I0422 11:20:36.139472  2460 solver.cpp:228] Iteration 9780, loss = 12000.9
I0422 11:20:36.139559  2460 solver.cpp:244]     Train net output #0: loss = 10172 (* 1 = 10172 loss)
I0422 11:20:36.139571  2460 sgd_solver.cpp:106] Iteration 9780, lr = 1e-10
I0422 11:21:13.887272  2460 solver.cpp:228] Iteration 9800, loss = 10943.6
I0422 11:21:13.887356  2460 solver.cpp:244]     Train net output #0: loss = 10943.6 (* 1 = 10943.6 loss)
I0422 11:21:13.887369  2460 sgd_solver.cpp:106] Iteration 9800, lr = 1e-10
I0422 11:21:43.870193  2460 solver.cpp:228] Iteration 9820, loss = 10967.1
I0422 11:21:43.870287  2460 solver.cpp:244]     Train net output #0: loss = 10983.7 (* 1 = 10983.7 loss)
I0422 11:21:43.870301  2460 sgd_solver.cpp:106] Iteration 9820, lr = 1e-10
I0422 11:22:13.853094  2460 solver.cpp:228] Iteration 9840, loss = 11062.2
I0422 11:22:13.853188  2460 solver.cpp:244]     Train net output #0: loss = 20264.4 (* 1 = 20264.4 loss)
I0422 11:22:13.853200  2460 sgd_solver.cpp:106] Iteration 9840, lr = 1e-10
I0422 11:22:43.845237  2460 solver.cpp:228] Iteration 9860, loss = 10379.8
I0422 11:22:43.845320  2460 solver.cpp:244]     Train net output #0: loss = 6858.39 (* 1 = 6858.39 loss)
I0422 11:22:43.845333  2460 sgd_solver.cpp:106] Iteration 9860, lr = 1e-10
I0422 11:23:13.831707  2460 solver.cpp:228] Iteration 9880, loss = 11930.3
I0422 11:23:13.831810  2460 solver.cpp:244]     Train net output #0: loss = 23254.4 (* 1 = 23254.4 loss)
I0422 11:23:13.831825  2460 sgd_solver.cpp:106] Iteration 9880, lr = 1e-10
I0422 11:23:51.583232  2460 solver.cpp:228] Iteration 9900, loss = 8799.87
I0422 11:23:51.583326  2460 solver.cpp:244]     Train net output #0: loss = 8799.87 (* 1 = 8799.87 loss)
I0422 11:23:51.583338  2460 sgd_solver.cpp:106] Iteration 9900, lr = 1e-10
I0422 11:24:21.552484  2460 solver.cpp:228] Iteration 9920, loss = 11054.2
I0422 11:24:21.552573  2460 solver.cpp:244]     Train net output #0: loss = 15473.9 (* 1 = 15473.9 loss)
I0422 11:24:21.552595  2460 sgd_solver.cpp:106] Iteration 9920, lr = 1e-10
I0422 11:24:51.534149  2460 solver.cpp:228] Iteration 9940, loss = 9690.85
I0422 11:24:51.534237  2460 solver.cpp:244]     Train net output #0: loss = 5101.54 (* 1 = 5101.54 loss)
I0422 11:24:51.534250  2460 sgd_solver.cpp:106] Iteration 9940, lr = 1e-10
I0422 11:25:21.513614  2460 solver.cpp:228] Iteration 9960, loss = 11659.8
I0422 11:25:21.513696  2460 solver.cpp:244]     Train net output #0: loss = 9911.75 (* 1 = 9911.75 loss)
I0422 11:25:21.513708  2460 sgd_solver.cpp:106] Iteration 9960, lr = 1e-10
I0422 11:25:51.500859  2460 solver.cpp:228] Iteration 9980, loss = 10629.4
I0422 11:25:51.500952  2460 solver.cpp:244]     Train net output #0: loss = 16383.8 (* 1 = 16383.8 loss)
I0422 11:25:51.500964  2460 sgd_solver.cpp:106] Iteration 9980, lr = 1e-10
I0422 11:26:19.988459  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_10000.caffemodel
I0422 11:26:23.326596  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_10000.solverstate
I0422 11:26:32.564961  2460 solver.cpp:228] Iteration 10000, loss = 13480.5
I0422 11:26:32.565094  2460 solver.cpp:244]     Train net output #0: loss = 13480.5 (* 1 = 13480.5 loss)
I0422 11:26:32.565124  2460 sgd_solver.cpp:106] Iteration 10000, lr = 1e-10
I0422 11:27:02.540666  2460 solver.cpp:228] Iteration 10020, loss = 12525.1
I0422 11:27:02.540756  2460 solver.cpp:244]     Train net output #0: loss = 6757.2 (* 1 = 6757.2 loss)
I0422 11:27:02.540781  2460 sgd_solver.cpp:106] Iteration 10020, lr = 1e-10
I0422 11:27:32.518685  2460 solver.cpp:228] Iteration 10040, loss = 10800.7
I0422 11:27:32.518779  2460 solver.cpp:244]     Train net output #0: loss = 10071.3 (* 1 = 10071.3 loss)
I0422 11:27:32.518791  2460 sgd_solver.cpp:106] Iteration 10040, lr = 1e-10
I0422 11:28:02.500856  2460 solver.cpp:228] Iteration 10060, loss = 11760.4
I0422 11:28:02.500939  2460 solver.cpp:244]     Train net output #0: loss = 15892.3 (* 1 = 15892.3 loss)
I0422 11:28:02.500952  2460 sgd_solver.cpp:106] Iteration 10060, lr = 1e-10
I0422 11:28:32.470173  2460 solver.cpp:228] Iteration 10080, loss = 13189.7
I0422 11:28:32.470264  2460 solver.cpp:244]     Train net output #0: loss = 40036.1 (* 1 = 40036.1 loss)
I0422 11:28:32.470278  2460 sgd_solver.cpp:106] Iteration 10080, lr = 1e-10
I0422 11:29:10.171552  2460 solver.cpp:228] Iteration 10100, loss = 10491.9
I0422 11:29:10.171655  2460 solver.cpp:244]     Train net output #0: loss = 10491.9 (* 1 = 10491.9 loss)
I0422 11:29:10.171676  2460 sgd_solver.cpp:106] Iteration 10100, lr = 1e-10
I0422 11:29:40.147559  2460 solver.cpp:228] Iteration 10120, loss = 10179.3
I0422 11:29:40.147698  2460 solver.cpp:244]     Train net output #0: loss = 23055.2 (* 1 = 23055.2 loss)
I0422 11:29:40.147727  2460 sgd_solver.cpp:106] Iteration 10120, lr = 1e-10
I0422 11:30:10.121455  2460 solver.cpp:228] Iteration 10140, loss = 12015.2
I0422 11:30:10.121542  2460 solver.cpp:244]     Train net output #0: loss = 17795.3 (* 1 = 17795.3 loss)
I0422 11:30:10.121556  2460 sgd_solver.cpp:106] Iteration 10140, lr = 1e-10
I0422 11:30:40.093670  2460 solver.cpp:228] Iteration 10160, loss = 8734.83
I0422 11:30:40.093765  2460 solver.cpp:244]     Train net output #0: loss = 6200.69 (* 1 = 6200.69 loss)
I0422 11:30:40.093777  2460 sgd_solver.cpp:106] Iteration 10160, lr = 1e-10
I0422 11:31:10.063436  2460 solver.cpp:228] Iteration 10180, loss = 10202.6
I0422 11:31:10.063530  2460 solver.cpp:244]     Train net output #0: loss = 10830 (* 1 = 10830 loss)
I0422 11:31:10.063551  2460 sgd_solver.cpp:106] Iteration 10180, lr = 1e-10
I0422 11:31:47.773164  2460 solver.cpp:228] Iteration 10200, loss = 10070.7
I0422 11:31:47.773254  2460 solver.cpp:244]     Train net output #0: loss = 10070.7 (* 1 = 10070.7 loss)
I0422 11:31:47.773268  2460 sgd_solver.cpp:106] Iteration 10200, lr = 1e-10
I0422 11:32:17.752596  2460 solver.cpp:228] Iteration 10220, loss = 12743.6
I0422 11:32:17.752683  2460 solver.cpp:244]     Train net output #0: loss = 8519.93 (* 1 = 8519.93 loss)
I0422 11:32:17.752696  2460 sgd_solver.cpp:106] Iteration 10220, lr = 1e-10
I0422 11:32:47.722393  2460 solver.cpp:228] Iteration 10240, loss = 10600.3
I0422 11:32:47.722481  2460 solver.cpp:244]     Train net output #0: loss = 9507.54 (* 1 = 9507.54 loss)
I0422 11:32:47.722494  2460 sgd_solver.cpp:106] Iteration 10240, lr = 1e-10
I0422 11:33:17.711159  2460 solver.cpp:228] Iteration 10260, loss = 11365.9
I0422 11:33:17.711248  2460 solver.cpp:244]     Train net output #0: loss = 5854.79 (* 1 = 5854.79 loss)
I0422 11:33:17.711261  2460 sgd_solver.cpp:106] Iteration 10260, lr = 1e-10
I0422 11:33:47.697021  2460 solver.cpp:228] Iteration 10280, loss = 9955.61
I0422 11:33:47.697108  2460 solver.cpp:244]     Train net output #0: loss = 16983.3 (* 1 = 16983.3 loss)
I0422 11:33:47.697119  2460 sgd_solver.cpp:106] Iteration 10280, lr = 1e-10
32 Iteration 9200 fwavacc 0.958020841776
>>> 2017-04-22 11:07:56.589051 Begin seg tests
>>> 2017-04-22 11:08:03.678138 Iteration 9300 loss 11037.4134521
>>> 2017-04-22 11:08:03.678235 Iteration 9300 overall accuracy 0.976384888889
>>> 2017-04-22 11:08:03.678297 Iteration 9300 mean accuracy 0.976185185687
>>> 2017-04-22 11:08:03.678523 Iteration 9300 mean IU 0.952833889603
>>> 2017-04-22 11:08:03.678633 Iteration 9300 fwavacc 0.953899937458
>>> 2017-04-22 11:10:34.169008 Begin seg tests
>>> 2017-04-22 11:10:41.251037 Iteration 9400 loss 10623.3791097
>>> 2017-04-22 11:10:41.251148 Iteration 9400 overall accuracy 0.977940444444
>>> 2017-04-22 11:10:41.251205 Iteration 9400 mean accuracy 0.977862977729
>>> 2017-04-22 11:10:41.251427 Iteration 9400 mean IU 0.956820097931
>>> 2017-04-22 11:10:41.251532 Iteration 9400 fwavacc 0.956825564526
>>> 2017-04-22 11:13:11.744069 Begin seg tests
>>> 2017-04-22 11:13:18.834443 Iteration 9500 loss 9857.12137858
>>> 2017-04-22 11:13:18.834557 Iteration 9500 overall accuracy 0.979556
>>> 2017-04-22 11:13:18.834638 Iteration 9500 mean accuracy 0.978395614834
>>> 2017-04-22 11:13:18.834870 Iteration 9500 mean IU 0.959496859728
>>> 2017-04-22 11:13:18.834983 Iteration 9500 fwavacc 0.959897797221
>>> 2017-04-22 11:15:49.344800 Begin seg tests
>>> 2017-04-22 11:15:56.427873 Iteration 9600 loss 8537.69462077
>>> 2017-04-22 11:15:56.427977 Iteration 9600 overall accuracy 0.982241333333
>>> 2017-04-22 11:15:56.428036 Iteration 9600 mean accuracy 0.982197316074
>>> 2017-04-22 11:15:56.428236 Iteration 9600 mean IU 0.965051630958
>>> 2017-04-22 11:15:56.428340 Iteration 9600 fwavacc 0.965102682073
>>> 2017-04-22 11:18:26.985733 Begin seg tests
>>> 2017-04-22 11:18:34.069226 Iteration 9700 loss 9152.77298991
>>> 2017-04-22 11:18:34.069333 Iteration 9700 overall accuracy 0.980345333333
>>> 2017-04-22 11:18:34.069404 Iteration 9700 mean accuracy 0.980323774833
>>> 2017-04-22 11:18:34.069637 Iteration 9700 mean IU 0.961445176049
>>> 2017-04-22 11:18:34.069743 Iteration 9700 fwavacc 0.961446590372
>>> 2017-04-22 11:21:04.615220 Begin seg tests
>>> 2017-04-22 11:21:11.697893 Iteration 9800 loss 9946.93351237
>>> 2017-04-22 11:21:11.698007 Iteration 9800 overall accuracy 0.978478666667
>>> 2017-04-22 11:21:11.698075 Iteration 9800 mean accuracy 0.978823918559
>>> 2017-04-22 11:21:11.698306 Iteration 9800 mean IU 0.95764843155
>>> 2017-04-22 11:21:11.698422 Iteration 9800 fwavacc 0.957887066865
>>> 2017-04-22 11:23:42.321899 Begin seg tests
>>> 2017-04-22 11:23:49.409536 Iteration 9900 loss 11187.8027344
>>> 2017-04-22 11:23:49.409646 Iteration 9900 overall accuracy 0.976673777778
>>> 2017-04-22 11:23:49.409719 Iteration 9900 mean accuracy 0.977458865765
>>> 2017-04-22 11:23:49.409925 Iteration 9900 mean IU 0.953112013396
>>> 2017-04-22 11:23:49.410035 Iteration 9900 fwavacc 0.954506997012
>>> 2017-04-22 11:26:24.318407 Begin seg tests
>>> 2017-04-22 11:26:30.405963 Iteration 10000 loss 10893.9044189
>>> 2017-04-22 11:26:30.406060 Iteration 10000 overall accuracy 0.975967555556
>>> 2017-04-22 11:26:30.406118 Iteration 10000 mean accuracy 0.975143888618
>>> 2017-04-22 11:26:30.406328 Iteration 10000 mean IU 0.95228397961
>>> 2017-04-22 11:26:30.406432 Iteration 10000 fwavacc 0.953061809248
>>> 2017-04-22 11:29:00.945585 Begin seg tests
>>> 2017-04-22 11:29:08.035247 Iteration 10100 loss 12707.6444499
>>> 2017-04-22 11:29:08.035345 Iteration 10100 overall accuracy 0.972459111111
>>> 2017-04-22 11:29:08.035404 Iteration 10100 mean accuracy 0.972499331716
>>> 2017-04-22 11:29:08.035607 Iteration 10100 mean IU 0.946393634507
>>> 2017-04-22 11:29:08.035734 Iteration 10100 fwavacc 0.946392608521
>>> 2017-04-22 11:31:38.551093 Begin seg tests
>>> 2017-04-22 11:31:45.633480 Iteration 10200 loss 10660.4256185
>>> 2017-04-22 11:31:45.633579 Iteration 10200 overall accuracy 0.976069777778
>>> 2017-04-22 11:31:45.633638 Iteration 10200 mean accuracy 0.975170501935
>>> 2017-04-22 11:31:45.633877 Iteration 10200 mean IU 0.952692605779
>>> 2017-04-22 11:31:45.633983 Iteration 10200 fwavacc 0.953241423847
>>> 2017-04-22 11:I0422 11:34:25.407327  2460 solver.cpp:228] Iteration 10300, loss = 12048.2
I0422 11:34:25.407413  2460 solver.cpp:244]     Train net output #0: loss = 12048.2 (* 1 = 12048.2 loss)
I0422 11:34:25.407424  2460 sgd_solver.cpp:106] Iteration 10300, lr = 1e-10
I0422 11:34:55.379717  2460 solver.cpp:228] Iteration 10320, loss = 13142.6
I0422 11:34:55.379813  2460 solver.cpp:244]     Train net output #0: loss = 7927.17 (* 1 = 7927.17 loss)
I0422 11:34:55.384764  2460 sgd_solver.cpp:106] Iteration 10320, lr = 1e-10
I0422 11:35:25.355460  2460 solver.cpp:228] Iteration 10340, loss = 11504
I0422 11:35:25.355556  2460 solver.cpp:244]     Train net output #0: loss = 8647.24 (* 1 = 8647.24 loss)
I0422 11:35:25.355568  2460 sgd_solver.cpp:106] Iteration 10340, lr = 1e-10
I0422 11:35:55.334292  2460 solver.cpp:228] Iteration 10360, loss = 12536
I0422 11:35:55.334395  2460 solver.cpp:244]     Train net output #0: loss = 16859 (* 1 = 16859 loss)
I0422 11:35:55.334408  2460 sgd_solver.cpp:106] Iteration 10360, lr = 1e-10
I0422 11:36:25.317745  2460 solver.cpp:228] Iteration 10380, loss = 9345.04
I0422 11:36:25.317826  2460 solver.cpp:244]     Train net output #0: loss = 8407.05 (* 1 = 8407.05 loss)
I0422 11:36:25.317838  2460 sgd_solver.cpp:106] Iteration 10380, lr = 1e-10
I0422 11:37:03.019071  2460 solver.cpp:228] Iteration 10400, loss = 7852.96
I0422 11:37:03.019174  2460 solver.cpp:244]     Train net output #0: loss = 7852.96 (* 1 = 7852.96 loss)
I0422 11:37:03.019187  2460 sgd_solver.cpp:106] Iteration 10400, lr = 1e-10
I0422 11:37:32.996644  2460 solver.cpp:228] Iteration 10420, loss = 10797.8
I0422 11:37:32.996739  2460 solver.cpp:244]     Train net output #0: loss = 6361.62 (* 1 = 6361.62 loss)
I0422 11:37:32.996759  2460 sgd_solver.cpp:106] Iteration 10420, lr = 1e-10
I0422 11:38:02.977901  2460 solver.cpp:228] Iteration 10440, loss = 10609.2
I0422 11:38:02.977999  2460 solver.cpp:244]     Train net output #0: loss = 5905.61 (* 1 = 5905.61 loss)
I0422 11:38:02.978011  2460 sgd_solver.cpp:106] Iteration 10440, lr = 1e-10
I0422 11:38:32.961521  2460 solver.cpp:228] Iteration 10460, loss = 12674.6
I0422 11:38:32.961601  2460 solver.cpp:244]     Train net output #0: loss = 8611.29 (* 1 = 8611.29 loss)
I0422 11:38:32.961614  2460 sgd_solver.cpp:106] Iteration 10460, lr = 1e-10
I0422 11:39:02.940572  2460 solver.cpp:228] Iteration 10480, loss = 11567.4
I0422 11:39:02.940667  2460 solver.cpp:244]     Train net output #0: loss = 9749.13 (* 1 = 9749.13 loss)
I0422 11:39:02.940680  2460 sgd_solver.cpp:106] Iteration 10480, lr = 1e-10
I0422 11:39:40.649037  2460 solver.cpp:228] Iteration 10500, loss = 9827.79
I0422 11:39:40.649124  2460 solver.cpp:244]     Train net output #0: loss = 9827.79 (* 1 = 9827.79 loss)
I0422 11:39:40.649137  2460 sgd_solver.cpp:106] Iteration 10500, lr = 1e-10
I0422 11:40:10.621883  2460 solver.cpp:228] Iteration 10520, loss = 8578.5
I0422 11:40:10.621970  2460 solver.cpp:244]     Train net output #0: loss = 9734.67 (* 1 = 9734.67 loss)
I0422 11:40:10.621983  2460 sgd_solver.cpp:106] Iteration 10520, lr = 1e-10
I0422 11:40:40.606765  2460 solver.cpp:228] Iteration 10540, loss = 10525.2
I0422 11:40:40.606853  2460 solver.cpp:244]     Train net output #0: loss = 11770.7 (* 1 = 11770.7 loss)
I0422 11:40:40.606865  2460 sgd_solver.cpp:106] Iteration 10540, lr = 1e-10
I0422 11:41:10.570188  2460 solver.cpp:228] Iteration 10560, loss = 11239.3
I0422 11:41:10.570279  2460 solver.cpp:244]     Train net output #0: loss = 7500.15 (* 1 = 7500.15 loss)
I0422 11:41:10.570292  2460 sgd_solver.cpp:106] Iteration 10560, lr = 1e-10
I0422 11:41:40.552403  2460 solver.cpp:228] Iteration 10580, loss = 10733.6
I0422 11:41:40.552487  2460 solver.cpp:244]     Train net output #0: loss = 8264.43 (* 1 = 8264.43 loss)
I0422 11:41:40.552500  2460 sgd_solver.cpp:106] Iteration 10580, lr = 1e-10
I0422 11:42:18.283869  2460 solver.cpp:228] Iteration 10600, loss = 9542.59
I0422 11:42:18.283965  2460 solver.cpp:244]     Train net output #0: loss = 9542.59 (* 1 = 9542.59 loss)
I0422 11:42:18.283978  2460 sgd_solver.cpp:106] Iteration 10600, lr = 1e-10
I0422 11:42:48.266680  2460 solver.cpp:228] Iteration 10620, loss = 9871.85
I0422 11:42:48.266769  2460 solver.cpp:244]     Train net output #0: loss = 7037.54 (* 1 = 7037.54 loss)
I0422 11:42:48.266782  2460 sgd_solver.cpp:106] Iteration 10620, lr = 1e-10
I0422 11:43:18.237612  2460 solver.cpp:228] Iteration 10640, loss = 11411.6
I0422 11:43:18.237711  2460 solver.cpp:244]     Train net output #0: loss = 7349.79 (* 1 = 7349.79 loss)
I0422 11:43:18.237725  2460 sgd_solver.cpp:106] Iteration 10640, lr = 1e-10
I0422 11:43:48.219528  2460 solver.cpp:228] Iteration 10660, loss = 8674.69
I0422 11:43:48.219633  2460 solver.cpp:244]     Train net output #0: loss = 6729.89 (* 1 = 6729.89 loss)
I0422 11:43:48.219647  2460 sgd_solver.cpp:106] Iteration 10660, lr = 1e-10
I0422 11:44:18.196251  2460 solver.cpp:228] Iteration 10680, loss = 9228
I0422 11:44:18.196343  2460 solver.cpp:244]     Train net output #0: loss = 5940.59 (* 1 = 5940.59 loss)
I0422 11:44:18.196357  2460 sgd_solver.cpp:106] Iteration 10680, lr = 1e-10
I0422 11:44:55.902848  2460 solver.cpp:228] Iteration 10700, loss = 7502.95
I0422 11:44:55.902928  2460 solver.cpp:244]     Train net output #0: loss = 7502.95 (* 1 = 7502.95 loss)
I0422 11:44:55.902941  2460 sgd_solver.cpp:106] Iteration 10700, lr = 1e-10
I0422 11:45:25.875929  2460 solver.cpp:228] Iteration 10720, loss = 10195
I0422 11:45:25.876020  2460 solver.cpp:244]     Train net output #0: loss = 6706.63 (* 1 = 6706.63 loss)
I0422 11:45:25.876041  2460 sgd_solver.cpp:106] Iteration 10720, lr = 1e-10
I0422 11:45:55.853183  2460 solver.cpp:228] Iteration 10740, loss = 11832.9
I0422 11:45:55.853258  2460 solver.cpp:244]     Train net output #0: loss = 10293.7 (* 1 = 10293.7 loss)
I0422 11:45:55.853271  2460 sgd_solver.cpp:106] Iteration 10740, lr = 1e-10
I0422 11:46:25.837462  2460 solver.cpp:228] Iteration 10760, loss = 10998.6
I0422 11:46:25.837548  2460 solver.cpp:244]     Train net output #0: loss = 14322.9 (* 1 = 14322.9 loss)
I0422 11:46:25.837561  2460 sgd_solver.cpp:106] Iteration 10760, lr = 1e-10
I0422 11:46:55.819182  2460 solver.cpp:228] Iteration 10780, loss = 11295.8
I0422 11:46:55.819260  2460 solver.cpp:244]     Train net output #0: loss = 24349.3 (* 1 = 24349.3 loss)
I0422 11:46:55.819272  2460 sgd_solver.cpp:106] Iteration 10780, lr = 1e-10
I0422 11:47:33.536286  2460 solver.cpp:228] Iteration 10800, loss = 13844.5
I0422 11:47:33.536370  2460 solver.cpp:244]     Train net output #0: loss = 13844.5 (* 1 = 13844.5 loss)
I0422 11:47:33.536382  2460 sgd_solver.cpp:106] Iteration 10800, lr = 1e-10
I0422 11:48:03.519343  2460 solver.cpp:228] Iteration 10820, loss = 10749.3
I0422 11:48:03.519438  2460 solver.cpp:244]     Train net output #0: loss = 6730.48 (* 1 = 6730.48 loss)
I0422 11:48:03.519450  2460 sgd_solver.cpp:106] Iteration 10820, lr = 1e-10
I0422 11:48:33.498401  2460 solver.cpp:228] Iteration 10840, loss = 10533.7
I0422 11:48:33.498487  2460 solver.cpp:244]     Train net output #0: loss = 14808.5 (* 1 = 14808.5 loss)
I0422 11:48:33.498513  2460 sgd_solver.cpp:106] Iteration 10840, lr = 1e-10
I0422 11:49:03.486970  2460 solver.cpp:228] Iteration 10860, loss = 9528.32
I0422 11:49:03.487059  2460 solver.cpp:244]     Train net output #0: loss = 5196.94 (* 1 = 5196.94 loss)
I0422 11:49:03.487072  2460 sgd_solver.cpp:106] Iteration 10860, lr = 1e-10
I0422 11:49:33.465123  2460 solver.cpp:228] Iteration 10880, loss = 11771.2
I0422 11:49:33.465214  2460 solver.cpp:244]     Train net output #0: loss = 6148.14 (* 1 = 6148.14 loss)
I0422 11:49:33.465227  2460 sgd_solver.cpp:106] Iteration 10880, lr = 1e-10
I0422 11:50:11.174101  2460 solver.cpp:228] Iteration 10900, loss = 10263.4
I0422 11:50:11.174185  2460 solver.cpp:244]     Train net output #0: loss = 10263.4 (* 1 = 10263.4 loss)
I0422 11:50:11.174198  2460 sgd_solver.cpp:106] Iteration 10900, lr = 1e-10
I0422 11:50:41.150118  2460 solver.cpp:228] Iteration 10920, loss = 9761.17
I0422 11:50:41.150207  2460 solver.cpp:244]     Train net output #0: loss = 8561.81 (* 1 = 8561.81 loss)
I0422 11:50:41.150219  2460 sgd_solver.cpp:106] Iteration 10920, lr = 1e-10
I0422 11:51:11.134145  2460 solver.cpp:228] Iteration 10940, loss = 9834.73
I0422 11:51:11.134235  2460 solver.cpp:244]     Train net output #0: loss = 7936.59 (* 1 = 7936.59 loss)
I0422 11:51:11.134258  2460 sgd_solver.cpp:106] Iteration 10940, lr = 1e-10
I0422 11:51:41.120177  2460 solver.cpp:228] Iteration 10960, loss = 9272.06
I0422 11:51:41.120271  2460 solver.cpp:244]     Train net output #0: loss = 7374.97 (* 1 = 7374.97 loss)
I0422 11:51:41.120285  2460 sgd_solver.cpp:106] Iteration 10960, lr = 1e-10
I0422 11:52:11.102397  2460 solver.cpp:228] Iteration 10980, loss = 9689.55
I0422 11:52:11.102486  2460 solver.cpp:244]     Train net output #0: loss = 8727.24 (* 1 = 8727.24 loss)
I0422 11:52:11.102500  2460 sgd_solver.cpp:106] Iteration 10980, lr = 1e-10
I0422 11:52:39.582605  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_11000.caffemodel
I0422 11:52:42.969565  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_11000.solverstate
I0422 11:52:52.101003  2460 solver.cpp:228] Iteration 11000, loss = 8270.07
I0422 11:52:52.101116  2460 solver.cpp:244]     Train net output #0: loss = 8270.07 (* 1 = 8270.07 loss)
I0422 11:52:52.101136  2460 sgd_solver.cpp:106] Iteration 11000, lr = 1e-10
I0422 11:53:22.081028  2460 solver.cpp:228] Iteration 11020, loss = 10706.7
I0422 11:53:22.081116  2460 solver.cpp:244]     Train net output #0: loss = 5285.56 (* 1 = 5285.56 loss)
I0422 11:53:22.081130  2460 sgd_solver.cpp:106] Iteration 11020, lr = 1e-10
I0422 11:53:52.067697  2460 solver.cpp:228] Iteration 11040, loss = 11811.1
I0422 11:53:52.067786  2460 solver.cpp:244]     Train net output #0: loss = 14323.8 (* 1 = 14323.8 loss)
I0422 11:53:52.067806  2460 sgd_solver.cpp:106] Iteration 11040, lr = 1e-10
I0422 11:54:22.041967  2460 solver.cpp:228] Iteration 11060, loss = 12116.9
I0422 11:54:22.042052  2460 solver.cpp:244]     Train net output #0: loss = 9143.17 (* 1 = 9143.17 loss)
I0422 11:54:22.042068  2460 sgd_solver.cpp:106] Iteration 11060, lr = 1e-10
I0422 11:54:52.017077  2460 solver.cpp:228] Iteration 11080, loss = 11546.5
I0422 11:54:52.017161  2460 solver.cpp:244]     Train net output #0: loss = 7012.21 (* 1 = 7012.21 loss)
I0422 11:54:52.017194  2460 sgd_solver.cpp:106] Iteration 11080, lr = 1e-10
I0422 11:55:29.732339  2460 solver.cpp:228] Iteration 11100, loss = 17881.1
I0422 11:55:29.732427  2460 solver.cpp:244]     Train net output #0: loss = 17881.1 (* 1 = 17881.1 loss)
I0422 11:55:29.732440  2460 sgd_solver.cpp:106] Iteration 11100, lr = 1e-10
I0422 11:55:59.713377  2460 solver.cpp:228] Iteration 11120, loss = 11584.8
I0422 11:55:59.713471  2460 solver.cpp:244]     Train net output #0: loss = 7790.15 (* 1 = 7790.15 loss)
I0422 11:55:59.713484  2460 sgd_solver.cpp:106] Iteration 11120, lr = 1e-10
I0422 11:56:29.693809  2460 solver.cpp:228] Iteration 11140, loss = 9398.03
I0422 11:56:29.693888  2460 solver.cpp:244]     Train net output #0: loss = 9342.28 (* 1 = 9342.28 loss)
I0422 11:56:29.693900  2460 sgd_solver.cpp:106] Iteration 11140, lr = 1e-10
I0422 11:56:59.667404  2460 solver.cpp:228] Iteration 11160, loss = 10327.6
I0422 11:56:59.667484  2460 solver.cpp:244]     Train net output #0: loss = 13713.2 (* 1 = 13713.2 loss)
I0422 11:56:59.667497  2460 sgd_solver.cpp:106] Iteration 11160, lr = 1e-10
I0422 11:57:29.642896  2460 solver.cpp:228] Iteration 11180, loss = 10504.2
I0422 11:57:29.642997  2460 solver.cpp:244]     Train net output #0: loss = 9035.26 (* 1 = 9035.26 loss)
I0422 11:57:29.643009  2460 sgd_solver.cpp:106] Iteration 11180, lr = 1e-10
I0422 11:58:07.342571  2460 solver.cpp:228] Iteration 11200, loss = 9384.26
I0422 11:58:07.342654  2460 solver.cpp:244]     Train net output #0: loss = 9384.26 (* 1 = 9384.26 loss)
I0422 11:58:07.342667  2460 sgd_solver.cpp:106] Iteration 11200, lr = 1e-10
I0422 11:58:37.320006  2460 solver.cpp:228] Iteration 11220, loss = 11334.3
I0422 11:58:37.320096  2460 solver.cpp:244]     Train net output #0: loss = 15954.8 (* 1 = 15954.8 loss)
I0422 11:58:37.320108  2460 sgd_solver.cpp:106] Iteration 11220, lr = 1e-10
I0422 11:59:07.305747  2460 solver.cpp:228] Iteration 11240, loss = 10895
I0422 11:59:07.305827  2460 solver.cpp:244]     Train net output #0: loss = 9650.59 (* 1 = 9650.59 loss)
I0422 11:59:07.305840  2460 sgd_solver.cpp:106] Iteration 11240, lr = 1e-10
I0422 11:59:37.287971  2460 solver.cpp:228] Iteration 11260, loss = 8938.22
I0422 11:59:37.288063  2460 solver.cpp:244]     Train net output #0: loss = 7211.86 (* 1 = 7211.86 loss)
I0422 11:59:37.288075  2460 sgd_solver.cpp:106] Iteration 11260, lr = 1e-10
I0422 12:00:07.269714  2460 solver.cpp:228] Iteration 11280, loss = 9422.88
I0422 12:00:07.269804  2460 solver.cpp:244]     Train net output #0: loss = 9894.85 (* 1 = 9894.85 loss)
I0422 12:00:07.269816  2460 sgd_solver.cpp:106] Iteration 11280, lr = 1e-10
34:16.188823 Begin seg tests
>>> 2017-04-22 11:34:23.272087 Iteration 10300 loss 11070.7055664
>>> 2017-04-22 11:34:23.272205 Iteration 10300 overall accuracy 0.976570666667
>>> 2017-04-22 11:34:23.272274 Iteration 10300 mean accuracy 0.976021568263
>>> 2017-04-22 11:34:23.272501 Iteration 10300 mean IU 0.953535214274
>>> 2017-04-22 11:34:23.272605 Iteration 10300 fwavacc 0.95422117093
>>> 2017-04-22 11:36:53.802598 Begin seg tests
>>> 2017-04-22 11:37:00.888807 Iteration 10400 loss 10142.1541748
>>> 2017-04-22 11:37:00.888916 Iteration 10400 overall accuracy 0.978460888889
>>> 2017-04-22 11:37:00.888991 Iteration 10400 mean accuracy 0.978468828828
>>> 2017-04-22 11:37:00.889227 Iteration 10400 mean IU 0.957823759008
>>> 2017-04-22 11:37:00.889353 Iteration 10400 fwavacc 0.95783067159
>>> 2017-04-22 11:39:31.430651 Begin seg tests
>>> 2017-04-22 11:39:38.512229 Iteration 10500 loss 8759.39078776
>>> 2017-04-22 11:39:38.512343 Iteration 10500 overall accuracy 0.981651555556
>>> 2017-04-22 11:39:38.512410 Iteration 10500 mean accuracy 0.980104583395
>>> 2017-04-22 11:39:38.512620 Iteration 10500 mean IU 0.963245436775
>>> 2017-04-22 11:39:38.512759 Iteration 10500 fwavacc 0.963931580866
>>> 2017-04-22 11:42:09.030921 Begin seg tests
>>> 2017-04-22 11:42:16.121224 Iteration 10600 loss 13120.7950033
>>> 2017-04-22 11:42:16.121328 Iteration 10600 overall accuracy 0.971073333333
>>> 2017-04-22 11:42:16.121396 Iteration 10600 mean accuracy 0.971122354416
>>> 2017-04-22 11:42:16.121609 Iteration 10600 mean IU 0.943766078582
>>> 2017-04-22 11:42:16.121712 Iteration 10600 fwavacc 0.943775634587
>>> 2017-04-22 11:44:46.677673 Begin seg tests
>>> 2017-04-22 11:44:53.765122 Iteration 10700 loss 10919.2272949
>>> 2017-04-22 11:44:53.765231 Iteration 10700 overall accuracy 0.976063111111
>>> 2017-04-22 11:44:53.765316 Iteration 10700 mean accuracy 0.976485194769
>>> 2017-04-22 11:44:53.765539 Iteration 10700 mean IU 0.952676341231
>>> 2017-04-22 11:44:53.765644 Iteration 10700 fwavacc 0.95329194292
>>> 2017-04-22 11:47:24.297952 Begin seg tests
>>> 2017-04-22 11:47:31.379293 Iteration 10800 loss 11316.1831868
>>> 2017-04-22 11:47:31.379392 Iteration 10800 overall accuracy 0.975612888889
>>> 2017-04-22 11:47:31.379451 Iteration 10800 mean accuracy 0.974677901367
>>> 2017-04-22 11:47:31.379693 Iteration 10800 mean IU 0.951685218574
>>> 2017-04-22 11:47:31.379801 Iteration 10800 fwavacc 0.952375603304
>>> 2017-04-22 11:50:01.944765 Begin seg tests
>>> 2017-04-22 11:50:09.037191 Iteration 10900 loss 8727.57381185
>>> 2017-04-22 11:50:09.037289 Iteration 10900 overall accuracy 0.980709333333
>>> 2017-04-22 11:50:09.037351 Iteration 10900 mean accuracy 0.980633711635
>>> 2017-04-22 11:50:09.037570 Iteration 10900 mean IU 0.962017475915
>>> 2017-04-22 11:50:09.037674 Iteration 10900 fwavacc 0.962151140584
>>> 2017-04-22 11:52:43.940574 Begin seg tests
>>> 2017-04-22 11:52:50.041837 Iteration 11000 loss 13121.2581787
>>> 2017-04-22 11:52:50.041946 Iteration 11000 overall accuracy 0.970096
>>> 2017-04-22 11:52:50.042014 Iteration 11000 mean accuracy 0.969622576899
>>> 2017-04-22 11:52:50.042231 Iteration 11000 mean IU 0.941704923342
>>> 2017-04-22 11:52:50.042334 Iteration 11000 fwavacc 0.941911685111
>>> 2017-04-22 11:55:20.489912 Begin seg tests
>>> 2017-04-22 11:55:27.575372 Iteration 11100 loss 8447.35880534
>>> 2017-04-22 11:55:27.575472 Iteration 11100 overall accuracy 0.980393333333
>>> 2017-04-22 11:55:27.575531 Iteration 11100 mean accuracy 0.980646965723
>>> 2017-04-22 11:55:27.575772 Iteration 11100 mean IU 0.961415635518
>>> 2017-04-22 11:55:27.575883 Iteration 11100 fwavacc 0.961554672743
>>> 2017-04-22 11:57:58.121753 Begin seg tests
>>> 2017-04-22 11:58:05.214607 Iteration 11200 loss 10786.6014811
>>> 2017-04-22 11:58:05.214713 Iteration 11200 overall accuracy 0.976196888889
>>> 2017-04-22 11:58:05.214779 Iteration 11200 mean accuracy 0.976301845959
>>> 2017-04-22 11:58:05.214997 Iteration 11200 mean IU 0.953496402116
>>> 2017-04-22 11:58:05.215105 Iteration 11200 fwavacc 0.953503695551
>>> 2017-04-22 12:00:35.756266 Begin seg teI0422 12:00:45.022279  2460 solver.cpp:228] Iteration 11300, loss = 14608
I0422 12:00:45.022361  2460 solver.cpp:244]     Train net output #0: loss = 14608 (* 1 = 14608 loss)
I0422 12:00:45.022374  2460 sgd_solver.cpp:106] Iteration 11300, lr = 1e-10
I0422 12:01:14.996029  2460 solver.cpp:228] Iteration 11320, loss = 13447.5
I0422 12:01:14.996116  2460 solver.cpp:244]     Train net output #0: loss = 13734.3 (* 1 = 13734.3 loss)
I0422 12:01:14.996129  2460 sgd_solver.cpp:106] Iteration 11320, lr = 1e-10
I0422 12:01:44.970010  2460 solver.cpp:228] Iteration 11340, loss = 10280.4
I0422 12:01:44.970111  2460 solver.cpp:244]     Train net output #0: loss = 16301.4 (* 1 = 16301.4 loss)
I0422 12:01:44.970124  2460 sgd_solver.cpp:106] Iteration 11340, lr = 1e-10
I0422 12:02:14.944475  2460 solver.cpp:228] Iteration 11360, loss = 9151.42
I0422 12:02:14.944579  2460 solver.cpp:244]     Train net output #0: loss = 12758.1 (* 1 = 12758.1 loss)
I0422 12:02:14.944592  2460 sgd_solver.cpp:106] Iteration 11360, lr = 1e-10
I0422 12:02:44.923038  2460 solver.cpp:228] Iteration 11380, loss = 10673.7
I0422 12:02:44.923116  2460 solver.cpp:244]     Train net output #0: loss = 5233.2 (* 1 = 5233.2 loss)
I0422 12:02:44.923130  2460 sgd_solver.cpp:106] Iteration 11380, lr = 1e-10
I0422 12:03:22.606976  2460 solver.cpp:228] Iteration 11400, loss = 11385.7
I0422 12:03:22.607076  2460 solver.cpp:244]     Train net output #0: loss = 11385.7 (* 1 = 11385.7 loss)
I0422 12:03:22.607105  2460 sgd_solver.cpp:106] Iteration 11400, lr = 1e-10
I0422 12:03:52.579438  2460 solver.cpp:228] Iteration 11420, loss = 9440.48
I0422 12:03:52.579516  2460 solver.cpp:244]     Train net output #0: loss = 9171.78 (* 1 = 9171.78 loss)
I0422 12:03:52.579530  2460 sgd_solver.cpp:106] Iteration 11420, lr = 1e-10
I0422 12:04:22.565513  2460 solver.cpp:228] Iteration 11440, loss = 9067.21
I0422 12:04:22.565598  2460 solver.cpp:244]     Train net output #0: loss = 8404.57 (* 1 = 8404.57 loss)
I0422 12:04:22.565610  2460 sgd_solver.cpp:106] Iteration 11440, lr = 1e-10
I0422 12:04:52.544721  2460 solver.cpp:228] Iteration 11460, loss = 12226.7
I0422 12:04:52.544821  2460 solver.cpp:244]     Train net output #0: loss = 10469.8 (* 1 = 10469.8 loss)
I0422 12:04:52.544834  2460 sgd_solver.cpp:106] Iteration 11460, lr = 1e-10
I0422 12:05:22.525480  2460 solver.cpp:228] Iteration 11480, loss = 10652.6
I0422 12:05:22.525557  2460 solver.cpp:244]     Train net output #0: loss = 7098.48 (* 1 = 7098.48 loss)
I0422 12:05:22.525569  2460 sgd_solver.cpp:106] Iteration 11480, lr = 1e-10
I0422 12:06:00.284999  2460 solver.cpp:228] Iteration 11500, loss = 7533.17
I0422 12:06:00.285079  2460 solver.cpp:244]     Train net output #0: loss = 7533.17 (* 1 = 7533.17 loss)
I0422 12:06:00.285091  2460 sgd_solver.cpp:106] Iteration 11500, lr = 1e-10
I0422 12:06:30.273064  2460 solver.cpp:228] Iteration 11520, loss = 10121.2
I0422 12:06:30.273157  2460 solver.cpp:244]     Train net output #0: loss = 7721.15 (* 1 = 7721.15 loss)
I0422 12:06:30.273169  2460 sgd_solver.cpp:106] Iteration 11520, lr = 1e-10
I0422 12:07:00.246881  2460 solver.cpp:228] Iteration 11540, loss = 11707
I0422 12:07:00.246978  2460 solver.cpp:244]     Train net output #0: loss = 13305.9 (* 1 = 13305.9 loss)
I0422 12:07:00.246994  2460 sgd_solver.cpp:106] Iteration 11540, lr = 1e-10
I0422 12:07:30.229054  2460 solver.cpp:228] Iteration 11560, loss = 10720.4
I0422 12:07:30.229145  2460 solver.cpp:244]     Train net output #0: loss = 8300.42 (* 1 = 8300.42 loss)
I0422 12:07:30.229156  2460 sgd_solver.cpp:106] Iteration 11560, lr = 1e-10
I0422 12:08:00.210006  2460 solver.cpp:228] Iteration 11580, loss = 9887.23
I0422 12:08:00.210093  2460 solver.cpp:244]     Train net output #0: loss = 14135.9 (* 1 = 14135.9 loss)
I0422 12:08:00.210106  2460 sgd_solver.cpp:106] Iteration 11580, lr = 1e-10
I0422 12:08:37.961855  2460 solver.cpp:228] Iteration 11600, loss = 8812.66
I0422 12:08:37.961944  2460 solver.cpp:244]     Train net output #0: loss = 8812.66 (* 1 = 8812.66 loss)
I0422 12:08:37.961957  2460 sgd_solver.cpp:106] Iteration 11600, lr = 1e-10
I0422 12:09:07.938491  2460 solver.cpp:228] Iteration 11620, loss = 12248.7
I0422 12:09:07.938587  2460 solver.cpp:244]     Train net output #0: loss = 8633.94 (* 1 = 8633.94 loss)
I0422 12:09:07.938607  2460 sgd_solver.cpp:106] Iteration 11620, lr = 1e-10
I0422 12:09:37.916533  2460 solver.cpp:228] Iteration 11640, loss = 10122.4
I0422 12:09:37.916622  2460 solver.cpp:244]     Train net output #0: loss = 7501.32 (* 1 = 7501.32 loss)
I0422 12:09:37.916635  2460 sgd_solver.cpp:106] Iteration 11640, lr = 1e-10
I0422 12:10:07.894114  2460 solver.cpp:228] Iteration 11660, loss = 13497.6
I0422 12:10:07.894209  2460 solver.cpp:244]     Train net output #0: loss = 27051.5 (* 1 = 27051.5 loss)
I0422 12:10:07.894222  2460 sgd_solver.cpp:106] Iteration 11660, lr = 1e-10
I0422 12:10:37.864271  2460 solver.cpp:228] Iteration 11680, loss = 10532.8
I0422 12:10:37.864356  2460 solver.cpp:244]     Train net output #0: loss = 19757.1 (* 1 = 19757.1 loss)
I0422 12:10:37.864369  2460 sgd_solver.cpp:106] Iteration 11680, lr = 1e-10
I0422 12:11:15.595407  2460 solver.cpp:228] Iteration 11700, loss = 7911.76
I0422 12:11:15.595492  2460 solver.cpp:244]     Train net output #0: loss = 7911.76 (* 1 = 7911.76 loss)
I0422 12:11:15.595505  2460 sgd_solver.cpp:106] Iteration 11700, lr = 1e-10
I0422 12:11:45.576757  2460 solver.cpp:228] Iteration 11720, loss = 12115.5
I0422 12:11:45.576846  2460 solver.cpp:244]     Train net output #0: loss = 16613.9 (* 1 = 16613.9 loss)
I0422 12:11:45.576876  2460 sgd_solver.cpp:106] Iteration 11720, lr = 1e-10
I0422 12:12:15.548763  2460 solver.cpp:228] Iteration 11740, loss = 9064.18
I0422 12:12:15.548854  2460 solver.cpp:244]     Train net output #0: loss = 5608.58 (* 1 = 5608.58 loss)
I0422 12:12:15.548877  2460 sgd_solver.cpp:106] Iteration 11740, lr = 1e-10
I0422 12:12:45.520064  2460 solver.cpp:228] Iteration 11760, loss = 9638.52
I0422 12:12:45.520148  2460 solver.cpp:244]     Train net output #0: loss = 6071.71 (* 1 = 6071.71 loss)
I0422 12:12:45.520161  2460 sgd_solver.cpp:106] Iteration 11760, lr = 1e-10
I0422 12:13:15.488144  2460 solver.cpp:228] Iteration 11780, loss = 11366.7
I0422 12:13:15.488230  2460 solver.cpp:244]     Train net output #0: loss = 7734.22 (* 1 = 7734.22 loss)
I0422 12:13:15.488243  2460 sgd_solver.cpp:106] Iteration 11780, lr = 1e-10
I0422 12:13:53.174975  2460 solver.cpp:228] Iteration 11800, loss = 11237.9
I0422 12:13:53.175071  2460 solver.cpp:244]     Train net output #0: loss = 11237.9 (* 1 = 11237.9 loss)
I0422 12:13:53.175087  2460 sgd_solver.cpp:106] Iteration 11800, lr = 1e-10
I0422 12:14:23.152983  2460 solver.cpp:228] Iteration 11820, loss = 13848.4
I0422 12:14:23.153070  2460 solver.cpp:244]     Train net output #0: loss = 31922.2 (* 1 = 31922.2 loss)
I0422 12:14:23.153084  2460 sgd_solver.cpp:106] Iteration 11820, lr = 1e-10
I0422 12:14:53.134594  2460 solver.cpp:228] Iteration 11840, loss = 8889.68
I0422 12:14:53.134685  2460 solver.cpp:244]     Train net output #0: loss = 6291.04 (* 1 = 6291.04 loss)
I0422 12:14:53.134698  2460 sgd_solver.cpp:106] Iteration 11840, lr = 1e-10
I0422 12:15:23.116538  2460 solver.cpp:228] Iteration 11860, loss = 10656.5
I0422 12:15:23.116627  2460 solver.cpp:244]     Train net output #0: loss = 9504.14 (* 1 = 9504.14 loss)
I0422 12:15:23.116641  2460 sgd_solver.cpp:106] Iteration 11860, lr = 1e-10
I0422 12:15:53.101497  2460 solver.cpp:228] Iteration 11880, loss = 10767.6
I0422 12:15:53.101583  2460 solver.cpp:244]     Train net output #0: loss = 13101.2 (* 1 = 13101.2 loss)
I0422 12:15:53.101595  2460 sgd_solver.cpp:106] Iteration 11880, lr = 1e-10
I0422 12:16:30.794765  2460 solver.cpp:228] Iteration 11900, loss = 8576.88
I0422 12:16:30.794852  2460 solver.cpp:244]     Train net output #0: loss = 8576.88 (* 1 = 8576.88 loss)
I0422 12:16:30.794865  2460 sgd_solver.cpp:106] Iteration 11900, lr = 1e-10
I0422 12:17:00.777662  2460 solver.cpp:228] Iteration 11920, loss = 10561.5
I0422 12:17:00.777751  2460 solver.cpp:244]     Train net output #0: loss = 12545.6 (* 1 = 12545.6 loss)
I0422 12:17:00.777765  2460 sgd_solver.cpp:106] Iteration 11920, lr = 1e-10
I0422 12:17:30.757736  2460 solver.cpp:228] Iteration 11940, loss = 10053.1
I0422 12:17:30.757828  2460 solver.cpp:244]     Train net output #0: loss = 16703.1 (* 1 = 16703.1 loss)
I0422 12:17:30.757840  2460 sgd_solver.cpp:106] Iteration 11940, lr = 1e-10
I0422 12:18:00.744196  2460 solver.cpp:228] Iteration 11960, loss = 10562.8
I0422 12:18:00.744292  2460 solver.cpp:244]     Train net output #0: loss = 8689.55 (* 1 = 8689.55 loss)
I0422 12:18:00.744305  2460 sgd_solver.cpp:106] Iteration 11960, lr = 1e-10
I0422 12:18:30.722991  2460 solver.cpp:228] Iteration 11980, loss = 11977.4
I0422 12:18:30.723094  2460 solver.cpp:244]     Train net output #0: loss = 11761.2 (* 1 = 11761.2 loss)
I0422 12:18:30.723107  2460 sgd_solver.cpp:106] Iteration 11980, lr = 1e-10
I0422 12:18:59.215464  2460 solver.cpp:454] Snapshotting to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_12000.caffemodel
I0422 12:19:02.576432  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ubuntu/pynb/caffe-future/ilsvrc-nets/_iter_12000.solverstate
I0422 12:19:11.713858  2460 solver.cpp:228] Iteration 12000, loss = 19334.5
I0422 12:19:11.713944  2460 solver.cpp:244]     Train net output #0: loss = 19334.5 (* 1 = 19334.5 loss)
I0422 12:19:11.713958  2460 sgd_solver.cpp:106] Iteration 12000, lr = 1e-10
I0422 12:19:41.690696  2460 solver.cpp:228] Iteration 12020, loss = 9134.87
I0422 12:19:41.690804  2460 solver.cpp:244]     Train net output #0: loss = 7873.28 (* 1 = 7873.28 loss)
I0422 12:19:41.690816  2460 sgd_solver.cpp:106] Iteration 12020, lr = 1e-10
I0422 12:20:11.669741  2460 solver.cpp:228] Iteration 12040, loss = 10268.2
I0422 12:20:11.669834  2460 solver.cpp:244]     Train net output #0: loss = 8198.22 (* 1 = 8198.22 loss)
I0422 12:20:11.669857  2460 sgd_solver.cpp:106] Iteration 12040, lr = 1e-10
I0422 12:20:41.652719  2460 solver.cpp:228] Iteration 12060, loss = 10492.8
I0422 12:20:41.652807  2460 solver.cpp:244]     Train net output #0: loss = 9147.97 (* 1 = 9147.97 loss)
I0422 12:20:41.652822  2460 sgd_solver.cpp:106] Iteration 12060, lr = 1e-10
I0422 12:21:11.625610  2460 solver.cpp:228] Iteration 12080, loss = 10214.3
I0422 12:21:11.625700  2460 solver.cpp:244]     Train net output #0: loss = 7170.54 (* 1 = 7170.54 loss)
I0422 12:21:11.625712  2460 sgd_solver.cpp:106] Iteration 12080, lr = 1e-10
I0422 12:21:49.291010  2460 solver.cpp:228] Iteration 12100, loss = 5944.38
I0422 12:21:49.291100  2460 solver.cpp:244]     Train net output #0: loss = 5944.38 (* 1 = 5944.38 loss)
I0422 12:21:49.291116  2460 sgd_solver.cpp:106] Iteration 12100, lr = 1e-10
I0422 12:22:19.281357  2460 solver.cpp:228] Iteration 12120, loss = 11255.8
I0422 12:22:19.281455  2460 solver.cpp:244]     Train net output #0: loss = 13220.2 (* 1 = 13220.2 loss)
I0422 12:22:19.281467  2460 sgd_solver.cpp:106] Iteration 12120, lr = 1e-10
I0422 12:22:49.259408  2460 solver.cpp:228] Iteration 12140, loss = 11483.4
I0422 12:22:49.259491  2460 solver.cpp:244]     Train net output #0: loss = 13103.2 (* 1 = 13103.2 loss)
I0422 12:22:49.259505  2460 sgd_solver.cpp:106] Iteration 12140, lr = 1e-10
I0422 12:23:19.236529  2460 solver.cpp:228] Iteration 12160, loss = 11318.5
I0422 12:23:19.236626  2460 solver.cpp:244]     Train net output #0: loss = 7146.52 (* 1 = 7146.52 loss)
I0422 12:23:19.236639  2460 sgd_solver.cpp:106] Iteration 12160, lr = 1e-10
I0422 12:23:49.211879  2460 solver.cpp:228] Iteration 12180, loss = 8923.99
I0422 12:23:49.211966  2460 solver.cpp:244]     Train net output #0: loss = 7666.04 (* 1 = 7666.04 loss)
I0422 12:23:49.211978  2460 sgd_solver.cpp:106] Iteration 12180, lr = 1e-10
I0422 12:24:26.924422  2460 solver.cpp:228] Iteration 12200, loss = 9668.61
I0422 12:24:26.924507  2460 solver.cpp:244]     Train net output #0: loss = 9668.61 (* 1 = 9668.61 loss)
I0422 12:24:26.924530  2460 sgd_solver.cpp:106] Iteration 12200, lr = 1e-10
I0422 12:24:56.900527  2460 solver.cpp:228] Iteration 12220, loss = 11270.9
I0422 12:24:56.900610  2460 solver.cpp:244]     Train net output #0: loss = 22827.7 (* 1 = 22827.7 loss)
I0422 12:24:56.900624  2460 sgd_solver.cpp:106] Iteration 12220, lr = 1e-10
I0422 12:25:26.884138  2460 solver.cpp:228] Iteration 12240, loss = 9415.94
I0422 12:25:26.884232  2460 solver.cpp:244]     Train net output #0: loss = 5980.95 (* 1 = 5980.95 loss)
I0422 12:25:26.884245  2460 sgd_solver.cpp:106] Iteration 12240, lr = 1e-10
I0422 12:25:56.882850  2460 solver.cpp:228] Iteration 12260, loss = 10174.2
I0422 12:25:56.882935  2460 solver.cpp:244]     Train net output #0: loss = 6411.88 (* 1 = 6411.88 loss)
I0422 12:25:56.882947  2460 sgd_solver.cpp:106] Iteration 12260, lr = 1e-10
I0422 12:26:26.862162  2460 solver.cpp:228] Iteration 12280, loss = 10513.8
I0422 12:26:26.862262  2460 solver.cpp:244]     Train net output #0: loss = 15751.1 (* 1 = 15751.1 loss)
I0422 12:26:26.862275  2460 sgd_solver.cpp:106] Iteration 12280, lr = 1e-10
sts
>>> 2017-04-22 12:00:42.837964 Iteration 11300 loss 8376.81947835
>>> 2017-04-22 12:00:42.838069 Iteration 11300 overall accuracy 0.982250222222
>>> 2017-04-22 12:00:42.838143 Iteration 11300 mean accuracy 0.981250400712
>>> 2017-04-22 12:00:42.838358 Iteration 11300 mean IU 0.964714405741
>>> 2017-04-22 12:00:42.838469 Iteration 11300 fwavacc 0.965096512621
>>> 2017-04-22 12:03:13.401157 Begin seg tests
>>> 2017-04-22 12:03:20.496884 Iteration 11400 loss 14227.7743734
>>> 2017-04-22 12:03:20.496996 Iteration 11400 overall accuracy 0.969594666667
>>> 2017-04-22 12:03:20.497058 Iteration 11400 mean accuracy 0.970905258927
>>> 2017-04-22 12:03:20.497282 Iteration 11400 mean IU 0.940194915294
>>> 2017-04-22 12:03:20.497396 Iteration 11400 fwavacc 0.94110226753
>>> 2017-04-22 12:05:51.003178 Begin seg tests
>>> 2017-04-22 12:05:58.083953 Iteration 11500 loss 12566.5892741
>>> 2017-04-22 12:05:58.084057 Iteration 11500 overall accuracy 0.973087111111
>>> 2017-04-22 12:05:58.084116 Iteration 11500 mean accuracy 0.97099353176
>>> 2017-04-22 12:05:58.084320 Iteration 11500 mean IU 0.946666106567
>>> 2017-04-22 12:05:58.084429 Iteration 11500 fwavacc 0.947513027757
>>> 2017-04-22 12:08:28.683852 Begin seg tests
>>> 2017-04-22 12:08:35.768708 Iteration 11600 loss 11598.5879313
>>> 2017-04-22 12:08:35.768812 Iteration 11600 overall accuracy 0.974444444444
>>> 2017-04-22 12:08:35.768876 Iteration 11600 mean accuracy 0.974468779536
>>> 2017-04-22 12:08:35.769092 Iteration 11600 mean IU 0.950162434064
>>> 2017-04-22 12:08:35.769204 Iteration 11600 fwavacc 0.950162793167
>>> 2017-04-22 12:11:06.346931 Begin seg tests
>>> 2017-04-22 12:11:13.441626 Iteration 11700 loss 9841.34985352
>>> 2017-04-22 12:11:13.441730 Iteration 11700 overall accuracy 0.977816888889
>>> 2017-04-22 12:11:13.441804 Iteration 11700 mean accuracy 0.977944092631
>>> 2017-04-22 12:11:13.442038 Iteration 11700 mean IU 0.956079895096
>>> 2017-04-22 12:11:13.442149 Iteration 11700 fwavacc 0.956625385621
>>> 2017-04-22 12:13:43.958780 Begin seg tests
>>> 2017-04-22 12:13:51.044846 Iteration 11800 loss 11549.0608724
>>> 2017-04-22 12:13:51.044943 Iteration 11800 overall accuracy 0.975200888889
>>> 2017-04-22 12:13:51.045008 Iteration 11800 mean accuracy 0.974850639814
>>> 2017-04-22 12:13:51.045217 Iteration 11800 mean IU 0.951518652148
>>> 2017-04-22 12:13:51.045334 Iteration 11800 fwavacc 0.951584087656
>>> 2017-04-22 12:16:21.583746 Begin seg tests
>>> 2017-04-22 12:16:28.667802 Iteration 11900 loss 10309.0218913
>>> 2017-04-22 12:16:28.667901 Iteration 11900 overall accuracy 0.975100888889
>>> 2017-04-22 12:16:28.667966 Iteration 11900 mean accuracy 0.974451275454
>>> 2017-04-22 12:16:28.668188 Iteration 11900 mean IU 0.951244309517
>>> 2017-04-22 12:16:28.668300 Iteration 11900 fwavacc 0.951380251436
>>> 2017-04-22 12:19:03.544641 Begin seg tests
>>> 2017-04-22 12:19:09.636560 Iteration 12000 loss 11078.7856852
>>> 2017-04-22 12:19:09.636663 Iteration 12000 overall accuracy 0.975942666667
>>> 2017-04-22 12:19:09.636732 Iteration 12000 mean accuracy 0.975923741917
>>> 2017-04-22 12:19:09.636993 Iteration 12000 mean IU 0.952988576899
>>> 2017-04-22 12:19:09.637138 Iteration 12000 fwavacc 0.953016089707
>>> 2017-04-22 12:21:40.104245 Begin seg tests
>>> 2017-04-22 12:21:47.193020 Iteration 12100 loss 11498.5660807
>>> 2017-04-22 12:21:47.193121 Iteration 12100 overall accuracy 0.973943111111
>>> 2017-04-22 12:21:47.193179 Iteration 12100 mean accuracy 0.974093091575
>>> 2017-04-22 12:21:47.193387 Iteration 12100 mean IU 0.949128380621
>>> 2017-04-22 12:21:47.193492 Iteration 12100 fwavacc 0.949220879084
>>> 2017-04-22 12:24:17.702858 Begin seg tests
>>> 2017-04-22 12:24:24.787265 Iteration 12200 loss 8674.27201335
>>> 2017-04-22 12:24:24.787356 Iteration 12200 overall accuracy 0.982301333333
>>> 2017-04-22 12:24:24.787416 Iteration 12200 mean accuracy 0.980416808257
>>> 2017-04-22 12:24:24.787617 Iteration 12200 mean IU 0.963929091442
>>> 2017-04-22 12:24:24.787764 Iteration 12200 fwavacc 0.965196770122
>>> 2017-04-22 12:26:55.348049 Begin seg tests
>>> 2017-04-22I0422 12:27:04.598564  2460 solver.cpp:228] Iteration 12300, loss = 4942.93
I0422 12:27:04.598651  2460 solver.cpp:244]     Train net output #0: loss = 4942.93 (* 1 = 4942.93 loss)
I0422 12:27:04.598665  2460 sgd_solver.cpp:106] Iteration 12300, lr = 1e-10
I0422 12:27:34.587185  2460 solver.cpp:228] Iteration 12320, loss = 8317.88
I0422 12:27:34.587273  2460 solver.cpp:244]     Train net output #0: loss = 9000.23 (* 1 = 9000.23 loss)
I0422 12:27:34.587287  2460 sgd_solver.cpp:106] Iteration 12320, lr = 1e-10
I0422 12:28:04.562873  2460 solver.cpp:228] Iteration 12340, loss = 10477.9
I0422 12:28:04.562974  2460 solver.cpp:244]     Train net output #0: loss = 10594.8 (* 1 = 10594.8 loss)
I0422 12:28:04.562988  2460 sgd_solver.cpp:106] Iteration 12340, lr = 1e-10
I0422 12:28:34.537216  2460 solver.cpp:228] Iteration 12360, loss = 10931.2
I0422 12:28:34.537317  2460 solver.cpp:244]     Train net output #0: loss = 8878.07 (* 1 = 8878.07 loss)
I0422 12:28:34.537328  2460 sgd_solver.cpp:106] Iteration 12360, lr = 1e-10
I0422 12:29:04.516649  2460 solver.cpp:228] Iteration 12380, loss = 10181.8
I0422 12:29:04.516743  2460 solver.cpp:244]     Train net output #0: loss = 9109.5 (* 1 = 9109.5 loss)
I0422 12:29:04.516757  2460 sgd_solver.cpp:106] Iteration 12380, lr = 1e-10
I0422 12:29:42.258940  2460 solver.cpp:228] Iteration 12400, loss = 24153
I0422 12:29:42.259026  2460 solver.cpp:244]     Train net output #0: loss = 24153 (* 1 = 24153 loss)
I0422 12:29:42.259038  2460 sgd_solver.cpp:106] Iteration 12400, lr = 1e-10
I0422 12:30:12.239653  2460 solver.cpp:228] Iteration 12420, loss = 9396.33
I0422 12:30:12.239737  2460 solver.cpp:244]     Train net output #0: loss = 5027.2 (* 1 = 5027.2 loss)
I0422 12:30:12.239750  2460 sgd_solver.cpp:106] Iteration 12420, lr = 1e-10
I0422 12:30:42.222199  2460 solver.cpp:228] Iteration 12440, loss = 11150.3
I0422 12:30:42.222295  2460 solver.cpp:244]     Train net output #0: loss = 7056.17 (* 1 = 7056.17 loss)
I0422 12:30:42.222307  2460 sgd_solver.cpp:106] Iteration 12440, lr = 1e-10
I0422 12:31:12.211088  2460 solver.cpp:228] Iteration 12460, loss = 10460.5
I0422 12:31:12.211197  2460 solver.cpp:244]     Train net output #0: loss = 13787.4 (* 1 = 13787.4 loss)
I0422 12:31:12.211220  2460 sgd_solver.cpp:106] Iteration 12460, lr = 1e-10
I0422 12:31:42.178331  2460 solver.cpp:228] Iteration 12480, loss = 10382.3
I0422 12:31:42.178423  2460 solver.cpp:244]     Train net output #0: loss = 16898.2 (* 1 = 16898.2 loss)
I0422 12:31:42.178445  2460 sgd_solver.cpp:106] Iteration 12480, lr = 1e-10
I0422 12:32:19.887125  2460 solver.cpp:228] Iteration 12500, loss = 6245.68
I0422 12:32:19.887217  2460 solver.cpp:244]     Train net output #0: loss = 6245.68 (* 1 = 6245.68 loss)
I0422 12:32:19.887229  2460 sgd_solver.cpp:106] Iteration 12500, lr = 1e-10
I0422 12:32:49.871008  2460 solver.cpp:228] Iteration 12520, loss = 10606.8
I0422 12:32:49.871095  2460 solver.cpp:244]     Train net output #0: loss = 7981.26 (* 1 = 7981.26 loss)
I0422 12:32:49.871115  2460 sgd_solver.cpp:106] Iteration 12520, lr = 1e-10
I0422 12:33:19.843914  2460 solver.cpp:228] Iteration 12540, loss = 10646.2
I0422 12:33:19.844009  2460 solver.cpp:244]     Train net output #0: loss = 6838.42 (* 1 = 6838.42 loss)
I0422 12:33:19.844022  2460 sgd_solver.cpp:106] Iteration 12540, lr = 1e-10
I0422 12:33:49.822047  2460 solver.cpp:228] Iteration 12560, loss = 12583.1
I0422 12:33:49.822158  2460 solver.cpp:244]     Train net output #0: loss = 7977 (* 1 = 7977 loss)
I0422 12:33:49.822173  2460 sgd_solver.cpp:106] Iteration 12560, lr = 1e-10
I0422 12:34:19.796754  2460 solver.cpp:228] Iteration 12580, loss = 10502.1
I0422 12:34:19.796847  2460 solver.cpp:244]     Train net output #0: loss = 8151.95 (* 1 = 8151.95 loss)
I0422 12:34:19.796864  2460 sgd_solver.cpp:106] Iteration 12580, lr = 1e-10
I0422 12:34:57.511241  2460 solver.cpp:228] Iteration 12600, loss = 6264.46
I0422 12:34:57.511329  2460 solver.cpp:244]     Train net output #0: loss = 6264.46 (* 1 = 6264.46 loss)
I0422 12:34:57.511343  2460 sgd_solver.cpp:106] Iteration 12600, lr = 1e-10
I0422 12:35:27.475015  2460 solver.cpp:228] Iteration 12620, loss = 10989.2
I0422 12:35:27.475101  2460 solver.cpp:244]     Train net output #0: loss = 16854.2 (* 1 = 16854.2 loss)
I0422 12:35:27.475114  2460 sgd_solver.cpp:106] Iteration 12620, lr = 1e-10
I0422 12:35:57.460710  2460 solver.cpp:228] Iteration 12640, loss = 8845.02
I0422 12:35:57.460813  2460 solver.cpp:244]     Train net output #0: loss = 7347.3 (* 1 = 7347.3 loss)
I0422 12:35:57.460827  2460 sgd_solver.cpp:106] Iteration 12640, lr = 1e-10
I0422 12:36:27.437988  2460 solver.cpp:228] Iteration 12660, loss = 10976
I0422 12:36:27.438098  2460 solver.cpp:244]     Train net output #0: loss = 9960.42 (* 1 = 9960.42 loss)
I0422 12:36:27.438122  2460 sgd_solver.cpp:106] Iteration 12660, lr = 1e-10
I0422 12:36:57.414455  2460 solver.cpp:228] Iteration 12680, loss = 10704.9
I0422 12:36:57.414542  2460 solver.cpp:244]     Train net output #0: loss = 6572.68 (* 1 = 6572.68 loss)
I0422 12:36:57.414556  2460 sgd_solver.cpp:106] Iteration 12680, lr = 1e-10
I0422 12:37:35.153571  2460 solver.cpp:228] Iteration 12700, loss = 10567.4
I0422 12:37:35.153684  2460 solver.cpp:244]     Train net output #0: loss = 10567.4 (* 1 = 10567.4 loss)
I0422 12:37:35.153700  2460 sgd_solver.cpp:106] Iteration 12700, lr = 1e-10
I0422 12:38:05.128278  2460 solver.cpp:228] Iteration 12720, loss = 11438
I0422 12:38:05.128371  2460 solver.cpp:244]     Train net output #0: loss = 13486.8 (* 1 = 13486.8 loss)
I0422 12:38:05.128386  2460 sgd_solver.cpp:106] Iteration 12720, lr = 1e-10
